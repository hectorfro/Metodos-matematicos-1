\section*{La ruta de este capítulo}
Este capítulo completa el esfuerzo de formalización de conceptos que comenzamos en el capítulo anterior. Iniciamos con el estudio de los funcionales lineales, extendiendo el concepto de función al de una aplicación entre elementos de espacios  vectoriales, lo cual nos llevará a la definición de los espacios vectoriales duales. Incorporamos el concepto de $1-$forma como ese conjunto de funcionales que forman al espacio vectorial dual. Ahora el producto interno se definirá entre $1-$formas y vectores. En la sección \ref{Tensores} definiremos una nueva clase de objetos matemáticos, los tensores, los cuales pueden verse como arreglos multilineales y constituyen una extensión de objetos ya conocidos: los escalares, los vectores y las $1-$formas.  Desarrollaremos el álgebra tensorial y sus leyes de transformación. Ejemplificamos la utilización de este concepto en Física cuando discutimos, en la sección \ref{unpardetensores}, el tensor de inercia y el de esfuerzos (\textit{strees}).  En la sección \ref{Pseudoeuclidianos} incursionaremos en los espacios pseudoeuclidianos para mostrar una situación en la cual podemos diferenciar $1-$formas de vectores, aprovechamos además para introducir algunas nociones básicas de la teoría especial de la relatividad. Para finalizar, el la sección \ref{espaciosdefunciones} extenderemos los conceptos de espacios vectoriales y bases discretas a espacios de funciones y bases continuas y en ese contexto discutimos algunos rudimentos de teorías de distribuciones. 

\section{Funcionales lineales}
\label{FuncionalesLineales}
\index{Funcionales lineales}
En los cursos más elementales de cálculo se estudiaron funciones de una y más variables reales. Estas funciones pueden considerarse que actúan sobre vectores en $\mathds{R}^3$ y podemos extender esta idea para  otras que tengan como argumento vectores de un espacio vectorial abstracto. Comenzaremos con las más sencillas, las lineales, que también son conocidas como {\it operadores lineales}.

Definiremos funcionales lineales  como aquella operación que asocia un número complejo (o real) $\in \textbf{\em K}$ a un vector $\left|  {v}\right> \in\textbf{\em V}$, es decir:
\[
\forall\,\, \left|v\right> \in \textbf{\em V} \quad \Rightarrow 
\mathcal{F}\left[ \left|  {v}\right> \right] \in\mathds{C}\,,
\]
y cumple con:
\[
\mathcal{F}\left[  \alpha\ \left|  {v}_{{1}}\right> +\beta\ \left|  {v}_{2}\right> \right]  \equiv 
\alpha \ \mathcal{F}\left[  \left|  {v}_{{1}}\right> \right] +\beta\ \mathcal{F}\left[  \left|  {v}_{2}\right> \right]\,,
\,\,\forall \,\, \left| {v}_{{1}}\right> ,\left|  {v}_{2}\right> \in \textbf{\em V}\,\, \mbox{y} \,\, \forall \,\, \alpha, \beta \in \textbf{\em K}\,.
\]

En otras palabras, un funcional lineal (o forma lineal) es un {\it morfismo}\footnote{Entenderemos por morfismo a toda regla o mapa que asigne a todo vector $\left|{v}\right>$  de un espacio vectorial $\textbf{\em V}$ un vector $\left|{w}\right>$ de un espacio vectorial $\textbf{\em W}$ y usualmente se denota por $\mathcal{F}: \textbf{\em V} \Rightarrow  \textbf{\em W}$.} del espacio lineal $ \textbf{\em V}$ a un espacio unidimensional $\textbf{\em K}$.

Notemos que cuando se escoge una base $\{\left|\mathrm{e}_i\right>\}$ de un espacio vectorial $\textbf{\em V}$ de manera que para cualquier vector $ \left|v \right> \in \textbf{\em V}$ se especifican sus componentes $\{\xi^1\}$ respecto a esa base, es decir, $\left|v \right>=\xi^i \left|\mathrm{e}_i\right>$, entonces lo que se tiene para cada componente es un funcional lineal, como por ejemplo,  $\mathcal{F}\left[ \left|  {v}\right> \right] =\xi^1$. Algo parecido ocurre con el producto interno, cuando en un espacio vectorial $\textbf{\em V}$ se define el producto escalar 
$\left< v\right.  \left| v_0\right>$, del vector $\left| v\right>$ con un vector fijo $\left| v_0 \right>$, lo que tenemos es un funcional lineal $\mathcal{F}\left[ \left|  {v}\right> \right] =\left< v\right.  \left| v_0\right>= \alpha \in \textbf{\em K}$.

Otro ejemplo sencillo de un funcional lineal es la integral de Riemann que podemos interpretar de la manera siguiente:
\[
\mathcal{I}\left[ \left| f \right>\right]=\int_a^b f_0(x) f(x) \mathrm{d}x \,,
\]
donde $f(x), f_0(x) \in \mathcal{C}_{[a,b]}$, es decir, pertenece al espacio vectorial de funciones reales y continuas en el intervalo $[a,b]$ y $f_0(x)$ es una función que se toma como fija.

\subsection{Espacio vectorial dual}
\label{EspacioVectorialDual}
\index{Dual!Espacios Vectoriales}
\index{Espacios vectoriales duales}
El conjunto de funcionales lineales $\left\{\mathcal{F}_{1},\mathcal{F}_{2},\mathcal{F}_{3}, \cdots,\mathcal{F}_{n},\cdots\right\}$  constituyen a su vez un espacio vectorial, el cual se denomina espacio vectorial dual de $\textbf{\em V}$ --que es el espacio directo-- y se denotará como $\textbf{\em V}^{\ast}$ (aquí $^*$ no es complejo conjugado). Si $\textbf{\em V}$ es de dimensión finita $n$, entonces dim$\textbf{\em V}=$ dim$\textbf{\em V}^{*}=n$.

Es fácil convencerse que los funcionales lineales forman un espacio vectorial ya que, dados $\ \mathcal{F}_{1},\mathcal{F}_{2} \in\textbf{\em V}^{\ast}$ se tiene:
\[
\left.
\begin{array}
[c]{c}
\left(\mathcal{F}_{1}+\mathcal{F}_{2}\right)\left[ \left|  {v}\right> \right] =
\mathcal{F}_{1}\left[  \left|  {v}\right>\right]+\mathcal{F}_{2}\left[\left|{v}\right> \right] \\
\\
\left(  \alpha\ \mathcal{F}\right)  \left[  \left|  {v}\right>\right]  =
\alpha\ \mathcal{F}\left[  \left|  {v}\right>\right]
\end{array}
\right\}  \quad \forall  \quad \left|  {v}\right> \in \textbf{\em V} \,.
\]
A este espacio lineal se le llama espacio de formas lineales y, a los funcionales se les denomina $1-$formas o covectores.
\index{Covector}

Como ya lo mencionamos, en aquellos espacios lineales con producto interno definido, el mismo producto interno constituye la expresión natural del funcional. Así tendremos que:
\[
 \mathcal{F}_{{a}}  \left[  \left|  {v}\right> \right]  \equiv\left< a\right.  \left| {v}\right> \quad\forall{\quad}\left|  {v}
\right> \in\textbf{\em V}\quad\wedge\quad\forall{\quad}\left< a\right|  \in\textbf{\em V}^{\ast} \,.
\]

Es claro comprobar que el producto interno garantiza que los $\left\{ \mathcal{F}_{{a}},\mathcal{F}_{{b}},\cdots\right\}$ forman un espacio vectorial:
\[
\left.
\begin{array}
[c]{c}
\left(  \mathcal{F}_{{a}}+\mathcal{F}_{{b}}\right)  \left[\left|  {v}\right> \right]  =
\mathcal{F}_{{a}}\left[\left|  {v}\right> \right]  +\mathcal{F}_{{b}}\left[ \left|  {v}\right> \right]  =\left< {a}\right.
\left|  {v}\right> +\left< {b}\right.  \left| {v}\right> \\
\\
\left(  \alpha\ \mathcal{F}_{{a}}\right)  \left[  \left| {v}\right> \right]  =\left< \alpha{a}\right.
\left|  {v}\right> =\alpha^{\ast}\left< {a}\right. \left|  {v}\right> =\alpha^{\ast}\ \mathcal{F}_{a}\left[  \left|  {v}\right> \right]
\end{array}
\right\}  \quad\forall{\quad}\left|  {v}\right> \in \textbf{\em V} \,.
\]
Esta última propiedad se conoce como antilinealidad. 

Se establece entonces una correspondencia $1$ a $1$ entre \textit{kets} y \textit{bras}, entre vectores y funcionales lineales (o formas diferenciales): 
\[
\lambda_{1}\left|  {v}_{1}\right> +\lambda_{2}\left| {v}_{2}\right> \qquad\rightleftarrows\qquad\lambda_{1}^{\ast}\left< {v}_{1}\right|  +\lambda_{2}^{\ast}\left<{v}_{2}\right| \,,
\]
que ahora podemos precisar de la siguiente forma:
\begin{align*}
\left< {a}\right.  \left|  {v}\right>  & =\left< {v}\right.  \left|  {a}\right> ^{\ast} \,,\\
\left< {a}\right.  \left|  \lambda_{1}{v}_{1}+\lambda_{2}{v}_{2}\right>  &  =\lambda_{1}\left< {a}\right.  \left|  {v}_{1}\right> +\lambda_{2}\left<{a}\right.  \left|  {v}_{2}\right> \,,\\
\left< \lambda_{1}{a}_{1}{+}\lambda_{2}{a}_{2}\right.  \left|  {v}\right>  &  =\lambda_{1}^{\ast}\left< {a}_{1}\right.  \left|  {v}\right>+\lambda_{2}^{\ast}\left< {a}_{2}\right.  \left|  {v}\right>\,.
\end{align*}

Más aún, dada una base  $\left\{  \left|  {e}_{1}\right> , \left|  \mathrm{e}_{2}\right> , \cdots \left|  \mathrm{e}_{n}\right> \right\}  $ para $\textbf{\em V}$ siempre es posible asociar una base para $\textbf{\em V}^{\ast}$ de tal manera que:
\[
\left|  {v}\right> =
\xi^{i}\left|\mathrm{e}_{i}\right> \,\,\rightleftarrows \,\, \left< {v}\right| = 
\xi_{i}^{\ast}\left< {e}^{i}\right|\,,  \quad\text{con }\,\,
\xi^{i}=\left< {e}^{i}\right.  \left|  {v}\right> \,\, \wedge \,\, 
\xi_{i}^{\ast}=\left< {v}\right.  \left|  \mathrm{e}_{i}\right>,  \quad \text{para } i=1,2,\cdots,n \, .
\]

En un lenguaje arcaico (y muchos textos de mecánica todavía lo reproducen) se denota a la base del espacio dual $\left\{ \left< {e}^{i}\right|  \right\}$ como la base recíproca de la base $\left\{ \left|  \mathrm{e}_{i}\right>  \right\}$, este caso lo ilustraremos más adelante. 

Se puede ver también que si  $\left|  {v}\right>= \xi^i \left|  \mathrm{e}_{i}\right>$, entonces 
\[
\mathcal{F}\left[  \left|  {v}\right> \right] =
\mathcal{F}\left[ \xi^i \left|  \mathrm{e}_{i}\right> \right] =
\xi^{i}\mathcal{F}\left[ \left|  \mathrm{e}_{i}\right> \right]=
\xi^{i}\omega_i \,, \quad \mbox{con } \,\, \mathcal{F}\left[ \left|\mathrm{e}_{i}\right> \right]\equiv \omega_i \,.
\] 

Nótese que estamos utilizando la notación de Einstein en la que índices repetidos indican suma. Nótese también que las bases del espacio dual de formas diferenciales $\left\{\left< {\mathrm{e}}^{k}\right| \right\}$ llevan los índices arriba, los llamaremos índices  contravariantes, mientras que los índices abajo, serán covariantes. Por lo tanto, las componentes de las formas diferenciales en una base dada, llevan índices abajo $\left< a\right|=a_{i}\left< {\mathrm{e}}^{i}\right|$ mientras que las
componentes de los vectores los llevan arriba $\left|  v\right> = \xi^{j}\left|  {\mathrm{e}}_{j}\right>$.  

Observe también que dada una base en el espacio directo $\left\{\left|  {\mathrm{e}}_{i}\right>\right\}$ existe una única base canónica en el dual definida como:
\[
\left< \mathrm{e}^{i}\right.\left|\mathrm{e}_{j}\right>=
\mathcal{F}^i\left[ \left|  \mathrm{e}_{j}\right> \right]= \delta^i_j \, .
\]
Esta $1-$forma al actuar sobre un vector arbitrario resulta en:
\[
\mathcal{F}^i\left[ \left| v \right> \right]= 
\mathcal{F}^i\left[ \xi^j\left|  \mathrm{e}_{j} \right> \right]=
\xi^{j}\mathcal{F}^i\left[ \left|  \mathrm{e}_{j}\right> \right]=
\xi^{j}\delta^i_j=\xi^{i} 
\]
su componente contravariante. El conjunto de $1-$formas  $\{ \mathcal{F}^i \}$ será linealmente independientes. 

Si $\left| v \right>=\xi^i \left| \mathrm{e}_{i} \right>$ es un vector arbitrario en el espacio directo y 
$\left< a\right|= a_{i}\left< {\mathrm{e}}^{i}\right|$ un vector en
el espacio dual, entonces:
\[
\left< a \right.\left| v \right>= 
\left< a_i \left< {\mathrm{e}}^{i}\right| \right. \left. \xi^j \left| \mathrm{e}_{j} \right> \right> = 
a_i^* \xi^j \left< {\mathrm{e}}^{i} \right.\left| \mathrm{e}_{j} \right>=
a_i^* \xi^j \delta_j^i = a_i^* \xi^i \, ,
\]
y para bases arbitrarias, no ortogonales, $\{ \left|  {\mathrm{w}}_{i}\right> \}$ de  
$\textbf{\em V}$ y $\{\left< {\mathrm{w}}^{i}\right|\}$ de $\textbf{\em V}^*$ se tiene:
$\left< a \right.\left| v \right>= \left< {\mathrm{w}}^{i} \right.\left| \mathrm{w}_{j} \right> a_i^* \xi^j   \, .$


\subsection{Espacios duales y bases recíprocas}
\label{BasesReciprocas}
\index{Recíprocas!Bases de vectores}
\index{Bases recíprocas de vectores}
Un ejemplo que ilustra las bases duales y directas son las bases recíprocas y directas que mencionamos anteriormente, vamos a desarrollar un poco más sobre este tema.

Consideremos el problema de expandir un vector $\left| a\right>$  con respecto a una base no ortogonal, $\{\left| \mathrm{w}_{i}\right>\}$, de tal forma que, $\left| a\right>= a^i \left| \mathrm{w}_{i}\right>$.
Por simplicidad, tomemos el caso $\mathds{R}^3$, de manera que  $ \mathbf{a} = a^i {\mathrm{\bf w}}_i $ ($i=1,2,3$).   

Al proyectar el vector $\mathbf{a}$ sobre los ejes de algún sistema de coordenadas, es posible resolver el sistema de tres ecuaciones que resulta para las incógnitas $a^i$. Las bases $ {\mathrm{\bf w}}_i$ y $ {\mathrm{\bf w}}^i$ serán duales, si satisfacen ${\mathrm{\bf w}}_i \cdot {\mathrm{\bf w}}^j=\delta^j_i$. Es decir, cada uno de los vectores bases duales es perpendicular a los otros dos de la base dual: ${\mathrm{\bf w}}^1$ será perpendicular a ${\mathrm{\bf w}}_2$  y ${\mathrm{\bf w}}_3$: ${\mathrm{\bf w}}^1= \alpha ({\mathrm{\bf w}}_2 \times {\mathrm{\bf w}}_3)$.  Como ${\mathrm{\bf w}}_1 \cdot {\mathrm{\bf w}}^1=1$, entonces:
\[
\alpha {\mathrm{\bf w}}_1 \cdot ({\mathrm{\bf w}}_2 \times {\mathrm{\bf w}}_3)=1 \,\, \Rightarrow  \,\, 
\alpha = \frac{1}{{\mathrm{\bf w}}_1 \cdot ({\mathrm{\bf w}}_2 \times {\mathrm{\bf w}}_3)} 
\,\, \Rightarrow \,\,
{\mathrm{\bf w}}^1= \frac{{\mathrm{\bf w}}_2 \times {\mathrm{\bf w}}_3}{ {\mathrm{\bf w}}_1 \cdot ({\mathrm{\bf w}}_2 \times {\mathrm{\bf w}}_3)}\,,
\]
y en general, es fácil ver que:
\[
{\mathrm{\bf w}}^i= \frac{{\mathrm{\bf w}}_j \times {\mathrm{\bf w}}_k}{{\mathrm{\bf w}}_i \cdot ({\mathrm{\bf w}}_j \times {\mathrm{\bf w}}_k)}\,,
\]
donde $i,j,k$ son permutaciones cíclicas de $1,2,3$. Notemos también que $V={\mathrm{\bf w}}_i \cdot ({\mathrm{\bf w}}_j \times {\mathrm{\bf w}}_k)$ es el volumen del paralelepípedo que soportan los vectores $\{ {\mathrm{\bf w}}_i \}$ y que además se puede obtener una expresión análoga para los $\{ {\mathrm{\bf w}}_i \}$ en término de los $\{{\mathrm{\bf w}}^i \}$. 

Al ser $\{ {\mathrm{\bf w}}_i\}$ y $\{ {\mathrm{\bf w}}^i\}$ duales, 
\[
\mathbf{a} = a^j{\mathrm{\bf w}}_j \,\, \Rightarrow  \,\,  
{\mathrm{\bf w}}^i \cdot  \mathbf{a}  = {\mathrm{\bf w}}^i \cdot  (a^j {\mathrm{\bf w}}_j) =  a^j ( {\mathrm{\bf w}}^i \cdot {\mathrm{\bf w}}_j)= a^j\delta_j^i =a^i \,, \quad \text{con } \quad  i=1,2,3 \,
\] 
y equivalentemente:
\[
\mathbf{a} = a_j{\mathrm{\bf w}}^j \,\, \Rightarrow   \,\,
\mathbf{a} \cdot {\mathrm{\bf w}}_i =(a_j{\mathrm{\bf w}}^j)\cdot  {\mathrm{\bf w}}_i =  a_j ( {\mathrm{\bf w}}^j \cdot  {\mathrm{\bf w}}_i) =a_j \delta_i^j=a_i\,,  \quad \text{con } \quad  i=1,2,3 \, .
\]

Es decir, un mismo vector se puede representar en ambas bases, $\mathbf{a}= a^i{\mathrm{\bf w}}_i =a_i{\mathrm{\bf w}}^i$, a través de sus componentes contravariantes $a^i$ (componentes en el espacio directo) o covariantes $a_i$ (componentes en el espacio dual). Si las base directa es ortonormal su dual también lo será y, más importante aún, ambas bases coinciden ${\mathrm{\bf e}}^i \equiv {\mathrm{\bf e}}_i $, y si la base original o directa es dextrógira su dual también lo será. 

Por otro lado, si $\mathbf{a}$ se expresa en un sistema de coordenadas definido por una base: ${\mathrm{\bf w}}_i$: $\mathbf{a}=a^i {\mathrm{\bf w}}_i$ y queremos saber como se escribe en otra base: ${\mathrm{\bf u}}_i$: $\mathbf{a}=\tilde{a}^i {\mathrm{\bf u}}_i$, entonces:
\[
\left\{
\begin{array}
[c]{c}
\mathbf{a} = a^i{\mathrm{\bf w}}_i \,\, \Rightarrow  \,\,
{\mathrm{\bf u}}^j \cdot \mathbf{a} =  {\mathrm{\bf u}}^j 
\cdot (a^i {\mathrm{\bf w}}_i )\,\, \Rightarrow  \,\, \tilde{a}^j= \tilde{A}_i^j a^i \\ \\
\mathbf{a}= a_i{\mathrm{\bf w}}^i \,\, \Rightarrow  \,\, 
{\mathrm{\bf u}}_j \cdot   \mathbf{a}= {\mathrm{\bf u}}_j \cdot (a_i 
{\mathrm{\bf w}}^i) \,\, \Rightarrow  \,\,  \tilde{a}_j=  \tilde{A}_j^i a_i  \,.
\end{array}
\right.
\]
Donde: $\tilde{A}_i^j = {\mathrm{\bf u}}^j \cdot {\mathrm{\bf w}}_i$ y $\tilde{A}^i_j = {\mathrm{\bf u}}_j \cdot {\mathrm{\bf w}}^i$. Del mismo modo el lector puede demostrar que:  ${a}^j= {A}_i^j {\tilde a}^i$ y ${a}_j= {A}_j^i {\tilde a}_i$.  

Para finalizar, es fácil obtener las relaciones entre componentes covariantes y contravariantes,  si definimos una operación entre vectores: ${\mathrm{\bf w}}_i \otimes {\mathrm{\bf w}}_j \equiv {\mathrm{\bf w}}_j \otimes {\mathrm{\bf w}}_i = g_{ij}$ y  ${\mathrm{\bf w}}^i \otimes {\mathrm{\bf w}}^j= {\mathrm{\bf w}}^j \otimes {\mathrm{\bf w}}^i = g^{ij}$, de manera que: 
\[
\mathbf{a} \otimes {\mathrm{\bf w}}_i = a^j({\mathrm{\bf w}}_j \otimes {\mathrm{\bf w}}_i) \,\, \Leftrightarrow \,\,
\mathbf{a} \otimes {\mathrm{\bf w}}^i = a_j({\mathrm{\bf w}}^i \otimes {\mathrm{\bf w}}^j) \, ,
\]
de esta forma tendremos:
\[
a_i = g_{ij}a^j \quad \Leftrightarrow \quad a^i = g^{ij}a_j \,,
\]
con  ${\mathrm{\bf w}}^i \otimes {\mathrm{\bf w}}_j = g^i_j=\delta^i_j$.

Adicionalmente, si las bases son ortogonales, tanto los ${\mathrm{\bf e}}_i$ como los ${\mathrm{\bf e}}^i$, entonces: $g_{ij}= g^{ji} = 0$ ($i \neq j$), lo que resulta en:
\[
a_1 = g_{11}a^1\,,\quad a^1 = g^{11}a_1\,,\quad
a_2 = g_{22}a^2\,,\quad a^2 = g^{22}a_2\,,\quad
a_3 = g_{33}a^3\,,\, a^3 = g^{33}a_3\,.
\]
La definición: ${\mathrm{\bf e}}_i \otimes {\mathrm{\bf e}}_j \equiv {\mathrm{\bf e}}_j \otimes {\mathrm{\bf e}}_i = g_{ij}$, se conoce como el tensor métrico o sencillamente la métrica, y la operación $\otimes$ representa un producto tensorial. Más adelante, en las secciones \ref{TensorMetrico} y \ref{ProductoTensorial} , mostraremos con detalle que estas definiciones que aquí hemos realizado de manera operacional, corresponden a una métrica tal y como la analizamos en \ref{EspaciosMetricos}.
\index{Tensor!metrico}
\index{M\'etrica}
\index{Tensorial!Producto}
\index{Producto!Tensorial}

\subsection{Vectores, formas y leyes de transformación}
\label{VectoresLeyesTransformacion}
\index{Leyes de Transformación para vectores}
\index{Vectores!Leyes de Transformaci\'on}
\index{Covectores}

Tal y como hemos mencionado anteriormente (tempranamente en la sección \ref{RotacionCoordenadas} y luego en la sección \ref{OrtogonalidadBases}),  un determinado vector $\left|  a\right> \in \textbf{\em V}$ puede expresarse en una base ortonormal $\left\{  \left| \mathrm{e}_{j}\right> \right\}$ como: $a^{j}\left| \mathrm{e}_{j}\right> $ donde las $a^{j}$ son las componentes  \textit{contravariantes} del vector en esa base. En general, como es muy largo decir ``componentes del vector contravariante'' uno se refiere (y nos referiremos de ahora en adelante) al conjunto $\left\{  a^{j}\right\}  $ como un \textit{vector contravariante} obviando la precisión de \textit{componente}, pero realmente las $a^{j}$ \textbf{son} las componentes del vector.

Adicionalmente, en esta etapa pensaremos a las bases como distintos observadores o sistemas de referencias. Con ello tendremos (algo que ya sabíamos) que un vector se puede expresar en distintas bases y tendrá distintas componentes referidas a esa base
\[
\left|  a\right> =a^{j}\left| \mathrm{e}_{j}\right>
=\tilde{a}^{j}\left|  {\mathrm{\tilde{e}}}_{j}\right> \,.
\]
Así una misma cantidad física vectorial se verá distinta (tendrá distintas componentes) desde diferentes sistemas de coordenadas.
Las distintas ``visiones'' están conectadas mediante un transformación de sistema de referencia como veremos más adelante.

Igualmente hemos dicho al comienzo de este capítulo (sección \ref{EspacioVectorialDual}) que una forma diferencial o 1-forma, $\left< b\right|  \in \textbf{\em V}^{\ast}$ es susceptible de expresarse en una base $\left\{  \left< \mathrm{e}^{i}\right|  \right\}$ del espacio dual $\textbf{\em V}^{\ast}$ como $b_{i}\left< \mathrm{e}^{i}\right|$ y, como el espacio está equipado con un producto interno, entonces: 
\[
\left< a\right.  \left|  b\right> =\left< b\right.  \left|a\right> =
b_{i}a^{j}\ \left< \mathrm{e}^{i}\right.  \left|  \mathrm{e}_{j}\right>   = b_{i}a^{j}\delta_{j}^{i}=a^{i}b_{i} \,.
\]
Con lo cual avanzamos otra vez en la interpretación de este tipo de objetos: una cantidad física escalar se verá igual (será invariante) desde  distintos sistemas de referencia.

Además sabemos que unas y otras componentes se relacionan como:
\[
\left.
\begin{array}
[c]{c}
\left< {\mathrm{e}}^{i}\right.  \left|  a\right> =a^{j}\left< {\mathrm{e}}^{i}\right.  \left|  \mathrm{e}_{j}\right> =
a^{j}\delta_{j}^{i}= \tilde{a}^{j}\left< {\mathrm{e}}^{i}\right.  \left| {\mathrm{\tilde{e}}}_{j}\right> \\
\\
\left< {\mathrm{\tilde{e}}}^{i}\right.  \left|  a\right> =
\tilde{a}^{j}\left< {\mathrm{\tilde{e}}}^{i}\right.  \left|  {\mathrm{\tilde{e}}}_{j}\right> =
\tilde{a}^{j}\delta_{j}^{i}=a^{j}\left<{\mathrm{\tilde{e}}}^{i}\right.  \left|  \mathrm{e}_{j}\right>
\end{array}
\right\} \,\, \Rightarrow \,\,
\left\{
\begin{array}
[c]{c}
a^{i}=A_{j}^{i}\tilde{a}^{j}\\ \\
\tilde{a}^{i}=\tilde{A}_{j}^{i}a^{j} \,,
\end{array}
\right.
\]
donde claramente:
\[
\left< {\mathrm{e}}^{i}\right.  \left|  {\mathrm{\tilde{e}}}_{j} \right> = A_{j}^{i};\qquad\left< {\mathrm{\tilde{e}}}^{i}\right. \left|  \mathrm{e}_{j}\right> =
\tilde{A}_{j}^{i} \qquad\text{y} \quad  A_{k}^{i}\tilde{A}_{j}^{k}=
\delta_{j}^{i} \quad \Longleftrightarrow  \qquad \tilde{A}_{j}^{i}= \left(  A_{j}^{i}\right)^{-1} \,.
\]

Diremos entonces que aquellos objetos cuyas componentes transforman como: $a^{i}=A_{j}^{i}\tilde{a}^{j}$ o,  equivalentemente como: $\tilde{a}^{i}=\tilde{A}_{j}^{i}a^{j}$ serán vectores, o en un lenguaje un poco más antiguo, vectores contravariantes\footnote{Algunos autores prefieren utilizar la siguiente notación para las transformaciones: $a^{i}=A_{j'}^{i}{a}^{j'}$ y ${a}^{i'}={A}_{j}^{i'}a^{j}$, por lo que  $\delta_{j}^{i}= A_{k'}^{i}{A}_{j}^{k'}$}. 

Tradicionalmente, e inspirados en la ley de transformación, la representación matricial de las componentes contravariantes de un vector, $\left< {\mathrm{e}}^{i}\right.  \left| a\right> =a^{j}$, para una base determinada $\left\{  \left| \mathrm{e}_{j}\right> \right\}$ se representan  como una columna
\[
\left|  a\right> \,\, \Rightarrow \,\, a^i= \left< {\mathrm{e}}^{i}\right.  \left|  a\right>\,, \quad\text{con }i=1,2,3,\cdots
,n \quad \Longleftrightarrow \quad 
\left(
\begin{array}
[c]{c}
a^{1}\\
a^{2}\\
\vdots\\
a^{n}
\end{array}
\right) \,.
\]

De la misma manera, en el espacio dual, $\textbf{\em V}^{\ast}$, las formas diferenciales se podrán expresar en término de una base de ese espacio vectorial como $\left< b\right|  =b_{i}\left< {\mathrm{e}}^{i}\right| =\tilde{b}_{i}\left< {\mathrm{\tilde{e}}}^{i}\right|$. Las $\left\{ b_{i}\right\}$ serán las componentes de las formas diferenciales o las componentes \textit{covariantes} de un vector $\left|  b\right>$, o dicho rápidamente un \textit{vector covariante} o \textit{covector}. Al igual que en el caso de las componentes contravariantes las componentes covariantes transforman de un sistema de referencia a otro mediante la siguiente ley de transformación:
\[
\left.
\begin{array}
[c]{c}
\left< b\right.  \left|  \mathrm{e}_{j}\right> =b_{i}\left< {\mathrm{e}}^{i}\right.  \left|  \mathrm{e}_{j}\right> =b_{i}\delta
_{j}^{i}=\tilde{b}_{i}\left< {\mathrm{\tilde{e}}}^{i}\right.  \left|
\mathrm{e}_{j}\right> \\
\\
\left< b\right.  \left|  {\mathrm{\tilde{e}}}_{j}\right> =\tilde{b}_{i}\left< {\mathrm{\tilde{\mathrm{e}}}}^{i}\right.  \left|  {\mathrm{\tilde{e}}
}_{j}\right> =\tilde{b}_{i}\delta_{j}^{i}=b_{i}\left< {\mathrm{e}}^{i}\right.  \left|  {\mathrm{\tilde{e}}}_{j}\right>
\end{array}
\right\}  \,\, \Rightarrow \,\, \left\{
\begin{array}
[c]{c}
b_{j}=\tilde{b}_{i}A_{j}^{i}\\
\\
\tilde{b}_{j}=b_{i}\tilde{A}_{j}^{i} \,.
\end{array}
\right.
\]

Otra vez, objetos cuyas componentes transformen como $b_{j}=\tilde{b}_{i} A_{j}^{i}$ los denominaremos formas diferenciales o \textit{vectores covariantes} o \textit{covectores} y serán representados como matrices en un arreglo tipo fila:
\[
\left< b\right|  \,\, \Rightarrow \,\, b_i=\left< b\right.  \left|  \mathrm{e}_{i}\right>\,, \quad\text{con }i=1,2,3,\cdots
,n\quad\Longleftrightarrow\quad\left(
\begin{array}
[c]{cccc}
b_{1} & b_{2} & \cdots &  b_{n}
\end{array}
\right) \,.
\]

Quizá hasta este punto la diferencia de formas y vectores, de componentes covariantes y contravariantes, así como sus esquemas de transformación es todavía confusa. No disponemos de ejemplos contundentes que ilustren esa diferencia. Estos serán evidentes cuando nos toque discutir las características de los espacios pseudoeuclidianos en la sección \ref{Pseudoeuclidianos}. Luego, en la sección \ref{CovectoresGeneralizados}, consideraremos algunos ejemplos en coordenadas generalizadas.



\subsection{{\color{Fuchsia}Ejemplos}}

\begin{enumerate}
\item Consideremos $\textbf{\em V}=\mathds{R}^3$ como el espacio vectorial conformado por todos los vectores columna
\[
\left|v\right> =\left(
\begin{array}
[c]{r}
\xi^1\\
\xi^2\\
\xi^3\\
\end{array}
\right) \, ,
\] 
el cual al representarse en su base canónica $\{\left|\mathrm{i}_i\right> \}$ resulta en:
\[
\left|v\right>=
\xi^1 \left(
\begin{array}
[c]{r}
1\\
0\\
0\\
\end{array}
\right) +
\xi^2\left(
\begin{array}
[c]{r}
0\\
1\\
0\\
\end{array}
\right) +
\xi^3 \left(
\begin{array}
[c]{r}
0\\
0\\
1\\
\end{array}
\right) = 
\xi^1\left|\mathrm{i}_1\right>+\xi^2\left|\mathrm{i}_2\right>+\xi^3\left|\mathrm{i}_3\right> = \xi^i\left|\mathrm{i}_i\right> \, .
\] 

Sea un funcional lineal $\mathcal{F}\in \textbf{\em V}^*$, de manera que  los vectores duales $\mathcal{F}[\circ] \equiv \left<F\right| \leftrightarrow  \left( w_1, w_2, w_3 \right)$, puedan ser representados por ``vectores'' filas.

Notemos que la base de funcionales lineals ${\boldsymbol \zeta}^i [\circ] \equiv \left< \mathrm{i}^{i}\right| $,  la definimos como:
\[
{\boldsymbol \zeta}^i\left[\left|\mathrm{i}_j\right>  \right] =
\left< \mathrm{i}^{i}\right.\left|\mathrm{i}_{j}\right>= \delta^i_j
\quad \Rightarrow 
\left< \mathrm{i}^{1}\right.\left|\mathrm{i}_{1}\right>= 1 \,,\,\,
\left< \mathrm{i}^{1}\right.\left|\mathrm{i}_{2}\right>= 0 \,,\,\,
\left< \mathrm{i}^{1}\right.\left|\mathrm{i}_{3}\right>= 0  \,,\,\,
\left< \mathrm{i}^{2}\right.\left|\mathrm{i}_{1}\right>= 0  \,,\,\,
\left< \mathrm{i}^{2}\right.\left|\mathrm{i}_{2}\right>= 1 \,,\cdots
\]
En este caso: ${\boldsymbol \zeta}^i= \left<\mathrm{i}_i\right|$ entonces
$
{\boldsymbol \zeta}^1 = (1,0,0), \quad {\boldsymbol \zeta}^2 = (0,1,0), \quad {\boldsymbol \zeta}^3 = (0,0,1) 
$
y además, 
\[
{\boldsymbol \zeta}^1\left[\left|v\right>  \right]= 
(1,0,0) 
\left(
\begin{array}
[c]{r}
\xi^1\\
\xi^2\\
\xi^3\\
\end{array}
\right)= \xi^1 \,, \quad
{\boldsymbol \zeta}^2\left[\left|v\right>  \right]= 
(0,1,0) 
\left(
\begin{array}
[c]{r}
\xi^1\\
\xi^2\\
\xi^3\\
\end{array}
\right)= \xi^2 \,, \quad
{\boldsymbol \zeta}^3\left[\left|v\right>  \right]= 
(0,0,1) 
\left(
\begin{array}
[c]{r}
\xi^1\\
\xi^2\\
\xi^3\\
\end{array}
\right)= \xi^3 \, .
\]

\item Encontremos la base dual para el espacio vectorial  
$\textbf{\em V}=\mathds{R}^3$, con base ortogonal: 
\[
\left| \mathrm{e}_{1}\right>= 
\left(
\begin{array}[c]{r}
1\\
1\\
-1\\
\end{array}
\right), \ 
\left| \mathrm{e}_{2}\right>=
\left(
\begin{array}[c]{r}
2\\
-1\\
1\\
\end{array}
\right), \  \left| \mathrm{e}_{3}\right>=
\left(
\begin{array}[c]{r}
0\\
-1\\
-1\\
\end{array}
\right) \,.
\]

Todo vector de ese espacio queda representado en esa base por:
\[
\left|v\right>= v^i \left|\mathrm{e}_i\right>=
{v^1} \left(
\begin{array}[c]{r}
1\\
1\\
-1\\
\end{array}
\right) +
{v^2}\left(
\begin{array}[c]{r}
2\\
-1\\
1\\
\end{array}
\right) +
{v^3} \left(
\begin{array}[c]{r}
0\\
-1\\
-1\\
\end{array}
\right) = 
\left(
\begin{array}[c]{c}
v^1+2v^2\\\
v^1-v^2-v^3\\
-v^1+v^2-v^3\\
\end{array}
\right) 
\]
 
Una vez más, sea un funcional lineal $\mathcal{F}\in \textbf{\em V}^*$, representa vectores duales
$\mathcal{F}[\circ] \equiv \left<F\right| \leftrightarrow  \left( w_1, w_2, w_3 \right)$ y  la base en el dual es: $\left< \mathrm{e}^{i}\right|=(a_i, b_i, c_i)$. Además sabemos que: 
$\left< \mathrm{e}^{i}\right.\left|\mathrm{e}_{j}\right>= \delta^i_j$.
Por lo tanto:
\begin{eqnarray*}
\left< \mathrm{e}^{1}\right.\left|\mathrm{e}_{1}\right>&=& 
( a_1, b_1, c_1) 
\left(
\begin{array}[c]{r}
1\\
1\\
-1\\
\end{array}
\right)= a_1 + b_1-c_1=1\\
\left< \mathrm{e}^{1}\right.\left|\mathrm{e}_{2}\right>&=& 
( a_1, b_1, c_1) 
\left(
\begin{array}
[c]{r}
2\\
-1\\
1\\
\end{array}
\right)= 2a_1-b_1+c_1 =0 \,\,\quad  \Rightarrow \,\, 
\left\{
\begin{array}
[c]{l}
 a_1=\frac13\\
 b_1=\frac13\\
 c_1=-\frac13
\end{array}
\right.
\\
\left< \mathrm{e}^{1}\right.\left|\mathrm{e}_{3}\right>&=& 
( a_1, b_1, c_1) 
\left(
\begin{array}
[c]{r}
0\\
-1\\
-1\\
\end{array}
\right)= - b_1- c_1= 0
\end{eqnarray*}

\begin{eqnarray*}
\left< \mathrm{e}^{2}\right.\left|\mathrm{e}_{1}\right>&=& 
( a_2, b_2, c_2) 
\left(
\begin{array}[c]{r}
1\\
1\\
-1\\
\end{array}
\right)= a_2 + b_2-c_2=0\\
\left< \mathrm{e}^{2}\right.\left|\mathrm{e}_{2}\right>&=& 
( a_2, b_2, c_2) 
\left(
\begin{array}
[c]{r}
2\\
-1\\
1\\
\end{array}
\right)= 2a_2-b_2+c_2 =1 \,\,\quad  \Rightarrow \,\, 
\left\{
\begin{array}
[c]{l}
 a_2=\frac13\\
 b_2=-\frac16\\
 c_2=\frac16\\
\end{array}
\right.
\\
\left< \mathrm{e}^{2}\right.\left|\mathrm{e}_{3}\right>&=& 
( a_2, b_2, c_2) 
\left(
\begin{array}
[c]{r}
0\\
-1\\
-1\\
\end{array}
\right)= -b_2- c_2= 0
\end{eqnarray*}
Y finalmente:
\begin{eqnarray*}
\left< \mathrm{e}^{3}\right.\left|\mathrm{e}_{1}\right>&=& 
( a_3, b_3, c_3) 
\left(
\begin{array}[c]{r}
1\\
1\\
-1\\
\end{array}
\right)= a_3 + b_3-c_3=0\\
\left< \mathrm{e}^{3}\right.\left|\mathrm{e}_{2}\right>&=& 
( a_3, b_3, c_3) 
\left(
\begin{array}
[c]{r}
2\\
-1\\
1\\
\end{array}
\right)= 2a_3-b_3+c_3 =0 \,\,\quad  \Rightarrow \,\, 
\left\{
\begin{array}
[c]{l}
 a_3=0\\
 b_3=-\frac12\\
 c_3=-\frac12\\
\end{array}
\right.
\\
\left< \mathrm{e}^{3}\right.\left|\mathrm{e}_{3}\right>&=& 
( a_3, b_3, c_3) 
\left(
\begin{array}
[c]{r}
0\\
-1\\
-1\\
\end{array}
\right)= -b_3- c_3= 1
\end{eqnarray*}

La base del dual es:
$\left< \mathrm{e}^{1}\right|=(\frac13,\frac13,-\frac13), \ 
 \left< \mathrm{e}^{2}\right|=(\frac13,-\frac16,\frac16), \  
 \left< \mathrm{e}^{3}\right|=(0,-\frac12,-\frac12)$, de manera que:
\[
\left< F \right| =w_1\left< \mathrm{e}^{1}\right|+
w_2\left< \mathrm{e}^{2}\right|+w_3\left< \mathrm{e}^{3}\right|. 
\]

Notemos que, como era de esperarse:
\[
\left< \mathrm{e}^{1}\right.\left|v\right>= 
\left(\frac13, \frac13, -\frac13\right)
\left(
\begin{array}[c]{c}
v^1+2v^2\\\
v^1-v^2-v^3\\
-v^1+v^2-v^3\\
\end{array}
\right)  = v^1 \,,\quad
\left< \mathrm{e}^{2}\right.\left|v\right>= 
\left(\frac13, -\frac16, \frac16\right)
\left(
\begin{array}[c]{c}
v^1+2v^2\\\
v^1 -v^2 -v^3\\
-v^1 +v^2 -v^3\\
\end{array}
\right) = v^2
\]
\[
\left< \mathrm{e}^{3}\right.\left|v\right>= 
\left(0, -\frac12, -\frac12\right)
\left(
\begin{array}[c]{c}
v^1 +2v^2 \\
v^1 -v^2 -v^3 \\
-v^1 +v^2 -v^3 \\
\end{array}
\right) = v^3 \,.
\]
%%%%%%%
%%%%%%%

\item Dados los vectores:
${\bf {u}}_1={\bf i}+{\bf j}+2{\bf k},\;$
${\bf {u}}_2={\bf i}+2{\bf j}+3{\bf k},\; $ y 
${\bf {u}}_3={\bf i}-3{\bf j}+4{\bf k}$.  
Revisaremos si estos vectores son mutuamente ortogonales. Encontraremos la base recíproca ${\bf {u}}^i$, el tensor métrico en ambas bases y para el vector ${\bf a}= 3{\bf {u}}_1+2{\bf {u}}_2+{\bf {u}}_3$ encontraremos sus componentes covariantes.

Para saber si son ortogonales simplemente calculamos el producto escalar entre ellos:
${\bf {u}}_1\cdot{\bf {u}}_2=9 \,,$
${\bf {u}}_1\cdot{\bf {u}}_3 =6\,$ y
${\bf {u}}_2\cdot{\bf {u}}_3= 7$, por lo tanto no son ortogonales y adicionalmente sabemos que:
\[
{\bf {u}}^i= \frac{{\bf {u}}_j \times {{\bf {u}}_k}}
{{\bf {u}}_i  \cdot ({\bf {u}}_j \times {\bf {u}}_k)}
\]

Procederemos a calcular primero el denominador:
\[
V={\bf u}_1 \cdot ({\bf u}_2 \times {\bf u}_3) \,\, \Rightarrow  \,\,  
\left({\bf i}+{\bf j}+2{\bf k}\right) \cdot
\left( \left[{\bf i}+2{\bf j}+3{\bf k}\right] \times \left[{\bf i}-3{\bf j}+4{\bf k}\right]\right)=6 \,.
\]
En general:
\[
{\bf u}^i= \frac{{\bf u}_j \times {\bf u}_k}{V } \,\, \Rightarrow  \,\, 
\left\{
\begin{array}[c]{l}
{\bf u}^1= \frac{{\bf u}_2 \times {\bf u}_3}{V}=
\frac{17}{6}{\bf i}-\frac16 {\bf j}-\frac56 {\bf k} \\ 
\\
{\bf u}^2= \frac{{\bf u}_3 \times {\bf u}_1}{V} =
-\frac53 {\bf i}+ \frac13 {\bf j} +\frac23 {\bf k}\\
\\
{\bf u}^3= \frac{{\bf u}_1 \times {\bf u}_2}{V }=
-\frac16{\bf i} -\frac16{\bf j} +\frac16{\bf k}
\end{array}
\right.
\] 
Notemos que:
\[
{\tilde V}={\bf u}^1 \cdot ({\bf u}^2 \times {\bf u}^3)\,\, \Rightarrow  \,\,   
\left(\frac{17}{6}{\bf i}-\frac16 {\bf j}-\frac56 {\bf k} \right) \cdot
\left( \left[-\frac53 {\bf i}+ \frac13 {\bf j} +\frac23 {\bf k}\right] \times \left[-\frac16{\bf i} -\frac16{\bf j} +\frac16{\bf k}\right]\right)=\frac16 \,.
\]

El tensor métrico para la base recíproca será:
\[
{\tilde{g}}^{ij}={\tilde{g}}^{ji}={\bf u}^i\otimes{\bf u}^j= 
\left(
\begin{array}[c]{rrr}
{\bf u}^1\cdot{\bf u}^1 &{\bf u}^1\cdot{\bf u}^2 &{\bf u}^1\cdot{\bf u}^3 \\
{\bf u}^2\cdot{\bf u}^1 &{\bf u}^2\cdot{\bf u}^2 &{\bf u}^2\cdot{\bf u}^3\\
{\bf u}^3\cdot{\bf u}^1 &{\bf u}^3\cdot{\bf u}^2 &{\bf u}^3\cdot{\bf u}^3
\end{array}
\right)=
\left(
\begin{array}[c]{ccc}
\frac{35}{4} &-\frac{16}{3} &-\frac{7}{12} \\
-\frac{16}{3} &\frac{10}{3} &\frac13\\
-\frac{7}{12} &\frac13 &\frac{1}{12}
\end{array}
\right) \,,
\]
mientras que para la base original:
\[
{{g}}_{ij}={{g}}_{ji}={\bf e}_i\otimes{\bf e}_j= 
\left(
\begin{array}[c]{ccc}
{\bf u}_1\cdot{\bf u}_1 &{\bf u}_1\cdot{\bf u}_2 &{\bf u}_1\cdot{\bf u}_3 \\
{\bf u}_2\cdot{\bf u}_1 &{\bf u}_2\cdot{\bf u}_2 &{\bf u}_2\cdot{\bf u}_3\\
{\bf u}_3\cdot{\bf u}_1 &{\bf u}_3\cdot{\bf u}_2 &{\bf u}_3\cdot{\bf u}_3
\end{array}
\right)=
\left(
\begin{array}[c]{ccc}
6 & 9  &6 \\
9 &14 &7\\
6 & 7  & 26
\end{array}
\right) \, .
\]
Y, como debe ser, 
\[
{\bf u}_i\otimes {\bf u}^j= g_i^j=
\left(
\begin{array}[c]{ccc}
6 & 9  &6 \\
9 &14 &7\\
6 & 7  & 26
\end{array}
\right)
\left(
\begin{array}[c]{ccc}
\frac{35}{4} &-\frac{16}{3} &-\frac{7}{12} \\
-\frac{16}{3} &\frac{10}{3} &\frac13\\
-\frac{7}{12} &\frac13 &\frac{1}{12}
\end{array}
\right)=
\left(
\begin{array}[c]{ccc}
1 &0&0 \\
0&1&0\\
0 &0&1
\end{array}
\right) \, .
\] 

Entonces, para un vector dado por: ${\bf a}= 3{\bf u}_1+2{\bf u}_2+{\bf u}_3=6{\bf i}+4{\bf j}+16{\bf k}=a^1{\bf i}+a^2{\bf j}+a^3{\bf k}$, podemos calcular sus componentes covariantes de la manera siguiente:
\[
a_i=g_{ij}a^j=
\left\{
\begin{array}[c]{l}
a_1=g_{11}a^1+g_{12}a^2+g_{13}a^3=(6)(6)+(9)(4)+(6)(16)=168 \\
a_2=g_{21}a^1+g_{22}a^2+g_{23}a^3=(9)(6)+(14)(4)+(7)(16)=222\\
a_3=g_{31}a^1+g_{32}a^2+g_{33}a^3=(6)(6)+(7)(4)+(26)(16)=480
\end{array}
\right.
\]
Esto es:
${\bf a}=a_1{\bf i}+a_2{\bf j}+a_3{\bf k}= 168{\bf u}^1+222{\bf u}^2+480{\bf u}^3=26{\bf i}-34{\bf j}+88{\bf k}$. 

\item Repetiremos los cálculos del ejercicio anterior pero con los vectores: 
\[
{\bf {w}}_1={\bf i}+{\bf j}+2{\bf k}\,,\quad
{\bf {w}}_2=-{\bf i}-{\bf j}+{\bf k}\,,\quad
{\bf {w}}_3=3{\bf i}-3{\bf j}\,.
\] 

Ortogonalidad:
${\bf {w}}_1\cdot{\bf {w}}_2=0 \,, $ ${\bf {w}}_1\cdot{\bf {w}}_3 =0\, $ y 
${\bf {w}}_2\cdot{\bf {w}}_3= 0 \,. $ Son ortogonales. 

La base recíproca se construye a partir de:
\[
{\bf w}^i= \frac{{\bf w}_j \times {\bf w}_k}{{\bf w}_i  \cdot ({\bf w}_j \times {\bf w}_k) } \, ,
\]
donde el denominador es: 
\[
V={\bf w}_1 \cdot ({\bf w}_2 \times {\bf w}_3) \,\, \Rightarrow  \,\,   
\left({\bf i}+{\bf j}+2{\bf k}\right) \cdot \left( \left[-{\bf i}-{\bf j}+{\bf k}\right] \times \left[3{\bf i}-3{\bf j}\right]\right)=18 \, ,
\]
y por lo tanto:
\[
{\bf w}^i= \frac{{\bf w}_j \times {\bf w}_k}{V } \,\, \Rightarrow  \,\,  
\left\{
\begin{array}[c]{l}
{\bf w}^1= \frac{{\bf w}_2 \times {\bf w}_3}{V}=
\frac{1}{6}{\bf i}+\frac16 {\bf j}+\frac13 {\bf k} \\ 
\\
{\bf w}^2= \frac{{\bf w}_3 \times {\bf w}_1}{V} = -\frac13 {\bf i}- \frac13 {\bf j} +\frac13 {\bf k}\\
\\
{\bf w}^3= \frac{{\bf w}_1 \times {\bf w}_2}{V }=
\frac16{\bf i} -\frac16{\bf j} 
\end{array}
\right.
\] 
El volumen recíproco, como era de esperarse:
\[
{\tilde V}={\bf w}^1 \cdot ({\bf w}^2 \times {\bf w}^3) \,\, \Rightarrow  \,\,   
\left(\frac{1}{6}{\bf i}+\frac16 {\bf j}+\frac13 {\bf k} \right) \cdot
\left( \left[-\frac13 {\bf i}- \frac13 {\bf j} +\frac13 {\bf k}\right] \times \left[\frac16{\bf i} -\frac16{\bf j} \right]\right)=\frac{1}{18} \,,
\]
mientras que el tensor métrico para la base recíproca es:
\[
{\tilde{g}}^{ij}={\tilde{g}}^{ji}={\bf w}^i \otimes {\bf w}^j= 
\left(
\begin{array}[c]{rrr}
{\bf w}^1\cdot{\bf w}^1 &{\bf w}^1\cdot{\bf w}^2 &{\bf w}^1\cdot{\bf w}^3 \\
{\bf w}^2\cdot{\bf w}^1 &{\bf w}^2\cdot{\bf w}^2 &{\bf w}^2\cdot{\bf w}^3\\
{\bf w}^3\cdot{\bf w}^1 &{\bf w}^3\cdot{\bf w}^2 &{\bf w}^3\cdot{\bf w}^3
\end{array}
\right)=
\left(
\begin{array}[c]{ccc}
\frac{1}{6} &0 &0 \\
0 &\frac{1}{3} &0\\
0&0 &\frac{1}{18}
\end{array}
\right)
\]
y para la base original
\[
{{g}}_{ij}={{g}}_{ji}={\bf w}_i \otimes {\bf w}_j= 
\left(
\begin{array}[c]{ccc}
{\bf w}_1\cdot{\bf w}_1 &{\bf w}_1\cdot{\bf w}_2 &{\bf w}_1\cdot {\bf w}_3 \\
{\bf w}_2\cdot{\bf w}_1 &{\bf w}_2\cdot{\bf w}_2 &{\bf w}_2\cdot {\bf w}_3\\
{\bf w}_3\cdot{\bf w}_1 &{\bf w}_3\cdot{\bf w}_2 &{\bf w}_3\cdot {\bf w}_3
\end{array}
\right)=
\left(
\begin{array}[c]{ccc}
6 & 0  &0 \\
0 &3 & 0 \\
0 & 0  & 18
\end{array}
\right) \, .
\]

Entonces el vector ${\bf a}= 3{\bf w}_1+2{\bf w}_2+{\bf w}_3 = 4{\bf i}-2{\bf j}+8{\bf k}=a^1{\bf i}+a^2{\bf j}+a^3{\bf k}$, tendrá como componentes covariantes 
\[
a_i=g_{ij}a^j=
\left\{
\begin{array}[c]{l}
a_1=g_{11}a^1=(6)(4)=24 \\
a_2=g_{22}a^2=(3)(-2)=-6\\
a_3=g_{33}a^3=(18)(8)=144
\end{array}
\right.
\]
Esto es:
${\bf a}=a_1{\bf i}+a_2{\bf j}+a_3{\bf k}= 24{\bf e}^1-6{\bf e}^2+144{\bf e}^3=30{\bf i}-18{\bf j}+6{\bf k}$.

Notemos que si la base original hubiese sido ortonormal:
\[
{\bf {w}}_1=({\bf i}+{\bf j}+2{\bf k})/\sqrt{6}\,,\quad
{\bf {w}}_2=(-{\bf i}-{\bf j}+{\bf k})/\sqrt{3}\,,\quad
{\bf {w}}_3=(3{\bf i}-3{\bf j})/\sqrt{18}\,,
\] 
entonces:
\[
g_{ij}={\tilde{g}}^{ij}=
\left(
\begin{array}[c]{ccc}
1 & 0  &0 \\
0 &1 &0\\
0 & 0  & 1
\end{array}
\right) \,\, \Rightarrow  \,\, 
\left\{
\begin{array}[c]{l}
a_1=a^1 \\
a_2=a^2\\
a_3=a^3
\end{array}
\right.
\]
\end{enumerate}

\subsection{{\color{red}Practicando con Maxima}}
\index{Maxima!Bases recíprocas}
\index{Bases recíprocas}


\begin{enumerate}

\item Tenemos para el espacio vectorial  
$\textbf{\em V}=\mathds{R}^3$, la base ortogonal: 
\[
\left| \mathrm{e}_{1}\right>=(1,1,-1), \ \left| \mathrm{e}_{2}\right>=(2,-1,1), \  \left| \mathrm{e}_{3}\right>=(0,-1,-1) \,.
\]

Con $\mathcal{F}\in \textbf{\em V}^*$  donde  $\mathcal{F}[\circ] \equiv \left<F\right| \leftrightarrow \left( w_1, w_2, w_3 \right)$ y  $\left< \mathrm{e}^{i}\right|=(a_i, b_i, c_i)$,  con  
$\left< \mathrm{e}^{i}\right.\left|\mathrm{e}_{j}\right>= \delta^i_j$. 

Comencemos introduciendo los vectores como listas:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e1:[1,1,-1];e2:[2,-1,1];e3:[0,-1,-1];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\left[ 1 , 1 , -1 \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\left[ 2 , -1 , 1 \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ 0 , -1 , -1 \right] 
\end{math}
\newline

Efectivamente son mutuamente ortogonales:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e1.e2;e1.e3;e2.e3;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
0
\end{math}
\newline

Para fines prácticos del cálculo que vamos a realizar, construiremos una matriz con los vectores como filas:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7)  
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
E:matrix([1,1,-1],[2,-1,1],[0,-1,-1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\begin{pmatrix}1 & 1 & -1 \\ 2 & -1 & 1 \\ 0 & -1 & -1 \\ 
 \end{pmatrix}
\end{math}
\newline

De manera que:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M1:E.[a1,b1,c1];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\begin{pmatrix}
-{\tt c1}+{\tt b1}+{\tt a1} \\ 
{\tt c1}- {\tt b1}+2\,{\tt a1} \\ 
-{\tt c1}-{\tt b1} \\ \end{pmatrix}
 \end{math}
\newline

De esta forma es sencillo calcular todas las combinaciones de $\left< \mathrm{e}^{i}\right.\left|\mathrm{e}_{i}\right>$, con las condición $\left< \mathrm{e}^{i}\right.\left|\mathrm{e}_{j}\right>= \delta^i_j$, y resolver los sistemas de ecuaciones para los diferentes $\{ a_i, b_i, c_i\}$. Veamos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ec1:M1[1,1]=1;ec2:M1[2,1]=0;ec3:M1[3,1]=0;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
-{\tt c1}+{\tt b1}+{\tt a1}=1
 \end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
{\tt c1}-{\tt b1}+2\,{\tt a1}=0
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
-{\tt c1}-{\tt b1}=0
 \end{math}
 
 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([ec1,ec2,ec3],[a1,b1,c1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\left[ {\tt a1}=\frac{1}{3} , {\tt b1}=\frac{1}{3} , {\tt c1}=- \frac{1}{3} \right]
 \end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M2:E.[a2,b2,c2];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
\begin{pmatrix}
-{\tt c2}+{\tt b2}+{\tt a2} \\ 
{\tt c2}- {\tt b2}+2\,{\tt a2} \\ 
-{\tt c2}-{\tt b2} \\ 
\end{pmatrix}
  \end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ec1:M2[1,1]=0;ec2:M2[2,1]=1;ec3:M2[3,1]=0;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
-{\tt c2}+{\tt b2}+{\tt a2}=0
 \end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
{\tt c2}-{\tt b2}+2\,{\tt a2}=1
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
-{\tt c2}-{\tt b2}=0
 \end{math}
 
 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([ec1,ec2,ec3],[a2,b2,c2]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
\left[ {\tt a2}=\frac{1}{3} , {\tt b2}=-\frac{1}{6} , {\tt c2}=\frac{1}{6} \right] 
 \end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M3:E.[a3,b3,c3];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
\begin{pmatrix}
-{\tt c_3}+{\tt b_3}+{\tt a_3} \\ 
{ \tt c_3}- {\tt b_3}+2\,{\tt a_3} \\ 
-{\tt c_3}-{\tt b_3} \\ 
\end{pmatrix}
 \end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ec1:M3[1,1]=0;ec2:M3[2,1]=0;ec3:M3[3,1]=1;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o19) }
-{\tt c3}+{\tt b3}+{\tt a3}=0
 \end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o20) }
{\tt c3}-{\tt b3}+2\,{\tt a3}=0
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o21) }
-{\tt c3}-{\tt b3}=1
 \end{math}
 
 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i22)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([ec1,ec2,ec3],[a3,b3,c3]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o22) }
\left[ {\tt a3}=0 , {\tt b3}=-\frac{1}{2} , {\tt c3}=-\frac{1}{2} \right] 
 \end{math}
\newline

De manera que la base dual $\left< \mathrm{e}^{i}\right|$ es:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i23)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
d1:[1/3,1/3,-1/3];d2:[1/3,-1/6,1/6];d3:[0,-1/2,-1/2];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o23) }
\left[ \frac{1}{3} , \frac{1}{3} , -\frac{1}{3} \right]
 \end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o24) }
\left[ \frac{1}{3} , -\frac{1}{6} , \frac{1}{6} \right]
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o25) }
\left[ 0 , -\frac{1}{2} , -\frac{1}{2} \right] 
 \end{math}
\newline

Y son mutuamente ortogonales:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i26)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
d1.d2;d1.d3;d2.d3;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o26) }
0
 \end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o27) }
0
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o28) }
0
 \end{math}
\newline

Originalmente teníamos que:
\[
\left|v\right>= v^i \left|\mathrm{e}_i\right>= v^1 \left|\mathrm{e}_1\right>+ v^2 \left|\mathrm{e}_2\right>+ v^3 \left|\mathrm{e}_3\right>
\]
Si construimos la siguiente matriz:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i29)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
V:transpose(matrix(v1*e1,v2*e2,v3*e3));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o29) }
\begin{pmatrix}
{\tt v1} & 2\,{\tt v2} & 0 \\ {\tt v1} & 
- {\tt v2} & -{\tt v3} \\ 
-{\tt v1} & {\tt v2} & -{ \tt v3} \\ 
 \end{pmatrix}
 \end{math}
\newline

Podremos comprobar que efectivamente: $\left< \mathrm{e}^{i}\right.\left|v\right>= v^i$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i30)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
d1.V;d2.V;d3.V;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o30) }
\begin{pmatrix}{\tt v1} & 0 & 0 \\ \end{pmatrix}
 \end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o31) }
\begin{pmatrix}0 & {\tt v2} & 0 \\ \end{pmatrix}
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o32) }
\begin{pmatrix}0 & 0 & {\tt v3} \\ \end{pmatrix}
 \end{math}
 
 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i33)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}

\item En este ejercicio veremos la manera de construir la matriz de transformación entre bases y el cálculo de las bases recíprocas. 

Si tenemos la siguiente transformación entre bases:
\[
\left|\mathrm{e}_1\right>=\left|\mathrm{j}\right>+\left|\mathrm{k}\right>\,,\quad
\left|\mathrm{e}_2\right>=\left|\mathrm{i}\right>+\left|\mathrm{k}\right>\,,\quad
\left|\mathrm{e}_3\right>=\left|\mathrm{i}\right>+\left|\mathrm{j}\right>\,.
\]
Para calcular la matriz de transformación:
\[
\left|\mathrm{e}_{i}\right>={\tilde A}^j_i\left| \mathrm{i}_{j}\right>\,, 
\]
podemos trabajar de la manera siguiente. Primero introducimos los vectores como una matriz y luego calculamos la transpuesta:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
v1:[0,1,1]$v2:[1,0,1]$v3:[1,1,0]$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Aij:transpose(matrix(v1,v2,v3));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\begin{pmatrix}0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \\ 
\end{pmatrix}
\end{math}
\newline

La matriz de transformación inversa $\left|\mathrm{i}_{i}\right>={A}^j_i\left| \mathrm{e}_{j}\right>$, 
es simplemente la matriz inversa:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A_ji:invert(%);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\begin{pmatrix}-\frac{1}{2} & \frac{1}{2} & \frac{1}{2} \\ \frac{1
 }{2} & -\frac{1}{2} & \frac{1}{2} \\ \frac{1}{2} & \frac{1}{2} & -
 \frac{1}{2} \\ 
 \end{pmatrix}
\end{math}
\newline

Es claro que ${A}^j_k{\tilde A}^k_i= \delta_i^j$ 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A_ij.A_ji;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\begin{pmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 
\end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}
\newline


\item Con el uso del paquete {\bf vect} podemos hacer algunos cálculos sencillos con vectores, como por ejemplo, el cálculo de las bases recíprocas.


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(vect)$
\end{verbatim}}
\end{minipage}

Dado el siguiente conjunto de vectores:
\[
{\tt b} 1={\bf e}_1= {\bf i}+{\bf j}+2{\bf k} \,, \,\,
{\tt b} 2={\bf e}_2= -{\bf i}-{\bf j}-{\bf k}\,, \,\,
{\tt b} 3={\bf e}_3=  2{\bf i}-2{\bf j}+{\bf k}
\]
Calcularemos la base recíproca a través de:
\[
{\bf e}^i= \frac{{\bf e}_j \times {\bf e}_k}{{\bf e}_i \cdot ({\bf e}_j \times {\bf e}_k)}\,,
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
b1:[1,1,2];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\left[ 1 , 1 , 2 \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
b2:[-1,-1,-1]
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ -1 , -1 , -1 \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
b3:[2,-2,1];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\left[ 2 , -2 , 1 \right] 
\end{math}
\newline

Podemos comprobar si  la base original ${\tt b} i$ es ortogonal calculando sus productos escalares:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
b1.b2; b1.b3; b2.b3;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
-4
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
2 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
-1
\end{math}
\newline

Por lo tanto, no son ortogonales.  

Ahora, los vectores recíprocos ${\tt e} i$ se calculan de la manera siguiente: 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e1:express(b2~b3)/(b3.(express(b1~b2)));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\left[ -\frac{3}{4} , -\frac{1}{4} , 1 \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e2:express(b3~b1)/(b3.(express(b1~b2)));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\left[ -\frac{5}{4} , -\frac{3}{4} , 1 \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e3:express(b1~b2)/(b3.(express(b1~b2)));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\left[ \frac{1}{4} , -\frac{1}{4} , 0 \right]
\end{math}
\newline

La base recíproca es entonces:
\[
{\bf e}^1= -\frac{3}{4}{\bf i}-\frac{1}{4} {\bf j}+{\bf k}\,, \,\,
{\bf e}^2= -\frac{5}{4}{\bf i}-\frac{3}{4}{\bf j}+{\bf k}\,, \,\,
{\bf e}^3=   \frac{1}{4}{\bf i}-\frac{1}{4}{\bf j}
\]
Que tampoco es ortogonal:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e1.e2; e1.e3; e2.e3;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\frac{17}{8}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
-\frac{1}{8}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
-\frac{1}{8}
\end{math}
\newline

El tensor métrico para la base original $g_{ij}={\bf e}_i\cdot {\bf e}_j$ lo podemos construir de la manera siguiente:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
gb:matrix([b1.b1,b1.b2,b1.b3],[b2.b1,b2.b2,b2.b3],[b3.b1,b3.b2,b3.b3]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\begin{pmatrix}6 & -4 & 2 \\ -4 & 3 & -1 \\ 2 & -1 & 9 \\ 
\end{pmatrix}
\end{math}
\newline

Para la base recíproca $g^{ij}={\bf e}^i\cdot {\bf e}^j$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ge:matrix([e1.e1,e1.e2,e1.e3],[e2.e1,e2.e2,e2.e3],[e3.e1,e3.e2,e3.e3]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
\begin{pmatrix}\frac{13}{8} & \frac{17}{8} & -\frac{1}{8} \\ \frac{
 17}{8} & \frac{25}{8} & -\frac{1}{8} \\ -\frac{1}{8} & -\frac{1}{8}
  & \frac{1}{8} \\ \end{pmatrix}
\end{math}
\newline

De manera que: ${\bf e}^i\cdot {\bf e}_j=g^i_j=\delta^i_j$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16)  \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
gb.ge;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
\begin{pmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}
\end{math}

\end{enumerate}


\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}


\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Encuentre las bases duales para los siguientes espacios vectoriales:
\begin{enumerate}
\item  $\mathds{R}^2$, donde:  
$\left|\mathrm{e}_{1}\right> \leftrightarrow
\left(
\begin{array}[c]{c}
2 \\
1
\end{array}
\right) $ y   $\left|\mathrm{e}_{2}\right> \leftrightarrow
\left(
\begin{array}[c]{c}
4 \\
1
\end{array}
\right)$. 
\item  $\mathds{R}^3$, donde:  
$\left|\mathrm{e}_{1}\right>\leftrightarrow
\left(
\begin{array}[c]{c}
1 \\
1 \\
3
\end{array}
\right),$     
$\left|\mathrm{e}_{2}\right>
\leftrightarrow
\left(
\begin{array}[l]{c}
0 \\
1 \\
-1
\end{array}
\right)$ y  $\left|\mathrm{e}_{3}\right>\leftrightarrow
\left(
\begin{array}[c]{c}
2 \\
1 \\
0
\end{array}
\right)$.
\end{enumerate}

\item Si $\textbf{\em V}$ es el espacio vectorial de todos los polinomios reales de grado $n\leq1$,  y definimos:
\[
{\boldsymbol \zeta}^1\left[\left|p\right>  \right]= \int_0^1 p(x)\mathrm{d}x  \,\, \wedge \,\,
{\boldsymbol \zeta}^2\left[\left|p\right>  \right]= \int_0^2 p(x)\mathrm{d}x \, ,
\] 
donde $\{{\boldsymbol \zeta}^1, {\boldsymbol \zeta}^2\} \in \textbf{\em V}^*$. Encuentre una base $\{\left|\mathrm{e}_{1}\right>, \left|\mathrm{e}_{2}\right>\} \in \textbf{\em V}$ que resulte ortogonal a la dual a $\{{\boldsymbol \zeta}^1\,, {\boldsymbol \zeta}^2\}$.

\item Si $\mathbf{V}$ es el espacio vectorial de todos los polinomios reales de grado $n\leq2$. Y si además definimos
\[
{\boldsymbol \zeta}^1\left[\left|p\right>  \right] = \int_0^1=p(x)\mathrm{d}x  \,, \,\,\,
{\boldsymbol \zeta}^2\left[\left|p\right>  \right]= p'(2)\,\, \wedge \,\,
{\boldsymbol \zeta}^3\left[\left|p\right>  \right]= p(1)
\] 
donde $\{{\boldsymbol \zeta}^1, {\boldsymbol \zeta}^2, {\boldsymbol \zeta}^3\} \in \mathbf{V}^*$. Encuentre una base $\{\left|\mathrm{e}_{1}\right>, \left|\mathrm{e}_{2}\right>, \left|\mathrm{e}_{3}\right>\} \in \mathbf{V}$ que resulte ortogonal a la dual a $\{{\boldsymbol \zeta}^1\,, {\boldsymbol \zeta}^2, {\boldsymbol \zeta}^3\}$.

\item Sean $\left|v_1\right>$ y $\left| v_2\right> \in \mathbf{V}$ y supongamos que $\mathcal{F}\left[ \left| v_1 \right>\right]=0$ implica que $\mathcal{F}\left[ \left| v_2 \right>\right]=0$ 
$\forall \,\, \mathcal{F} \in \mathbf{V}^*$. Muestre que $\left| v_2 \right>=\alpha\left| v_1 \right>$ con $\alpha \in \mathbf{K}$.

\item Sean $\mathcal{F}_1$ y $\mathcal{F}_2 \in \mathbf{V}^*$ y supongamos que $\mathcal{F}_1\left[ \left| v \right>\right]=0$ implica que $\mathcal{F}_2\left[ \left| v \right>\right]=0$ 
$\forall \,\, \left| v \right> \in \mathbf{V}$. Muestre que $\mathcal{F}_2=\alpha\mathcal{F}_1$ con $\alpha \in \mathbf{K}$.

\item En el caso 3D tenemos que si $\{\mathrm{\bf e}_i\}$ define un sistema de  coordenadas (dextrógiro) no necesariamente ortogonal, entonces, demuestre que: 
\[
\mathrm{\bf e}_i= \frac{\mathrm{\bf e}^j \times \mathrm{\bf e}^k}
{\mathrm{\bf e}^i  \cdot (\mathrm{\bf e}^j \times \mathrm{\bf e}^k)  } \,, \quad 
i,j,k=1,2,3 \mbox{ y sus permutaciones cíclicas}
\]
\item Demuestre que si los volumenes: $V=\mathrm{\bf e}_1 \cdot (\mathrm{\bf e}_2 \times \mathrm{\bf e}_3)$ y ${\tilde V} = \mathrm{\bf e}^1 \cdot (\mathrm{\bf e}^2 \times \mathrm{\bf e}^3)$, entonces $V{\tilde V}= 1$.

\item ¿Qué vector satisface $\mathbf{a} \cdot \mathrm{\bf e}_i$? Demuestre que $\mathbf{a}$ es único.
\item Demuestre que $g_{ij}= \mathrm{\bf e}_i \cdot \mathrm{\bf e}_j $. 
\item Si la base $\{\mathrm{\bf e}_i\}$ es ortogonal, demuestre que:
\begin{enumerate}
\item $g_{ij}$ es diagonal.
\item $g^{ii}=1/g_{ii}$ (no hay suma).
\item $|\mathrm{\bf e}^i|=1/|\mathrm{\bf e}_i|$.
\end{enumerate}
\item Encuentre el producto vectorial de dos vectores $\mathbf{a}$ y $\mathbf{b}$ que están representados en un sistema de coordenadas oblicuo. 
\item Dada la base:
$\mathrm{\bf e}_1= 4{\bf i}+2{\bf j}+{\bf k}\,,\,\,
\mathrm{\bf e}_2= 3{\bf i}+3{\bf j}\,,\,\,  
\mathrm{\bf e}_3= 2{\bf k}$. Encuentre:
\begin{enumerate}
\item Las bases recíprocas $\{\mathrm{\bf e}^i\}$.
\item Las componentes de la métrica $g_{ij}$, $g^{ij}$ y $g^i_j$.
\item Las componentes covariantes y contravariantes del vector 
$\mathbf{a}={\bf i}+2{\bf j}+3{\bf k}$.
\end{enumerate}
\item Resuelva los problemas anteriores utilizando {\bf Maxima}. 
\end{enumerate}

\section{Tensores y producto tensorial}
\label{Tensores}
\index{Tensor}
Las funciones más simples que se pueden definir sobre un espacio vectorial, el funcional lineal, nos permite extendernos al concepto de tensor. Para llegar a la noción de tensores ampliaremos la idea de funcionales lineales, que actúan sobre un único vector, al de funcionales bilineales (o formas bilineales) que tienen dos vectores en su argumento. Como veremos más adelante este tipo de funcionales nos revelarán un contenido geométrico de gran riqueza.

\subsection{Tensores, una definición funcional}
\label{DefinicionTensores}
\index{Tensor!definicion}

Definiremos como un tensor a un funcional lineal (bilineal en este caso) que asocia un elemento del campo $\textbf{\em K}$, complejo o real, a un vector $\ \left|  {v}\right> \in \textbf{\em V}$,  a una forma $\left< {u}\right|  \in \textbf{\em V}^{\ast}$,  o ambas, y cumple con la linealidad. 
Esto es: 
\[
\forall \quad \left| {v} \right \rangle \in \textbf{\em V} \quad \wedge \quad 
\left< {u}\right|  \in \textbf{\em V}^{\ast} \rightarrow \quad \mathcal{T} \left[  \left< {u}\right|  ;\left| {v}\right> \right]  \in\mathds{C}\,.
\]
Entonces se debe cumplir:
\begin{itemize}
\item $\mathcal{T}\left[ \left< {u}\right|  ;\alpha  \left| {v}_{{1}}\right> +\beta  \left|  {v}_{2}
\right> \right]  \equiv \alpha  \mathcal{T}\left[  \left< {u}\right|  ;\left|  {v}_{{1}}\right> \right]
+\beta \ \mathcal{T}\left[  \left< {u}\right|  ;\left| {v}_{2}\right> \right]  \quad \forall  \left| {v}_{{1}} 
\right> ,\left|  {v}_{2}\right> \in \textbf{\em V} \quad \wedge \quad \left< {u} \right|  \in\textbf{\em V}^{\ast}$.

\item $\mathcal{T}\left[  \mu \left< {u}_{{1}}\right| + \nu  \left< {u}_{2}\right|  ;  \left|  {v}\right>
\right]  \equiv \mu \mathcal{T}\left[  \left< {u}_{1}\right|  ;\left|  {v}\right> \right]  +\nu \mathcal{T}
\left[  \left< {u}_{2}\right|  ;\left|  {v}\right> \right]  \quad \forall \ \left|  {v}\right> ,\in \textbf{\em V}\quad\wedge\quad\left< {u}_{1}\right|,\left< {u}_{2}\right|  \in\textbf{\em V}^{\ast}$.
\end{itemize}

En pocas palabras: un tensor es un funcional generalizado cuyos argumentos son vectores y/o formas\footnote{Una presentación interesante y detallada de la utilización del concepto de tensor para el manejo de datos en computación la pueden encontrar en: Lu, H., Plataniotis, K. N., \& Venetsanopoulos, A. N. (2011). A survey of multilinear subspace learning for tensor data. Pattern Recognition, 44(7), 1540-1551.}, lo que significa que $\mathcal{T}\left[\circ ; \bullet\right]$ es una cantidad con dos ``puestos'' y una vez ``cubiertos'' se convierte en un escalar (complejo o real). 

Las combinaciones son muy variadas:

\begin{itemize}
\item Un tensor, con un argumento correspondiente a un vector y un argumento correspondiente a una forma, lo podremos representar de la siguiente manera:
\[
\mathcal{T}\left[  \overset{\left|  {v}\right> }{\overset
{\downarrow}{\circ}};\overset{\left< {u}\right|  }{\overset
{\downarrow}{\mathbf{\bullet}}}\right]  \in\mathds{C} \quad 
\rightrightarrows \quad
\text{tensor de tipo }\left(
\begin{array}
[c]{c}
1\\
1
\end{array}
\right)\,.
\]

\item Un tensor con dos argumentos correspondientes a vectores y uno a una forma sería:
\[
\mathcal{T}\left[  \circ,\circ;\ \mathbf{\bullet}\right]  \rightrightarrows \mathcal{T}\left[  \overset{\left|  {v}_{1}\right> }
{\overset{\downarrow}{\circ}},\overset{\left|  {v}_{2}\right>}{\overset{\downarrow}{\circ}};\overset{\left< {u}\right|}{\overset{\downarrow}{\mathbf{\bullet}}}\right]  \in\mathds{C} \quad 
\rightrightarrows \quad
\text{tensor de tipo }\left(
\begin{array}
[c]{c}
1\\
2
\end{array}
\right) \,,
\]
\item Un tensor con dos argumentos correspondientes a formas y uno a un vector:
\[
\mathcal{T}\left[  \circ;\ \mathbf{\bullet},\mathbf{\bullet} \right]  \rightrightarrows
\mathcal{T}\left[  \overset{\left|  {v}\right> }{\overset
{\downarrow}{\circ}};\overset{\left< {u}_{1}\right|  }
{\overset{\downarrow}{\mathbf{\bullet}}},\overset{\left< {u}
_{2}\right|  }{\overset{\downarrow}{\mathbf{\bullet}}}\right]  \in
\mathds{C} \quad \rightrightarrows \quad \text{tensor de tipo }  
\left(
\begin{array}
[c]{c}
2\\
1
\end{array}
\right)\,.
\]

\item En general:
\[
\mathcal{T}\left[  \overset{\left|  {v}_{1}\right> }
{\overset{\downarrow}{\circ}},\overset{\left|  {v}_{2}\right>
}{\overset{\downarrow}{\circ}},\cdots,\overset{\left|  {v}_{n}\right> }{\overset{\downarrow}{\circ}};\overset{\left<
{u}_{1}\right|  }{\overset{\downarrow}{\mathbf{\bullet}}}
,\overset{\left< {u}_{2}\right|  }{\overset{\downarrow}{\mathbf{\bullet}}}\cdots,\overset{\left< {u}_{m}\right|
}{\overset{\downarrow}{\mathbf{\bullet}}}\right]  \in
\mathds{C}
\quad \rightrightarrows \quad
\text{tensor de tipo }
\left(
\begin{array}
[c]{c}
m\\
n
\end{array}
\right) \,.
\]

\end{itemize}


En ésta notación el punto y coma ($;$) separa las ``entradas'' 
formas de las ``entradas'' vectores. Es importante recalcar que \textbf{el orden si importa}, no sólo para las cantidades separadas por el punto y coma, sino el orden de los ``puestos'' vectores y ``puestos'' formas separados por coma, y repercutirá en las propiedades de los tensores. Por ejemplo: \textbf{si el orden de las ``entradas'' vectores no importa}, podremos permutarlas sin alterar al tensor, \textbf{tendremos entonces tensores simétricos respecto a esos ``puestos'' o ``entradas''}; del mismo modo, serán tensores  antisimétricos aquellos en los cuales \textbf{el orden si importa} y al permutar esos ``puestos'' o ``entradas'' hay un cambio de signo en el tensor. Todos estos casos serán tratados con detalle más adelante, pero vale la pena recalcar que en general, para un tensor genérico el orden de la  ``entradas'' o ``puestos''  si importa pero no necesariamente se comporta como los casos reseñados anteriormente.

Notemos que en el caso general un tensor es básicamente un aplicación multilineal $\mathcal{T}$ sobre $\textbf{\em V}^* \times \textbf{\em V}$:
\[
\mathcal{T}:\textbf{\em V}^{*m} \times \textbf{\em V}^n= 
\underbrace{\textbf{\em V}^*\times \textbf{\em V}^* \cdots \textbf{\em V}^*\times \textbf{\em V}^*}_m
\times
\underbrace{\textbf{\em V}\times \textbf{\em V}\cdots \textbf{\em V}\times \textbf{\em V}}_n \quad \Rightarrow  \mathds{C} \,,
\]
con $n$ el orden covariante y $m$ el orden contravariante. 

Por lo tanto, {\bf un tensor} $\left(
\begin{array}
[c]{c}
m\\
n
\end{array}
\right)$ es un funcional multilineal que asocia  $m$ $1-$formas y $n$ vectores con $\mathds{C}$.

Un ejemplo sencillo de un funcional bilineal sobre un espacio vectorial $\textbf{\em V}^n$ con una base $\left| \mathrm{e}_i \right>$ es:
\[
\mathcal{T}\left[ \left| {v}_{1}\right>, \left| {v}_{2}\right> \right]=
a_{ij} \xi^i \zeta^j \, \qquad (i,j=1,2,\dots,n) \,,
\]
donde:
\[
\left| {v}_{1}\right>=\xi^i\left| \mathrm{e}_i \right>  \,\, \wedge \,\,
\left| {v}_{2}\right>=\zeta^i\left| \mathrm{e}_i \right> \,.
\]
son vectores arbitrarios $\in \textbf{\em V}^n$ y los $a_{ij}$ son números. 

Notemos que:
\[
\mathcal{T}\left[ \left| {v}_{1}\right>, \left| {v}_{2}\right> \right]=
\mathcal{T}\left[ \xi^i\left| \mathrm{e}_i \right> , \zeta^j\left| \mathrm{e}_j \right> \right]=\xi^i\zeta^j \mathcal{T}\left[ \left| \mathrm{e}_i \right> , \left| \mathrm{e}_j \right>\right]=\xi^i\zeta^ja_{ij} \,.
\]
Diremos que ésta será la representación del funcional bilineal para $ \textbf{\em V}^n$. 


Obviamente las formas pueden ser representadas por tensores ya que son funcionales lineales de vectores. Para finalizar, notemos lo siguiente:
\begin{itemize}
\item {\bf Un vector} es un tensor del tipo:

\[
\left(
\begin{array}
[c]{c}
1\\
0
\end{array}
\right)  \rightrightarrows\mathcal{T}\left[  \overset{\left<
{u}\right|  }{\overset{\downarrow}{ \mathbf{\bullet}}}\right]  \in\mathds{C}.
\]
Los vectores constituyen un caso especial de tensores.

\item {\bf Una forma} es un tensor del tipo:
\[
\left(
\begin{array}
[c]{c}
0\\
1
\end{array}
\right)  \rightrightarrows\mathcal{T}\left[  \overset{\left| v\right> }{\overset{\downarrow}{  \circ} }\right]  \in\mathds{C},
\]
porque son funcionales lineales para las formas diferenciales. 

\item {\bf Un escalar} es un tensor del tipo:
\[
\left(
\begin{array}
[c]{c}
0\\
0
\end{array}
\right)   \in\mathds{C}.
\]
\end{itemize}



\subsection{Producto tensorial}
\label{ProductoTensorial}
\index{Tensor!Producto}
\index{Producto!Tensorial}
\index{Producto!Exterior}
\index{Exterior!Producto}

Como será evidente más adelante, los tensores (\textit{simples}) pueden provenir del \textit{producto tensorial} (exterior o directo) de espacios vectoriales. Esto es, consideraremos $\textbf{\em E}_{1}$ y $\textbf{\em E} _{2}$ dos espacios vectoriales con dimensiones $n_{1}$ y $n_{2}$, respectivamente y vectores genéricos, $\left|  \varphi(1)\right> $ y $\left|  \chi(2)\right> $ pertenecientes a estos espacios vectoriales: $\left|  \varphi(1)\right> \in\textbf{\em E}_{1}$ y  $\left|  \chi(2)\right> \in\textbf{\em E}_{2}$. 

Definiremos el \textit{producto tensorial} (\textit{exterior o directo}) de espacios vectoriales, $\textbf{\em E}=\textbf{\em E}_{1}\otimes\textbf{\em E}_{2}$, si a cada par de vectores $\left| \varphi(1)\right>$ y $\left|  \chi(2)\right>$ le asociamos un tensor tipo $\left(
\begin{array}
[c]{c}
2\\
0
\end{array}
\right) $ 
y si se cumple que: 
\[
\left|  \varphi(1)\chi(2) \right> \equiv  
\left|  \varphi(1)\right> \otimes \left|  \chi(2)\right>  \Leftrightarrow 
\mathcal{T}\left[  \overset{\left< \psi(1)\right|  }{\overset{\downarrow}{\mathbf{\bullet}}},\overset{\left< \upsilon(2)\right|  }{\overset{\downarrow}{\mathbf{\bullet}}}\right]
=\left< \psi(1)\right.  \left|  \varphi(1)\right> \left< \upsilon(2)\right.  \left|  \chi(2)\right> \in\mathds{C} \,,
\]
y si además se cumplen  las siguientes propiedades:

\begin{enumerate}
\item  La suma entre tensores de $\textbf{\em E}$ viene definida como:
\[
\left|  \varphi(1)\chi(2)\right> +\left|  \zeta(1)\xi(2)\right> =
\left|  \varphi(1)\right> \otimes\left|  \chi(2)\right> +
\left|  \zeta(1)\right> \otimes\left|  \xi(2)\right> =
\left|  \varphi(1)+\zeta(1)\right> \otimes\left|  \chi(2)+\xi(2)\right>\,.
\]

\item  El producto tensorial es lineal respecto a la multiplicación con
números reales $\lambda$ y $\mu$:
\begin{align*}
\left[  \left|  \lambda\varphi(1)\right> \right]  \otimes\left| \chi(2)\right>  &  = \left[  \lambda\left|  \varphi(1)\right>
\right]  \otimes\left|  \chi(2)\right> =\lambda\left[  \left| \varphi(1)\right> \otimes\left|  \chi(2)\right> \right] =\lambda\left|  \varphi(1)\chi(2)\right>\,,\\
\left|  \varphi(1)\right> \otimes\left[  \left|  \mu\chi(2)\right> \right]   &  =\left|  \varphi(1)\right> \otimes\left[  \mu\left| \chi(2)\right> \right]  =\mu\left[  \left|  \varphi(1)\right> \otimes\left|  \chi(2)\right> \right]  =\mu\left|  \varphi(1)\chi(2)\right>\,.
\end{align*}

\item  El producto tensorial es distributivo respecto a la suma:
\begin{align*}
\left|  \varphi(1)\right> \otimes\left[  \left|  \chi_{1}(2)\right> +\left|  \chi_{2}(2)\right> \right]   &  =\left| \varphi(1)\right> \otimes\left|  \chi_{1}(2)\right> +\left| \varphi(1)\right> \otimes\left|  \chi_{2}(2)\right> \\ 
\left[  \left|  \varphi_{1}(1)\right> +\left|  \varphi_{2}(1)\right> \right]  \otimes\left|  \chi(2)\right>  &  =\left| \varphi_{1}(1)\right> \otimes\left|  \chi(2)\right> +\left| \varphi_{2}(1)\right> \otimes\left|  \chi(2)\right>\,.
\end{align*}
Nótese que las etiquetas $(1)$ y $(2)$ denotan la pertenencia al espacio respectivo.
\end{enumerate}

Es fácil convencerse que los tensores $\left|  \varphi(1)\chi (2)\right> \in\textbf{\em E}=\textbf{\em E}_{1}\otimes\textbf{\em E}_{2}$ forman un espacio vectorial y la demostración se basa en comprobar los axiomas o propiedades de los espacios vectoriales tal y como lo describimos en la sección \ref{EspaciosVectoriales}:

\begin{enumerate}
\item  La operación suma $\boxplus$ es cerrada en $\textbf{\em V}:\forall\ \left|  {v}_{i}\right> ,\left|  {v}_{j}\right> \in\textbf{\em V} \Rightarrow\left|  {v}_{k}\right> =\left|  {v}_{i}\right> \boxplus\left|  {v}_{j}\right> \in \textbf{\em V}$.

Esto se traduce en demostrar que sumados dos tensores $\left|  \varphi(1)\chi(2)\right>$ y $\left|  \zeta(1)\xi(2)\right> \in\textbf{\em E}$ el tensor suma también pertenece a $\textbf{\em E}$,  con $\alpha$ y $\beta$ pertenecientes al campo del espacio vectorial
\[
\alpha\left|  \varphi(1)\chi(2)\right> +\beta\left|  \zeta(1)\xi(2)\right> =
\left|  \alpha\varphi(1)+\zeta(1)\right> \otimes\left|  \chi(2) +\beta\xi(2)\right>\,,
\]
y esto se cumple siempre ya que, el producto tensorial es lineal respecto a la multiplicación con números reales, y por ser $\textbf{\em E}_{1}$ y $\textbf{\em E}_{2}$ espacios vectoriales se cumple:
\[
\left.
\begin{array}
[c]{c}
\left|  \alpha\varphi(1)+\zeta(1)\right> =\alpha\left|  \varphi(1)\right> +\left|  \zeta(1)\right> \in\textbf{\em E}_{1}\\
\\
\left|  \varphi(2)+\beta\zeta(2)\right> = \left|  \varphi(2)\right> +\beta\left|  \zeta(2)\right> \in\textbf{\em E}_{2}
\end{array}
\right\}  \,\, \Rightarrow \,\, \left|  \varphi(1)+\zeta(1)\right> \otimes \left|  \chi(2)+\xi(2)\right> \in \textbf{\em E} \,.
\]

\item  La operación suma $\boxplus$ es conmutativa y asociativa.

Conmutativa: $\forall  \left|  {v}_{i}\right> ,\left|{v}_{j}\right> \in \textbf{\em V} \Rightarrow \left|  {v}_{i}\right> \boxplus \left|  {v}_{j}\right> = \left|{v}_{j}\right> \boxplus \left|  {v}_{i}\right>$. 

Esta primera es clara de la definición de suma:
\begin{align*}
\left|  \varphi(1)\chi(2)\right> +\left|  \zeta(1)\xi(2)\right> &  =\left|  \varphi(1)+\zeta(1)\right> \otimes\left|  \chi(2)+\xi(2)\right> \\ 
\left|  \zeta(1)\xi(2)\right> +\left|  \varphi(1)\chi(2)\right> &  =\left|  \zeta(1)+\varphi(1)\right> \otimes\left|  \xi(2)+\chi(2)\right>\,,
\end{align*}
por ser $\textbf{\em E}_{1}$ y $\textbf{\em E}_{2}$ dos espacios
vectoriales.

Asociativa:  $\forall\ \left|  {v}_{i}\right> ,\left|{v}_{j}\right> ,\left|  {v}_{k}\right>
\in\textbf{\em V} \quad \Rightarrow  \left(  \left|  {v}_{i}\right>
\boxplus \left|  {v}_{j}\right> \right)  \boxplus\left|{v}_{k}\right> =\left|  {v}_{j}\right>
\boxplus \left(  \left|  {v}_{i}\right> \boxplus\left|
{v}_{k}\right> \right)  $

Una vez más, esto se traduce en:
\[
\left(  \left|  \varphi(1)\chi(2)\right> +\left|  \zeta(1)\xi(2)\right> \right)  +\left|  \varkappa(1)\kappa(2)\right> =
\left|\varphi(1)\chi(2)\right> +\left(  \left|  \zeta(1)\xi(2)\right>
+\left|  \varkappa(1)\kappa(2)\right> \right)\,,
\]
con lo cual, por la definición de suma, la expresión anterior queda como:
\begin{align*}
\left(  \left|  \varphi(1)+\zeta(1)\right> \otimes\left|  \xi(2)+\chi(2)\right> \right)  +\left|  \varkappa(1)\kappa(2)\right>
&  =\left|  \varphi(1)\chi(2)\right> +\left(  \left|  \zeta(1)+\varkappa(1)\right> \otimes\left|  \xi(2)+\kappa(2)\right>
\right) \\
\left|  \left(  \varphi(1)+\zeta(1)\right)  +\varkappa(1)\right>\otimes\left|  \left(  \xi(2)+\chi(2)\right)  +\kappa(2)\right>  &
=\left|  \varphi(1)+\left(  \zeta(1)+\varkappa(1)\right)  \right> \otimes\left|  \xi(2)+\left(  \chi(2)+\kappa(2)\right)  \right>\,.
\end{align*}

\item  Existe un único elemento neutro: $\exists \left|  {0}\right>\,\, /  \,\, \left|  {0}\right> \boxplus \left|  {v}_{j}\right> =
\left|  {v}_{j}\right> \boxplus\left|  {0}\right> =
\left|  {v}_{j}\right>  \,\,\forall  \,\, \left|  {v}_{j}\right> \in \textbf{\em V}$.

Es decir:
\[
\left|  \varphi(1)\chi(2)\right> +\left|  0(1)0(2)\right> =
\left|\varphi(1)+0(1)\right> \otimes\left|  \chi(2)+0(2)\right> =
\left|\varphi(1)\right> \otimes\left|  \chi(2)\right> =\left|\varphi(1)\chi(2)\right>\,.
\]

\item  Existe un elemento simétrico para cada elemento de $\textbf{\em V}$:  $\forall\,\, \left|  {v}_{j}\right> \in V \,\, \exists \,\, \left|  -{v}_{j}\right> \,\, /  \,\,\left|{v}_{j}\right> \boxplus\left|  -{v}_{j}\right> =\left|  {0}\right>$.

\[
\left|  \varphi(1)\chi(2)\right> -\left|  \varphi(1)\chi(2)\right>=
\left|  \varphi(1)-\varphi(1)\right> \otimes\left|  \chi(2)-\chi(2)\right> =\left|  0(1)\right> \otimes\left|  0(2)\right>=\left|  0(1)0(2)\right> \,.
\]

\item $\alpha\left(  \beta\left|  {v}_{i}\right> \right)
=\left(  \alpha\beta\right)  \left|  {v}_{i}\right> $:
\[
\alpha\left(  \beta\left|  \varphi(1)\chi(2)\right> \right)=
\alpha\left(  \left|  \beta\chi(2)\right> \otimes\left|\varphi(1)\right> \right) =\left|\alpha\beta\chi(2)\right>\otimes\left|  \varphi(1)\right> =
\left(\alpha\beta\right) \left| \chi(2)\right> \otimes\left|\varphi(1)\right> =\left(\alpha\beta\right)\left| \varphi(1)\chi(2)\right>\,.
\]

\item $\left(  \alpha+\beta\right)  \left|  {v}_{i}\right>
=\alpha\left|  {v}_{i}\right> +\beta\left|  {v}
_{i}\right> $:
\begin{align*}
\left(  \alpha+\beta\right)  \left|  \varphi(1)\chi(2)\right>  &
=\left|  \varphi(1)\right> \otimes\left|  \left(  \alpha+\beta\right)
\chi(2)\right> =\left|  \varphi(1)\right> \otimes\left|
\alpha\chi(2)+\beta\chi(2)\right> \\
&  =\left|  \varphi(1)\right> \otimes\left[  \left(  \alpha\left|
\chi(2)\right> +\beta\left|  \chi(2)\right> \right)  \right] \\
&  =\alpha\left|  \varphi(1)\right> \otimes\left|  \chi(2)\right>
+\beta\left|  \varphi(1)\right> \otimes\left|  \chi(2)\right>\,.
\end{align*}

\item $\alpha\left(  \left|  {v}_{i}\right> \boxplus\left|
{v}_{j}\right> \right)  =\alpha\left|  {v}_{i}
\right> \boxplus\alpha\left|  {v}_{j}\right> $:
\begin{align*}
\alpha\left(  \left|  \varphi(1)\chi(2)\right> +\left|  \zeta
(1)\xi(2)\right> \right)   
&  =
\alpha\left(  \left|  \varphi(1)+\zeta(1)\right> \otimes\left|  \xi(2)+\chi(2)\right> \right)=
\left|  \alpha\left(  \varphi(1)+\zeta(1)\right)  \right>
\otimes\left|  \xi(2)+\chi(2)\right> \\
&  =\left|  \alpha\varphi(1)+\alpha\zeta(1)\right> \otimes\left|
\xi(2)+\chi(2)\right>   =  
\left|  \alpha\varphi(1)\chi(2)\right> +\left|  \alpha
\zeta(1)\xi(2)\right>  \\
&  =\alpha\left|  \varphi(1)\chi(2)\right> +\alpha\left|  \zeta
(1)\xi(2)\right>\,.
\end{align*}
\end{enumerate}

Equivalentemente, podemos construir un producto tensorial entre espacios de formas diferenciales. Si $\textbf{\em E}_{1}^{\ast}$ y $\textbf{\em E}_{2}^{\ast}$ son dos espacios vectoriales duales a $\textbf{\em E}_{1}$ y $\textbf{\em E}_{2},$ con dimensiones $n_{1}$ y $n_{2}$, respectivamente. A estos espacios pertenecen las formas diferenciales genéricas $\left< \zeta
(1)\right|  \in\textbf{\em E}_{1}^{\ast}$ y $\left< \xi(2)\right| \in\textbf{\em E}_{2}^{\ast}$. 

Definiremos el producto tensorial de espacios vectoriales duales, 
$\textbf{\em E}^{\ast}= \textbf{\em E}_{1}^{\ast} \otimes \textbf{\em E}_{2}^{\ast},$ si a cada par de formas diferenciales $\left< \zeta(1)\right|  \in\textbf{\em E}_{1}^{\ast}$ y $\left< \xi(2)\right| \in \textbf{\em E}_{2}^{\ast}$ le asociamos un tensor tipo $\left(
\begin{array}
[c]{c}
0\\
2
\end{array}
\right)$.
 
Esto es:
\[
\left< \zeta(1)\xi(2)\right|  =\left< \zeta(1)\right|
\otimes\left< \xi(2)\right| \,.
\]

\subsection{La tentación del producto interno}

A partir de las definiciones de productos internos en $\textbf{\em E}_{1}$ y $\textbf{\em E}_{2}$, uno puede verse tentado a definir un producto interno de la siguiente forma
\[
\left< \tilde{\varphi}(1)\tilde{\chi}(2)\right.  \left|  \varphi(1)\chi(2)\right> =
\left< \tilde{\varphi}(1)\right.  \left| \varphi(1)\right> \cdot\left< \tilde{\chi}(2)\right.  \left| \chi(2)\right> \, .
\]
 Mostraremos, sin embargo, que NO es una buena definición de producto interno, y para ello debemos demostrar que no se satisfacen los axiomas o propiedades del producto interno que expusimos en la sección \ref{ProductoInterno}. Para facilitar la lectura repetiremos aquí las propiedades que definen el producto interno (expuestas en la sección \ref{ProductoInterno}) y haremos las ``adaptaciones'' del caso:

\begin{enumerate}
\item $\left< {x}\right|  \left.  {x}\right>
\,\, {\in}\quad \mathds{R} \,\,\wedge\,\,\left< {x}\right|  \left.
{x}\right> \geq0 \,\,\forall \,\, \left|  {x}
\right> \in\textbf{\em V}\,\,\mathrm{si}\,\,\left<
{x}\right|  \left.  {x}\right> =0\Rightarrow\left|
{x}\right> \equiv\left|  {0}\right> $.

Esto es:
\[
\left< \varphi(1)\chi(2)\right.  \left|  \varphi(1)\chi(2)\right>
=\left< \varphi(1)\right.  \left|  \varphi(1)\right>
\cdot\left< \chi(2)\right.  \left|  \chi(2)\right> \,,
\]
como $\left< \varphi(1)\right.  \left|  \varphi(1)\right> $ y $\left< \chi(2)\right.  \left|  \chi(2)\right> $ son buenas definiciones de producto interno tendremos que:
\[
\left.
\begin{array}
[c]{c}
\left< \varphi(1)\right.  \left|  \varphi(1)\right> \geq0\\
\\
\left< \chi(2)\right.  \left|  \chi(2)\right> \geq0
\end{array}
\right\}  \Rightarrow\left< \varphi(1)\chi(2)\right.  \left|
\varphi(1)\chi(2)\right> \geq0 \,.
\]

Aquí vale la pena mencionar algunos puntos sutiles sobre la
segunda parte de la propiedad a demostrar: Si  
$\left< {x}\right|  \left.  {x}\right>=0\,\,\Rightarrow\,\,\left|  {x}\right> \equiv\left|  {0}\right> $, lo cual para este caso se traducen en: 
\begin{align*}
\left< \varphi(1)\chi(2)\right.  \left|  \tilde{\varphi}(1)\tilde{\chi
}(2)\right>  &  =\left< \varphi(1)\right.  \left|  \tilde{\varphi
}(1)\right> \cdot\left< \chi(2)\right.  \left|  \tilde{\chi
}(2)\right> =0\\
\left< \varphi(1)\right.  \left|  \tilde{\varphi}(1)\right>
\cdot\left< \chi(2)\right.  \left|  \tilde{\chi}(2)\right>  &
=0\quad \Rightarrow  \left\{
\begin{array}
[c]{c}
\left.
\begin{array}
[c]{c}
\left< \varphi(1)\right.  \left|  \tilde{\varphi}(1)\right> =0\\
\\
\left< \chi(2)\right.  \left|  \tilde{\chi}(2)\right> \neq0
\end{array}
\right\}  \quad \Rightarrow  \left|  \tilde{\varphi}(1)\right> =\left| 0(1)\right> \\
\\
\left.
\begin{array}
[c]{c}
\left< \varphi(1)\right.  \left|  \tilde{\varphi}(1)\right> \neq0\\
\\
\left< \chi(2)\right.  \left|  \tilde{\chi}(2)\right> =0
\end{array}
\right\}  \quad \Rightarrow  \left|  \tilde{\chi}(2)\right> =\left| 0(2)\right> \\
\\
\left.
\begin{array}
[c]{c}
\left< \varphi(1)\right.  \left|  \tilde{\varphi}(1)\right> =0\\
\\
\left< \chi(2)\right.  \left|  \tilde{\chi}(2)\right> =0
\end{array}
\right\} \quad \Rightarrow \left\{
\begin{array}
[c]{c}
\left|  \tilde{\varphi}(1)\right> =\left|  0(1)\right> \\
\\
\left|  \tilde{\chi}(2)\right> =\left|  0(2)\right>
\end{array}
\right.
\end{array}
\right.
\end{align*}
definitivamente, habría que restringir los posibles vectores que intervienen en el producto tensorial, de modo que no fuera posible vectores del tipo: 
\[
\left|  \varphi(1)0(2)\right> \equiv \left|  \varphi(1)\right> \otimes \left|  0(2)\right> 
\qquad \text{o} \qquad
\left| 0(1)\chi(2)\right> \equiv \left|  0(1)\right> \otimes\left| \chi(2)\right>\,,
\]
\textbf{sólo así se cumple la propiedad mencionada}.

\item $\left< {x}\right|  \left.  {y}\right>
=\left< {y}\right|  \left.  {x}\right> ^{\ast
}{\quad}\forall\ \left|  {x}\right> ,\left|
{y}\right> \in\textbf{\em V}$.

Esto puede ser demostrado fácilmente como sigue:
\begin{align*}
\left< \tilde{\varphi}(1)\tilde{\chi}(2)\right.  \left|  \varphi
(1)\chi(2)\right>  &  =\left< \tilde{\varphi}(1)\right.  \left|
\varphi(1)\right> \cdot\left< \tilde{\chi}(2)\right.  \left|
\chi(2)\right> \\
&  =\left< \varphi(1)\right.  \left|  \tilde{\varphi}(1)\right>
^{\ast}\cdot\left< \chi(2)\right.  \left|  \tilde{\chi}(2)\right>
^{\ast}\\
&  =\left(  \left< \varphi(1)\right.  \left|  \tilde{\varphi
}(1)\right> \cdot\left< \chi(2)\right.  \left|  \tilde{\chi
}(2)\right> \right)  ^{\ast}\\
&  =\left< \varphi(1)\chi(2)\right.  \left|  \tilde{\varphi}
(1)\tilde{\chi}(2)\right> ^{\ast}\,.
\end{align*}

\item $\left< {x}\right|  \left.  {y+z}\right>
=\left< {x}\right|  \left.  {y}\right> +\left<
{x}\right|  \left.  {z}\right> \quad\wedge\quad
\left< {x+z}\right|  \left.  {y}\right>
=\left< {x}\right|  \left.  {y}\right> +\left<
{z}\right|  \left.  {y}\right> {\quad}
\forall\ \left|  {x}\right> ,\left|  {y}\right>
,\left|  {z}\right> \in\textbf{\em V}$.

Partimos del lado derecho de la primera de las igualdades anteriores:
\begin{align*}
\left< \tilde{\varphi}(1)\tilde{\chi}(2)\right|  \left[  \left|
\varphi(1)\chi(2)\right> +\left|  \zeta(1)\xi(2)\right> \right]
&  =\left< \tilde{\varphi}(1)\tilde{\chi}(2)\right|  \left[  \left|
\varphi(1)+\zeta(1)\right> \otimes\left|  \xi(2)+\chi(2)\right>
\right] \\
&  =\left< \tilde{\varphi}(1)\right.  \left|  \varphi(1)+\zeta
(1)\right> \cdot\left< \tilde{\chi}(2)\right.  \left|  \xi
(2)+\chi(2)\right>\,,
\end{align*}
y otra vez, como $\left< \varphi(1)\right.  \left|  \varphi
(1)\right> $ y $\left< \chi(2)\right.  \left|  \chi (2)\right> $ son buenas definiciones de producto interno tendremos que:
\begin{align*}
\left< \tilde{\varphi}(1)\right.  \left|  \varphi(1)+\zeta
(1)\right>  &  =\left< \tilde{\varphi}(1)\right.  \left|
\varphi(1)\right> +\left< \tilde{\varphi}(1)\right.  \left|
\zeta(1)\right> \\
\left< \tilde{\chi}(2)\right.  \left|  \xi(2)+\chi(2)\right>  &
=\left< \tilde{\chi}(2)\right.  \left|  \xi(2)\right>
+\left< \tilde{\chi}(2)\right.  \left|  \chi(2)\right>,
\end{align*}
y al multiplicar $\left< \tilde{\chi}(2)\right.  \left|  \xi
(2)+\chi(2)\right> $ por $\left< \tilde{\varphi}(1)\right.
\left|  \varphi(1)+\zeta(1)\right> $ surgirán cuatro sumandos:
\[
\left< \tilde{\varphi}(1)\right.  \left|  \varphi(1)\right>
\left< \tilde{\chi}(2)\right.  \left|  \xi(2)\right> +\left<
\tilde{\varphi}(1)\right.  \left|  \varphi(1)\right> \left<
\tilde{\chi}(2)\right.  \left|  \chi(2)\right> +\left<
\tilde{\varphi}(1)\right.  \left|  \zeta(1)\right> \left<
\tilde{\chi}(2)\right.  \left|  \xi(2)\right> +\left<
\tilde{\varphi}(1)\right.  \left|  \zeta(1)\right> \left<
\tilde{\chi}(2)\right.  \left|  \chi(2)\right>,
\]
lo cual contrasta con el lado izquierdo al utilizar la definición dos veces que contiene dos sumandos:
\[
\left< \tilde{\varphi}(1)\tilde{\chi}(2)\right.  \left|  \varphi
(1)\chi(2)\right> +\left< \tilde{\varphi}(1)\tilde{\chi
}(2)\right.  \left|  \zeta(1)\xi(2)\right> =\left< \tilde{\varphi
}(1)\right.  \left|  \varphi(1)\right> \cdot\left< \tilde{\chi
}(2)\right.  \left|  \chi(2)\right> +\left< \tilde{\varphi
}(1)\right.  \left|  \zeta(1)\right> \cdot\left< \tilde{\chi
}(2)\right.  \left|  \xi(2)\right>,
\]
por lo tanto, \textbf{NO se cumple esta propiedad} y no hay forma de enmendarla.
\end{enumerate}

\subsection{Bases para un producto tensorial}
\label{BasesTensoriales}
\index{Tensor!Bases}

Si $\left\{  \left|  u_{i}(1)\right> \right\}  $ y $\left\{  \left|
v_{i}(2)\right> \right\}  $ son, respectivamente, bases discretas para $\textbf{\em E}_{1}$ y
$\textbf{\em E}_{2}$ entonces podremos construir el tensor:
\[
\left|  u_{i}(1)v_{j}(2)\right> =\left|  u_{i}(1)\right>
\otimes\left|  v_{j}(2)\right> \in\textbf{\em E} \,,
\]
el cual funcionará como una base para $\textbf{\em E}$ y, por lo tanto, podremos construir un
tensor genérico de $\textbf{\em E}$:
\[
\left|  \varphi(1)\chi(2)\right> =\left|  \varphi(1)\right> \otimes \left|  \chi(2)\right> =\varphi^{i}\chi^{j}\left|  u_{i}(1)v_{j}(2)\right>\,,
\]
donde $\varphi^{i}$ y $\chi^{j}$ son las componentes de $\left| \varphi(1)\right> $ y $\left|  \chi(2)\right> $ en sus respectivas bases. En otras palabras, las componentes de un tensor en $\textbf{\em E}$ corresponden a la multiplicación de las componentes de los vectores en $\textbf{\em E}_{1}$ y $\textbf{\em E}_{2}$. Recuerde que estamos utilizando la convención de Einstein de suma tácita en índices covariantes y contravariantes, en la cual $c^{k}\left|  v_{k}\right> \equiv\sum_{k=1}^{n}c^{k}\left|  v_{k}\right>$.

Es importante señalar que si bien un tensor genérico $\left| \Psi\right> \in\textbf{\em E}$ siempre se puede expandir en la base $\left|  u_{i}(1)v_{j}(2)\right> $ no es cierto que todo tensor de $\textbf{\em E}$ provenga del producto tensorial de $\textbf{\em E}_{1}$ y $\textbf{\em E}_{2}.$ Es decir, $\textbf{\em E}$ tiene más tensores de los que provienen el producto tensorial. Esta afirmación puede intuirse del hecho que si $\left|  \Psi\right> \in\textbf{\em E}$ entonces:
\[
\left|  \Psi\right> =c^{ij}\left|  u_{i}(1)v_{j}(2)\right> \,,
\]
por ser $\left\{  \left|  u_{i}(1)v_{j}(2)\right> \right\}  $ base para $\textbf{\em E}$.  Es claro que dados dos números $\alpha_{1}$ y $\alpha_{2}$ habrá $c^{ij}$ que no provienen de la multiplicación de $\alpha_{1}\alpha_{2}$.

El conjunto de todas funciones bilineales $\mathcal{T} \left[  \left< {u}\right|  ;\left| {v}\right> \right]$ forman un espacio vectorial sobre el espacio directo $\textbf{\em E}$. Este espacio vectorial de funciones tendrá una base dada por:
\[
\left|  u_{i}(1)v_{j}(2)\right> = \left|  u_{i}(1)\right> \otimes \left|  v_{j}(2)\right> \,.
\]

\subsection{Tensores, sus componentes y sus contracciones}
\label{ComponentesTensores}
\index{Tensor!Componentes de}
\index{Componentes!Tensores}

Hemos mencionado anteriormente, ubicándonos en $\mathds{R}^3$, que un escalar es una cantidad que se puede especificar, independientemente del sistema de coordenadas, por un sólo número. Los vectores geométricos que dibujábamos con flechas los sustituimos ahora por tres números respecto a una base seleccionada, es decir, a través de sus tres componentes. Los escalares y los vectores son casos particulares de objetos más generales que denominamos tensores, tensores de orden o rango $k$ y cuya especificación en cualquier sistema de coordenadas requerirá de $3^k$ números, llamadas las componentes del tensor. Esto significa que un escalar es un tensor de orden $0$ ($3^0=1$ componente) y un vector un tensor de orden $1$ ($3^1=3$ componentes). Si el espacio vectorial es de dimensión $n$, entonces un tensor de orden $k$ tendrá $n^k$ componentes\footnote{Existen varias presentaciones operativas de estos conceptos para Física e Ingeniería, pueden consultar:
\begin{itemize}
  \item Battaglia, F., \& George, T. F. (2013). Tensors: A guide for undergraduate students. American Journal of Physics, 81(7), 498-511.
  \item Comon, P. (2014). Tensors: a brief introduction. IEEE Signal Processing Magazine, 31(3), 44-53.
\end{itemize}}. 

Como veremos más adelante, los tensores son mucho más que simples números respecto a un sistema de coordenadas y la clave radica en la ``ley de transformación'' de sus componentes, es decir, en la relación que existe entre las componentes de un tensor en un sistema de coordenadas y las componentes del mismo tensor en otro sistema de coordenadas diferente. Lo que hay detrás de todo esto es el hecho de que las leyes matemáticas que describen los fenómenos físicos deben ser ``invariantes'' bajo transformaciones de coordenadas, como por ejemplo: traslaciones (el espacio es homogéneo) y rotaciones (el espacio es isótropo). 

\subsubsection{Componentes de un tensor}

Denominaremos componentes de un tensor, aquellos números que surgen de incorporar bases de formas diferenciales y vectores. Así, si 
$\left\{ \left|  u_{i}(1)\right> ,\left|  v_{j}(2)\right> ,\left| t_{k}(3)\right> \right\}  $ y 
$\left\{  \left< x^{m}(1)\right| ,\left< y^{n}(2)\right|  \right\} $ son bases para los vectores y las formas, respectivamente, las componentes de un tensor 
$\left(
\begin{array}[c]{c}
2 \\
3
\end{array}
\right)$ 
serán:
\[
S_{ijk}^{mn}=\mathcal{S}\left[  \overset{\left| u_{i}(1)\right>}{\overset{\downarrow}{\circ}},\overset{\left|  v_{j}(2)\right>}{\overset{\downarrow}{\circ}},\overset{\left|  w_{k}(3)\right>}{\overset{\downarrow}{\circ}};\overset{\left< x^{m}(1)\right|}{\overset{\downarrow}{\mathbf{\bullet}}},\overset{\left< y^{n}
(2)\right|  }{\overset{\downarrow}{\mathbf{\bullet}}}\right] \,.
\]

Es de hacer notar que la selección de las bases no es arbitraria sino que deben corresponderse, entre el espacio directo, $\textbf{\em E}$, y su dual $\textbf{\em E}^{*}$, i.e.
\[
\left\{ \left|  u_{i}(1)\right> ,\left|  v_{j}(2)\right> ,\left| w_{k}(3)\right> \right\} \otimes \left\{  \left< x^{m}(1)\right| ,\left< y^{n}(2)\right|  \right\} \Leftrightarrow 
\left\{ \left|  x_{p}(1)\right> ,\left|  y_{q}(2)\right>  \right\} \otimes \left\{  \left< u^{a}(1)\right| ,\left< v^{b}(2)\right|, \left< w^{c}(2)\right|  \right\}\,.
\] 

Claramente, esta definición de componentes contiene a las componentes  de aquellos espacios tensoriales generados por el producto tensorial. Si consideramos un tensor como resultado de un producto tensorial y consideramos  las bases: 
$\left\{ \left|  u_{i}(1)\right> ,\left< x^{m}(1)\right|  \right\}$, sus componentes se pueden expresar $\left\{\varphi^{m}(1)\chi_{i}(1)\right\}  $, vale decir:
\[
\left(
\begin{array}
[c]{c}
1\\
1
\end{array}
\right)  \Longleftrightarrow\left|  \varphi(1)\right> \otimes
\left< \Delta(1)\right|  \quad \Rightarrow  \left< x^{m}(1)\right.
\left|  \varphi(1)\right> \otimes\left< \Delta(1)\right|  \left.
u_{i}(1)\right> \quad \Rightarrow  \left\{\varphi^{m}(1)\chi
_{i}(1)\right\}\,.
\]

\subsubsection{Combinaciones lineales de tensores}
\label{CombinacionesLinealesTensores}
\index{Tensor!Combinaciones Lineales}
\index{Desigualdad!Cauchy-Schwarz}

Es claro que podremos sumar (componentes) de tensores como lo hemos hecho con la suma de (componentes) de vectores:
\[
{\bf a}+{\bf b} =\left( a_{x}+b_{x}\right)  \mathbf{i}+\left(a_{y}+b_{y}\right)  \mathbf{j}+\left(  a_{z}+b_{z}\right)\mathbf{{k}}=
\left(a^{1}+b^{1}\right)  \mathbf{i}+\left(a^{2}+b^{2}\right)  \mathbf{j}+\left(a^{3}+b^{3}\right)\mathbf{{k}=}\left(a^{i}+b^{i}\right)  \left|\mathrm{i}_{i}\right>\,,
\]
esto es:
\[
R_{kl}^{ij}  = \alpha Q_{kl}^{ij}+\beta P_{kl}^{ij}\,.
\]

\subsubsection{Producto tensorial de tensores}
\label{ProductoTensorialTensores}
\index{Tensor!Producto tensorial de}
\index{Producto!Tensorial de tensores}

Podemos extender aún más la idea del producto directo y extenderla
para tensores. Así, para dos tensores, uno tipo: 
\[
\left(
\begin{array}
[c]{c}
2\\
0
\end{array}
\right)  \quad \Rightarrow 
\left|  \varphi(1)\chi(2)\right> =
\left|  \varphi(1)\right>\otimes\left|  \chi(2)\right> =\mathcal{T}\left[  \overset{\left<\zeta(1)\right|  }{\overset{\downarrow}
{\mathbf{\bullet}}},\overset
{\left< \xi(2)\right|  }{\overset{\downarrow}{\mathbf{\bullet}}}\right] \\
\left|  \mu(1)\kappa(2)\Theta(1)\right> \,,
\]
y el otro tipo:
\[
\left(
\begin{array}
[c]{c}
2\\
1
\end{array}
\right)\quad \Rightarrow 
\left|  \mu(1)\right>\otimes\left|  \kappa(2)\right> \otimes
\left< \Theta\left(1\right)  \right|  =
\mathcal{P}\left[  \overset{\left|  u_{i}(1)\right>}
{\overset{\downarrow}{\circ}};\overset{\left< \varepsilon(1)\right|
}{\overset{\downarrow}{\mathbf{\bullet}}},\overset{\left< \phi
(2)\right|  }{\overset{\downarrow}{\mathbf{\bullet}}}\right]\,,
\]
el producto directo es:
\begin{eqnarray*}
\left|  \varphi(1)\chi(2)\right> \otimes\left|  \mu(1)\kappa
(2)\Theta(1)\right>  &  = &\left|  \varphi(1)\right> \otimes\left|
\chi(2)\right> \otimes\left|  \mu(1)\right> \otimes\left|
\kappa(2)\right> \otimes\left< \Theta\left(  1\right)  \right|\\
 &=& 
\mathcal{T}\left[  \overset{\left< \zeta(1)\right|  }{\overset
{\downarrow}{\mathbf{\bullet}}},\overset{\left< \xi(2)\right|
}{\overset{\downarrow}{\mathbf{\bullet}}}\right]  \otimes\mathcal{P}\left[
\overset{\left|  u_{i}(1)\right> }{\overset{\downarrow}{\circ}}
;\overset{\left< \varepsilon(1)\right|  }{\overset{\downarrow
}{\mathbf{\bullet}}},\overset{\left< \phi(2)\right|  }{\overset
{\downarrow}{\mathbf{\bullet}}}\right] \\
&=&
 \mathcal{R}\left[  \overset{\left|  u_{i}(1)\right> }{\overset
{\downarrow}{\circ}};\overset{\left< \varepsilon(1)\right|  }
{\overset{\downarrow}{\mathbf{\bullet}}},\overset{\left< \phi(2)\right|
}{\overset{\downarrow}{\mathbf{\bullet}}},\overset{\left< \zeta
(3)\right|  }{\overset{\downarrow}{\mathbf{\bullet}}},\overset{\left<\xi(4)\right|  }{\overset{\downarrow}{\mathbf{\bullet}}}\right] \,.
\end{eqnarray*}
 
En componentes será como se muestra a continuación:
\[
R^{ijlm}_{k}  =  T^{ij} P_{k}^{lm} \,.
\]


\subsubsection{Contracción de un tensor}
\index{Tensor!Contracción}
\index{Contracción de Tensores}

Denominaremos una contracción cuando sumamos las componentes covariantes y contravariantes, esto es, si tenemos $\varphi^{i}(1)\chi_{i}(1)$, entonces se genera un escalar independiente de la base. Esta situación será más evidente cuando definamos métricas y contracción de tensores. Por analogía y considerando un caso más general, dada las componentes $S_{ijk}^{mn}$
correspondiente a un tensor $\left(
\begin{array}
[c]{c}
2\\
3
\end{array}
\right) $ podremos construir un nuevo tensor $\left(
\begin{array}
[c]{c}
1\\
2
\end{array}
\right)$ a partir de una contracción. Las componentes de este nuevo
tensor serán: $S_{ijk}^{mn} \,\,\Rightarrow \,\,  S_{ijk}^{in}\equiv$ ${S}_{jk}^{n}$.
 
Del mismo modo, dadas las componentes de dos tensores, 
$P^{lm}$ y $Q_{zk}^{ij}$ generarán componentes de nuevos tensores $R_{k}^{lij}=P^{lm}Q_{mk}^{ij}$. Así:
\[
\left.
\begin{array}
[c]{c}
\left(
\begin{array}
[c]{c}
2\\
0
\end{array}
\right)  \,\,\Rightarrow \,\, P^{lm}\\
\left(
\begin{array}
[c]{c}
2\\
2
\end{array}
\right)  \,\,\Rightarrow \,\, Q_{zk}^{ij}
\end{array}
\right\} \,\,\Rightarrow \,\, \left(
\begin{array}
[c]{c}
3\\
1
\end{array}
\right) \,\,\Rightarrow \,\, R_{k}^{lij}=P^{lm}Q_{mk}^{ij}\,.
\]

Es claro que si dos tensores derivan de productos tensoriales y si 
$\left\{\left|  u_{i}(1)\right> \right\}  ,\left\{  \left< u^{m}(1)\right|  \right\}  $ y $\left\{  \left|  v_{i}(2)\right> \right\}$ son bases ortonormales para $\textbf{\em E}_{1}$, $\textbf{\em E}_{1}^{\ast}$ y $\textbf{\em E}_{2}$, entonces sus productos podrán ser expresados como:
\[
\left|  \gamma(1)\delta(2)\right> =\underset{P^{ij}}{\underbrace{\left(
\gamma^{i}(1)\delta^{j}(2)\right)  }}\left|  u_{i}(1)\right>
\otimes\left|  v_{j}(2)\right> \,, \qquad 
\left|  \alpha(1)\beta(1)\right> =\underset{Q_{m}^{l}}{\underbrace
{\left(  \alpha^{l}(1)\beta_{m}(2)\right)  }}\left|  u_{l}(1)\right>
\otimes\left< u^{m}(1)\right|\,,
\]
entonces:
\begin{eqnarray*}
& & \left[  \left(  \alpha^{l}(1)\beta_{m}(2)\right)  \left|  u_{l}
(1)\right> \otimes\left< u^{m}(1)\right|  \right]  \left[  \left(
\gamma^{i}(1)\delta^{j}(2)\right)  \left|  u_{i}(1)\right>
\otimes\left|  v_{j}(2)\right> \right]   \\
&=&\alpha^{l}(1)\beta_{m}(2)\left(  \gamma^{i}(1)\delta^{j}(2)\right)
\underset{\delta_{i}^{m}}{\underbrace{\left\{  \left< u^{m}(1)\right.
\left|  u_{i}(1)\right> \right\}  }}\left|  v_{j}(2)\right>
\otimes\left|  u_{l}(1)\right>  \\
&=&\alpha^{l}(1)\beta_{k}(2)\left(  \gamma^{k}(1)\delta^{j}(2)\right)  \left|
v_{j}(2)\right> \otimes\left|  u_{l}(1)\right>  \\
&=& P^{ij}Q_{i}^{l}\left|  v_{j}(2)u_{l}(1)\right> \\
&=& R^{jl}\left|  v_{j}(2)u_{l} (1)\right>\,.
\end{eqnarray*}

Pero más aún, si $\left|  u_{i}(1)v_{j}(2)\right> =\left|
u_{i}(1)\right> \otimes\left|  v_{j}(2)\right> \in\textbf{\em E}$ es base de $\textbf{\em E}$ entonces se puede demostrar lo anterior sin circunscribirnos a tensores cuyas componentes provengan de multiplicación de las componentes en cada espacio vectorial.

\subsubsection{Simetrización de tensores}
\index{Tensor!Simetrización}
\index{Simetrización de tensores}

Un tensor (las componentes) será simétrico respecto a dos de sus índices si su permutación no cambia su valor:
\[
S_{ij}=S_{ji}\,, \quad S^{ij}=S^{ji}\,, \quad S_{ij\cdots kl\cdots mn}=S_{ij\cdots
lk\cdots mn}\,, \quad S^{ij\cdots kl\cdots mn}=S^{ij\cdots lk\cdots mn} \,,
\]
y será antisimétrico si:
\[
A_{ij}=-A_{ji}\,, \quad A^{ij}=-A^{ji}\,, \quad A_{ij\cdots kl\cdots mn}
=-A_{ij\cdots lk\cdots mn}\,, \quad A^{ij\cdots kl\cdots mn}=-A^{ij\cdots
lk\cdots mn} \,.
\]

Un tensor de rango 2, viene representado por una matriz que tendrá $3^2=9$ componentes. Si la matriz es simétrica tendrá como máximo 6 componentes distintas. 
\[
S_{j}^{i}=S_{i}^{j}=\left(
\begin{array}
[c]{ccc}
S_{1}^{1} & S_{2}^{1} & S_{3}^{1}\\
S_{1}^{2} & S_{2}^{2} & S_{3}^{2}\\
S_{1}^{3} & S_{2}^{3} & S_{3}^{3}
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}
S_{1}^{1} & S_{1}^{2} & S_{1}^{3}\\
S_{2}^{1} & S_{2}^{2} & S_{2}^{3}\\
S_{3}^{1} & S_{3}^{2} & S_{3}^{3}
\end{array}
\right) \,.
\]
Mientras que un tensor antisimétrico de segundo orden tendrá, cuando
máximo, tres componentes con valor absoluto distintos de cero,
\[
A_{j}^{i}=-A_{i}^{j}=\left(
\begin{array}
[c]{ccc}
0 & A_{2}^{1} & A_{3}^{1}\\
-A_{1}^{2} & 0 & A_{3}^{2}\\
-A_{1}^{3} & -A_{2}^{3} & 0
\end{array}
\right) \,.
\]

Siempre es posible construir tensores simétricos y antisimétricos a
partir de un tensor genérico. Esto es:
\begin{align*}
S_{ij}  &  =\frac{1}{2}\left(  T_{ij}+T_{ji}\right)  \equiv T_{\left(ij\right)  } \quad \Longleftrightarrow \quad 
S_{ij\cdots kl\cdots mn}=\frac{1}{2}\left(  T_{ij\cdots kl \cdots mn} +T_{ij \cdots lk \cdots mn}\right)
=T_{ij\cdots\left(  kl\right)  \cdots mn}\\
& \\
A_{ij}  &  =\frac{1}{2}\left(  T_{ij}-T_{ji}\right)  \equiv T_{\left[
ij\right]  }\quad\Longleftrightarrow\quad A_{ij\cdots kl\cdots mn}=\frac{1}
{2}\left(  T_{ij\cdots kl\cdots mn}-T_{ij\cdots lk\cdots mn}\right)
=T_{ij\cdots\left[  kl\right]  \cdots mn} \,.
\end{align*}
Es evidente que las componentes de un tensor genérico, $T_{ij}$, pueden expresarse como una combinación de su parte simétrica y antisimétrica:
\[
T_{ij}=S_{ij}+A_{ij}\,.
\]
Obviamente que algo equivalente se puede realizar para componentes contravariantes de tensores.

\subsection{Tensor métrico, índices y componentes}
\label{TensorMetrico}
\index{Tensor!Métrico}
\index{Métrica!Tensor}
Para una base genérica, $\left\{  \left|  {u}_{j}\right>
\right\}$, no necesariamente ortogonal, de un espacio vectorial con producto interno, podemos definir la expresión de un tensor simétrico, 
$\left(
\begin{array}
[c]{c}
0\\
2
\end{array}
\right)$ 
que denominaremos ``tensor métrico'', de la siguiente manera:
\[
\mathbf{g}\left[  \overset{\left|  {u}_{i}\right> }
{\overset{\downarrow}{\circ}},\overset{\left|  {u}_{j}\right>
}{\overset{\downarrow}{\circ},}\right] =
\mathbf{g}\left[  \left| {u}_{i}\right> ,\left|  {u}_{j}\right> \right]= 
g_{ij}\equiv g_{ji} \,\,, \quad 
\mathbf{g}\left[  \overset{\left< {u}^{i}\right|  }
{\overset{\downarrow}{\bullet}},\overset{\left< {u}^{j}\right|
}{\overset{\downarrow}{\bullet}}\right]  =
\mathbf{g}\left[  \left< {u}^{i}\right| ,\left<  {u}^{j}\right| \right]=
g^{ij}\equiv g^{ji} \,,
\]
con $g^{ij}= \left(  g_{ij}\right)  ^{-1}$.


Nótese que las $g_{ij}\equiv g_{ji}$ son las componentes del tensor 
$\mathbf{g}\left[  \overset{}{\overset{}{\circ}},\overset{}{\overset{}{\circ}}\right]$ una vez que la base  $\left\{\left|  {u}_{j}\right> \right\}$ ha actuado. 

La denominación de tensor \textbf{métrico}, no es gratuita, $\mathbf{g}$  cumple con todas las propiedades de la métrica definida para un {\bf espacio vectorial euclidiano} expuestas en la sección \ref{EspaciosMetricos}. 

Una vez más, para facilitar la lectura, transcribiremos a continuación esas propiedades:
\begin{enumerate}
\item $\mathbf{g}\left[  \left|
{u}_{i}\right> ,\left|  {u}_{j}\right> \right]
=g_{ij}\equiv g_{ji}\geq0$\quad$\forall\left|  {u}_{j}\right>$\,, 
y si $\mathbf{\quad g}\left[  \left|  {u}_{i}\right>
,\left|  {u}_{j}\right> \right]  =0\Rightarrow i=j$ \,.

\item $\mathbf{g}\left[  \left|  {u}_{i}\right> ,\left|
{u}_{j}\right> \right]  =\mathbf{g}\left[  \left|  {u}
_{j}\right> ,\left|  {u}_{i}\right> \right]  
\quad \Rightarrow \quad g_{ij}\equiv g_{ji}$ \,.---
\item $\mathbf{g}\left[  \left|  {u}_{i}\right> ,\left|
{u}_{j}\right> \right]  \leq\mathbf{g}\left[  \left|
{u}_{i}\right> ,\left|  {u}_{k}\right> \right]
+\mathbf{g}\left[  \left|  {u}_{k}\right> ,\left|  {u}
_{j}\right> \right]$:  La desigualdad Triangular.
\end{enumerate}

Si la base genérica es ortonormal, $\left\{  \left| {u}_{i}\right>\right\} \rightarrow \left\{  \left| \mathrm{e}_{i}\right>\right\}$, entonces estas propiedades emergen de manera natural:
\begin{equation}
\label{TensorMetrico}
\mathbf{g}\left[ \circ,\circ \right]  \equiv g_{ij}\left< \mathrm{e}^{i}\right| \otimes \left< \mathrm{e}^{j}\right|  \equiv 
g_{ji}\left<\mathrm{e}^{j}\right|  \otimes\left< \mathrm{e}^{i}\right| \quad \text{y} \quad 
\mathbf{g}\left[  \bullet,\bullet \right]  \equiv g^{ij}\left|  \mathrm{e}_{i}\right> \otimes \left|  \mathrm{e}_{j}\right> \equiv g^{ji}\left|  \mathrm{e}_{j}\right>
\otimes\left|  \mathrm{e}_{i}\right> \,,
\end{equation}
con lo cual sus componentes serán matrices simétricas $g_{ij}=g_{ji}$, e igualmente $g^{ij}=g^{ji}$. 

En general impondremos que:
\[
\left(  g_{ij}\left< {u}^{i}\right|  \otimes\left< {u}^{j}\right|  \right)  \left(  g^{km}\left|  {u}_{k}\right> \otimes\left|  {u}_{m}\right> \right)=
g_{ij}g^{km}\left< {u}^{i}\right.  \left|  {u}_{k}\right> \left< {u}^{j}\right.  \left|  {u}_{m}\right> =g_{ij}g^{km}\delta_{k}^{i}\delta_{m}^{j}=
g_{ij}g^{ji}=\delta_{i}^{i}=n \,,
\]
ya que $i,j=1,2,3,\cdots, n$.  Con lo cual $g_{ij}$ es la matriz inversa de $g^{ij}$, es decir, hemos definido las componentes contravariantes del tensor de modo que cumplan con $g_{ik}g^{kj}=\delta_{i}^{j}$.

Adicionalmente, también es claro que si $\left|  a\right>=a^{k} \left|  {u}_{k}\right>$, entonces:
\[
\left(  g_{ij}\left< {u}^{i}\right|  \otimes\left< {u}^{j}\right|  \right)  \left|  a\right> =
a^{k}\left(g_{ij}\left< {u}^{i}\right|  \otimes\left< {u}^{j}\right|  \right)  \left|  {u}_{k}\right> =
a^{k} g_{ij}\left< {u}^{j}\right.  \left|  {u}_{k}\right> \left< {u}^{i}\right|  =a^{k}g_{ij}\delta_{k}^{j}\left<{u}^{i}\right|  =a^{k}g_{ik}\left< {u}^{i}\right|  \equiv
a_{i}\left< {u}^{i}\right| \,,
\]
con lo cual $a_{i}=a^{k}g_{ik}$. 

De la misma forma:
\[
\left< a\right|  \left(  g^{ij}\left|  {u}_{i}\right> \otimes \left|  {u}_{j}\right> \right)  =
\left< a\right| \left(  g^{ij}\left|  {u}_{i}\right> \otimes \left| {u}_{j}\right> \right)  =
g^{ij}\left< a\right.  \left| {u}_{i}\right> \otimes \left|  {u}_{j}\right>=
a_{k}g^{ij}\left< {u}^{k}\right.  \left|  {u}_{i}\right> \left|  {u}_{j}\right> =
a_{k}g^{kj}\left| {u}_{j}\right> \equiv a^{j}\left|  {u}_{j}\right> \,,
\]
otra vez $a^{j}=a_{k}g^{kj}$, ahora subimos el índice correspondiente. 

De esta manera, el tensor métrico nos permite asociar formas con vectores, componentes covariantes (formas) a componentes contravariantes (vectores) y dicho rápido y mal, pero de uso muy frecuente: el tensor métrico nos ``permite subir y bajar índices''. 

Otra forma de verlo es combinando las propiedades del producto directo de tensores y contracción de índices:
\begin{align*}
g^{ij}\left|  {u}_{i}\right> \otimes \left|  {u}_{j}\right> \otimes P_{k}^{lmn}\left|  {u}_{l}\right> \otimes \left|  {u}_{m}\right> \otimes\left|  {u}_{n}\right> \otimes\left< {u}^{k}\right| & \,\, \Rightarrow \,\,  
g^{ij}P_{k}^{lmn}\left|  {u}_{j}\right> \otimes P_{k}^{lmn}\left|  {u}_{l}\right> \otimes\left|  {u}_{m}\right> \otimes\left|  {u}_{n}\right> \otimes \left< {u}^{k}\right|  \left.  {u}_{i}\right> \\
& \\
g^{ij}P_{k}^{lmn}\left|  {u}_{j}\right> \otimes\left| {u}_{l}\right> \otimes\left|  {u}_{m}\right>
\otimes\left|  {u}_{n}\right> \cdot\underset{\delta_{i}^{k}}{\underbrace{\left< {u}^{k}\right|  \left.  {u}_{i}\right> }}  &  =P^{jlmn}\left|  {u}_{j}\right> \otimes \left|  {u}_{l}\right> \otimes\left|  {u}_{m}\right> \otimes\left|  {u}_{n}\right> 
\quad \Rightarrow  g^{ij}P_{i}^{lmn}\equiv P^{jlmn}\,.
\end{align*}

Adicionalmente, el tensor métrico permite la contracción de índices. Así, dado un producto tensorial de dos vectores que se pueden expresar en una base ortonormal $\left\{  \left| \mathrm{e}_{i}\right> \right\}$:
\begin{gather*}
\left|  a,b\right> =\left|  a\right> \otimes\left|  b\right> = a^{k}b^{m}\left|  \mathrm{e}_{k}\right> \otimes\left|  \mathrm{e}_{m}\right> \\
\Downarrow\\
\left(  g_{ij}\left< \mathrm{e}^{i}\right|  \otimes\left< \mathrm{e}^{j}\right|  \right)  \left(  a^{k}\left|  \mathrm{e}_{k}
\right> \otimes b^{m}\left|  \mathrm{e}_{m}\right> \right) = a^{k}b^{m}g_{ij}\delta_{k}^{i}\delta_{m}^{j}=a^{k}b^{m}g_{km}=a^{k} b_{k}=\left< b\right.  \left|  a\right> =\left< a\right. \left|  b\right> \,.
\end{gather*}

Es decir, el producto interno de dos vectores involucra, de manera natural, la métrica del espacio,
\[
\left< b\right.  \left|  a\right> =\left< a\right.  \left| b\right> =a^{k}b_{k}=a_{k}b^{k}=a^{k}b^{m}g_{km}=a_{k}b_{m}g^{km} \,.
\]
Obviamente la norma de un vector, también incluirá al tensor métrico:
\[
\left\|  \left|  a\right> \right\|  ^{2}=\left< a\right.  \left| a\right> =
a_{i}a^{j}\left< \mathrm{e}^{i}\right.  \left|\mathrm{e}_{j}\right> =a_{i}a^{i}=a_{i}a_{j}\ g^{ij}=a^{i}a^{j}\ g_{ij} \,.
\]
\subsection{Métrica, elemento de línea y factores de escala}
\label{MetricaElementoLinea}
\index{Elemento de línea}
El caso más emblemático lo constituye la norma de un desplazamiento infinitesimal. Para una base genérica, $\left\{\left| {u}_{i}\right> \right\} $,  no necesariamente ortogonal de un espacio vectorial con producto interno, el desplazamiento infinitesimal puede expresarse como:
\begin{equation}
\label{DesplazaInfinitesimal}
\mathrm{d}s^{2} \equiv \left< \mathrm{d}{r}\right.  \left|  \mathrm{d}{r}\right> = 
\left(  \mathrm{d} {x}_{k}\ \left< u^{k}\right|  \right)  \left(\mathrm{d} {x}^{m}\ \left|  u_{m}\right>\right)  =
\left< u^{k}\right.  \left|  u_{m}\right> \ \mathrm{d} {x}_{k}\ \mathrm{d} {x}^{m}=
\mathrm{d} {x}_{m}\ \mathrm{d} {x}^{m}={g}_{km}\ \mathrm{d} {x}^{k}\mathrm{d} {x}^{m}\,.
\end{equation}
Si las bases de formas y vectores son ortogonales, $\left\{\left| \mathrm{e}_{i}\right> \right\} $, (cosa más o menos común pero no necesariamente cierta siempre) la métrica será diagonal y como en general: $\left\| \left| \mathrm{e}_{i}\right>  \right\|~\neq~1$, entonces surgen los llamados factores de escala $h_{i} = \sqrt{g_{ii}}$:
\begin{equation}
\label{MetricaFactoresEscala}
g_{ii}=\frac{1}{g^{ii}} \quad \Rightarrow \quad\left(  \mathrm{d}s\right)^{2}=\left(  h_{1}\ \mathrm{d}x^{1}\right)^{2}+\left(  h_{2}\ \mathrm{d}x^{2}\right)^{2}+\left( h_{3}\ \mathrm{d}x^{3}\right)^{2}\,,
\end{equation}
donde $h_{i}=\sqrt{g_{ii}}  $,  con $i,j=1,2,3$ (aquí no hay suma). 

En la sección \ref{CoordenadasGeneralizadas} discutiremos con detalle la construcción y el significado de los factores de escala para sistemas de coordenadas tridimensionales, por ahora los mencionamos destacando sus consecuencias en la transformación de las componentes covariantes y contravariantes entre distintas bases de vectores.

De esta manera, las componentes covariantes y contravariantes estarán relacionadas, a través de los factores de escala como:
\[
a_{j} = g_{jk}a^{k} \,\, \Rightarrow \,\,  a_{i} = h_{[i]}a^{[i]}\,. 
\quad (\mbox{Aquí } h_{[i]}a^{[i]} \quad \mbox{NO indica suma}).
\]
En otras palabras, en aquellos sistemas de coordenadas en los cuales la métrica es diagonal pero no viene representada por la matriz unidad, subir y bajar indices puede incluir los cambios de escala.

Obviamente,  si la base $\left\{  \left| \mathrm{i}_{i}\right> \right\}$ es la canónica, es fácil ver que:
\[
\left(  \mathrm{d}s\right)^{2}\equiv\left< \mathrm{d}{r}\right.  
\left| {d}{r}\right>  =
\delta^k_m  \mathrm{d} {x}_{k}\ \mathrm{d} {x}^{m}=
\mathrm{d} {x}_{m} \mathrm{d} {x}^{m}. 
\]
es decir:
 $g_{11}=g_{22}=g_{33}=1 \,, \quad g_{ij}=0 \,\, \mbox{si} \,\, i \neq j\,,$
esto significa que en coordenadas cartesianas el desplazamiento infinitesimal es la ya conocida expresión:
 $\mathrm{d}s^{2}=\mathrm{d}x^{2}+\mathrm{d}y^{2}+\mathrm{d}z^{2}$. 

\subsubsection{Teorema del cociente}
Al igual que existe el producto directo entre tensores, cabe preguntarse si es posible multiplicar un tensor arbitrario (de cualquier rango) por un ``objeto'' desconocido ¿Ese producto será un tensor? 

Existe importantes situaciones físicas en las cuales es aplicable esta pregunta. Si $T_{ij}$ son las componentes de un tensor de rango 2 y el producto $T_{ij}V^{i}=B_{j}$ es un objeto representado por un sólo índice ¿Este objeto $V^{i}$ será un vector?  La respuesta es siempre afirmativa, y puede ser utilizada como un criterio para identificar componente de tensores.  Este criterio se denomina el \textit{Teorema del Cociente.}

La respuesta a ésta pregunta surge de la respuesta a una pregunta distinta pero equivalente. Supongamos que nos dan $n^{2}$ números $a_{ij}$ y un (una componente de un) vector genérico $V^{i}$, si la cantidad $a_{ij}V^{i}V^{j}$ es
un escalar entonces la parte simétrica $a_{\left(ij\right)}=\frac{1}{2}\left(  a_{ij}+a_{ji}\right) $ será un (una componente de) tensor del tipo: 
$\left(
\begin{array}[c]{c}
0\\
2
\end{array}
\right)$. La demostración involucra algunas de las ideas antes expuestas y la haremos para fijar conceptos.

Dados dos sistemas de coordenadas $x^{i}=x^{i}\left(  \tilde{x}^{m}\right)$ y $\tilde{x}^{j}=\tilde{x}^{j}\left(  x^{m}\right)$ (con $i,j=1,2,3,\cdots,n$)  se cumple que:
\[
a_{ij}\ x^{i}x^{j}=\psi=\tilde{\psi}=\tilde{a}_{ij}\ \tilde{x}^{i}\tilde{x}^{j}\,,\, \text{\quad donde }\psi=\tilde{\psi}\text{ constituye un escalar}\,,
\]
y por lo tanto, derivando y utilizando la regla de la cadena:
\begin{align*}
x^{i} &  =x^{i}\left(  \tilde{x}^{j}\left(  x^{m}\right)  \right) \,\, \Rightarrow  \,\,   
\frac{\partial x^{i}}{\partial x^{m}} =\frac{\partial x^{i}}{\partial \tilde{x}^{j}}\frac{\partial \tilde{x}^{j}}{\partial x^{m}}=\delta_{m}^{i} \,,
\end{align*}
por lo que:
\begin{align*}
a_{ij}\ x^{i}x^{j}-\tilde{a}_{ij}\ \tilde{x}^{i}\tilde{x}^{j} &  \equiv\left(  a_{ij}\ -\tilde{a}_{kl}\ \frac{\partial \tilde{x}^{k}}{\partial x^{i}}\frac{\partial \tilde{x}^{l}}{\partial x^{j}}\right) x^{i}x^{j}=0 \,,
\end{align*}
como hay una suma en $ij$ no se puede afirmar que la cantidad del paréntesis se anula. Como esta afirmación vale para cualquier sistema de coordenadas,  seleccionaremos las componentes coordenadas en la base canónica.
\[
x^{1}=\left(  1,0,0,\cdots,0\right)\,, \, \text{\quad}x^{2}=\left(  0,1,0,\cdots ,0\right)\,, \, 
\cdots\, \cdots x^{n}=\left(  0,0,0,\cdots,1\right) \,,
\]
con lo cual:
\[
a_{11}\ -\tilde{a}_{kl}\ \frac{\partial \tilde{x}^{k}}{\partial x^{1}} \frac{\partial \tilde{x}^{l}}{\partial x^{1}}=0\,, \, 
a_{22} \ -\tilde{a}_{kl} \frac{\partial \tilde{x}^{k}}{\partial x^{2}} \frac{\partial \tilde{x}^{l}}{\partial x^{2}}=0\,, \, \cdots \,, \, \cdots \,
a_{nn}\ -\tilde{a}_{kl}\ \frac{\partial \tilde{x}^{k}}{\partial x^{n}} \frac{\partial \tilde{x}^{l}}{\partial x^{n}}=0 \,,
\]

Como siempre podemos hacer: 
$\tilde{a}_{\left(  kl\right)  }=\frac{1}{2}\left(\tilde{a}_{kl}+\tilde{a}_{lk}\right)  $ y $\tilde{a}_{\left[  kl\right] }=\frac{1}{2}\left(  \tilde{a}_{kl}-\tilde{a}_{lk}\right)$ y separar el tensor:
\begin{gather*}
\tilde{a}_{kl}=\tilde{a}_{\left(  kl\right)  }+\tilde{a}_{\left[  kl\right]} 
\,\, \Rightarrow \,\,  
a_{\left( mm\right)  }\ -\left( \tilde{a}_{\left(  kl\right)  }+\tilde{a}_{\left[  kl\right]  }\right) \ \frac{\partial\tilde{x}^{k}}{\partial x^{m}}\frac{\partial \tilde{x}^{l}}{\partial x^{m}}=0 \,\, \Rightarrow \,\,  
a_{\left( mm\right)  }=\tilde{a}_{\left(  kl\right)  }\ \frac{\partial \tilde{x}^{k}}{\partial x^{m}}\frac{\partial \tilde{x}^{l}}{\partial x^{m}} \,.
\end{gather*}

La parte simétrica del tensor transforma siguiendo la regla: $a_{mn}=\Lambda_{m}^{k}\Lambda_{n}^{l}{\tilde a}_{kl}$ y es lo que de ahora en adelante denominaremos un tensor de segundo orden. En este caso, la parte simétrica de un tensor transforma como un verdadero tensor una vez que se contrae con un par de vectores. 
%%%%%%%%%
%%%%%%%%%
\subsection{Vectores, formas, tensores y leyes de transformación}
\label{TransformaVectoresTensores}
\index{Tensor!Transformación}
\index{Transformación!Tensor}
\index{Vector!Transformación}
\index{Transformación!Vector}

En esta sección discutiremos la importancia de caracterizar objetos identificando las leyes de transformación. La invariancia bajo determinadas leyes de transformación es clave en Física y nos permite identificar propiedades fundamentales. Ya hemos visto, en la sección \ref{PseudoCantidades}, como los esquemas de transformación de coordenadas nos han permitido diferenciar escalares y pseudoescalares y vectores de pseudovectores. En esta sección puntualizaremos como transforman vectores y formas y como esos esquemas de transformaciones los heredan los tensores.

%%%%%%%%%%%%
\subsubsection{Transformaciones y las coordenadas}
\label{TransformacionCoordenadas}
\index{Coordenadas!Transformaciones}
\index{Transformación!Coordenadas}
En general las afirmaciones anteriores se pueden generalizar considerando que las coordenadas que definen un determinado punto, $P,$ expresado en un sistema de coordenadas particular, son $\left(  x^{1},x^{2},\cdots,x^{n}\right)  $ y las coordenadas de ese mismo punto $P,$ expresado en otro sistema de
coordenadas son $\left(  \tilde{x}^{1},\tilde{x}^{2},\cdots,\tilde{x}^{n}\right)$.  Ambas coordenadas estarán relacionadas por:
\[
\left.
\begin{array}
[c]{c}
\tilde{x}^{1}=\tilde{x}^{1}\left(  x^{1},x^{2},\cdots,x^{n}\right) \\
\tilde{x}^{2}=\tilde{x}^{2}\left(  x^{1},x^{2},\cdots,x^{n}\right) \\
\vdots\\
\tilde{x}^{n}=\tilde{x}^{n}\left(  x^{1},x^{2},\cdots,x^{n}\right)
\end{array}
\right\}  \,\,\Longleftrightarrow\,\, \left\{
\begin{array}
[c]{c}
x^{1}=x^{1}\left(  \tilde{x}^{1},\tilde{x}^{2},\cdots,\tilde{x}^{n}\right) \\
x^{2}=x^{2}\left(  \tilde{x}^{1},\tilde{x}^{2},\cdots,\tilde{x}^{n}\right) \\
\vdots\\
x^{n}=x^{n}\left(  \tilde{x}^{1},\tilde{x}^{2},\cdots,\tilde{x}^{n}\right)
\end{array}
\right.
\]
En una notación más compacta lo que tenemos es: 
\begin{equation}
\label{EcTransfCoord}
\tilde{x}^{i}=\tilde{x}^{i}\left(  x^{j}\right)   \,\, \Longleftrightarrow \,\,
x^{i}=x^{i}\left(  \tilde{x}^{j}\right) \,, \quad \mbox{con } \,\,   i,j=1,2,3,\cdots,n \,.
\end{equation} 

Retomemos lo que discutimos en la sección \ref{VectoresLeyesTransformacion}, pero ahora en el lenguaje de coordenadas. En esa oportunidad mostramos como transformaban las bases y las componentes de formas y vectores. Ahora lo generalizaremos a los tensores. Otra vez, sólo exigiremos (y es bastante) que:

\begin{enumerate}
\item  Las funciones $x^{i}=x^{i}\left(  \tilde{x}^{m}\right)  $ y $\tilde{x}^{j}=\tilde{x}^{j}\left(  x^{m}\right)  $ sean al menos $\mathcal{C}^{2}$
(función y derivada continua)
\index{Jacobiano}
\index{Matriz!Jacobiana}
\item  Que el determinante de la matriz jacobiana sean finito y distinto de cero, esto es
\[
\det\left|  \frac{\partial x^{i}\left(  \tilde{x}^{m}\right)  }{\partial \tilde{x}^{j}}\right|  \neq 0 
\,\, \Rightarrow  \,\, 
\left|
\begin{array}
[c]{cccc}
\frac{\partial x^{1}}{\partial \tilde{x}^{1}} & \frac{\partial x^{1}}{\partial \tilde{x}^{2}} & \cdots & \frac{\partial x^{1}}{\partial \tilde{x}^{n}}\\ \\
\frac{\partial x^{2}}{\partial \tilde{x}^{1}} & \frac{\partial x^{2}}{\partial \tilde{x}^{2}} & \cdots &   \frac{\partial x^{2}}{\partial \tilde{x}^{n}}  \\
\vdots & \vdots &  & \vdots\\
\frac{\partial x^{n}}{\partial \tilde{x}^{1}} & \frac{\partial x^{n}}{\partial \tilde{x}^{2}} & \cdots & \frac{\partial x^{n}}{\partial \tilde{x}^{n}}
\end{array}
\right|  \neq0 \,\, \Rightarrow  \,\, x^{i}=x^{i}\left(  \tilde{x}^{m}\right)
\,\, \Longleftrightarrow\,\, \tilde{x}^{j}=\tilde{x}^{j}\left(x^{m}\right) \,.
\]
\end{enumerate}

Ahora bien, una vez más, derivando y utilizando la regla de la cadena:
\[
x^{i}=x^{i}\left(  \tilde{x}^{j}\left(x^{m}\right)  \right)  
\,\, \Rightarrow  \,\,  \frac{\partial x^{i}}{\partial x^{m}}=\frac{\partial x^{i}}{\partial \tilde{x}^{j}}\frac{\partial \tilde{x}^{j}}{\partial x^{m}} = \delta_{m}^{i} \,\, \Rightarrow  \,\, 
\mathrm{d} x^{i}=\frac{\partial x^{i}}{\partial \tilde{x}^{j}}\mathrm{d} \tilde{x}^{j}\,,
\]
como hemos comprobado para los dos casos particulares estudiados con anterioridad. 

De ahora en adelante tendremos las siguientes \textbf{ReDefiniciones}:
\begin{itemize}
\item Un conjunto de cantidades $\left\{a^{1},a^{2},\cdots,a^{n}\right\}$ se denominarán componentes \textit{contravariantes} de un vector $\left|  a\right> \in
{\bf V}$ en un punto $P$ de coordenadas $\left( x^{1},x^{2},\cdots,x^{n}\right)$, si:

\begin{enumerate}
\item  dada dos bases ortonormales de vectores coordenados: $\left\{  \left| \mathrm{{e}}_{1}\right>,\left|  \mathrm{{e}}_{2}\right>, \cdots , \left|  \mathrm{{e}}_{n}\right> \right\}  $ y $\left\{  \left| {\tilde{\mathrm{e}}}_{1}\right>, \left|  {\tilde{\mathrm{e}}}_{2}\right>, \cdots , \left|  {\tilde{\mathrm{e}}}_{n}\right> \right\} $, se cumple que:
\[
\left|  a\right> =a^{i}\left| {\mathrm{e}}_{i}\right> =\tilde{a}^{i}\left|  {\tilde{\mathrm{e}}}_{i}\right> 
\,\, \Rightarrow  \,\,
\left\{
\begin{array}
[c]{c}
\left< {\mathrm{e}}^{i}\right|  \left.  a\right> =a^{i}\\
\left< {\tilde{\mathrm{e}}}^{i}\right|  \left.  a\right> =\tilde{a}^{i}
\end{array}
\right\}  \,\, \Rightarrow  \,\,  \tilde{a}^{i}=a^{j} \left< {\tilde{\mathrm{e}}}^{i}\right.  \left|  {\mathrm{e}}_{j}\right>. 
\]

\item  o equivalentemente, bajo una transformación de coordenadas: $x^{i}=x^{i}\left(  \tilde{x}^{j}\right)$, con $i,j=1,2,3,\cdots,n$, estas cantidades transforman como:
\begin{equation}
\label{TransfCompContravariantes}
\tilde{a}^{i}=\frac{\partial \tilde{x}^{i}}{\partial x^{k}}\ a^{k} \,\, \Longleftrightarrow\,\, a^{i}=\frac{\partial x^{i}}{\partial \tilde{x}^{k}}\ \tilde{a}^{k}\,, \quad \text{con: } \, \frac
{\partial x^{i}}{\partial \tilde{x}^{k}}\frac{\partial \tilde{x}^{k}}{\partial x^{l}}=\delta_{l}^{i} \,,
\end{equation}
y donde las cantidades $\frac{\partial\tilde{x}^{i}}{\partial x^{k}}$\ y $\frac{\partial x^{i}}{\partial \tilde{x}^{k}}$ deberán ser evaluadas en el punto $P$.
\end{enumerate}

\item Un conjunto de cantidades $\left\{ b_{1},b_{2},\cdots,b_{n}\right\} $ se denominarán componentes \textit{covariantes} de un vector $\left< b\right|  \in {\bf V}^{\ast}$ en un punto $P $ de coordenadas $\left(  x^{1},x^{2},\cdots,x^{n}\right)$, si:

\begin{enumerate}
\item  dada dos bases de formas: $\left\{  \left< {\mathrm{e}}^{1}\right|, \left< {\mathrm{e}}^{2}\right|,\cdots , \left< {\mathrm{e}}^{n}\right|  \right\} $ y 
$\left\{  \left< {\tilde{\mathrm{e}}}^{1}\right|, \left< {\tilde{\mathrm{e}}}^{2}\right|, \cdots , \left< {\tilde{\mathrm{e}}}^{n}\right|  \right\}$ se cumple que:
\[
\left< b\right|  =b_{i}\left< {\mathrm{e}}^{i}\right|  =\tilde{b}_{i}\left< {\tilde{\mathrm{e}}}^{i}\right|  
\,\, \Rightarrow  \,\,
\left\{
\begin{array}
[c]{c}
\left< b \right|  \left.  {\mathrm{e}}^{i}\right> =b^{i}\\ \left< b\right|  \left.  {\tilde{\mathrm{e}}}^{i}\right> =\tilde{b}^{i}
\end{array}
\right\}\,\, \Rightarrow  \,\, \tilde{b}^{i}=b^{j} \left< {\mathrm{e}}_{j}\right.  \left|  {\tilde{\mathrm{e}}}^{i}\right> \,.
\]

\item  o equivalentemente, bajo una transformación de coordenadas $x^{i}=x^{i}\left(  \tilde{x}^{j}\right) $ (con $i,j=1,2,3,\cdots,n$) estas cantidades transforman como:
\begin{equation}
\label{TransfCompCovariantes}
\tilde{b}_{k}=\frac{\partial x^{i}}{\partial \tilde{x}^{k}}\ b_{i}
\,\, \Longleftrightarrow\,\, b_{k}=\frac{\partial \tilde{x}^{i}}{\partial x^{k}}\ \tilde{b}_{i} \,, \,
\text{con: } \, \frac{\partial x^{i}}{\partial \tilde{x}^{k}}\frac{\partial \tilde{x}^{k}}{\partial x^{l}}=\delta_{l}^{i}\,,
\end{equation}
y donde las cantidades: $\frac{\partial \tilde{x}^{i}}{\partial x^{k}}$ y $\frac{\partial x^{i}}{\partial \tilde{x}^{k}}$ deberán ser evaluadas en el punto $P$.
\end{enumerate}

\item Generalizamos los conceptos anteriores de la siguiente manera. Dado un conjunto bases para las formas diferenciales $\left\{  \left< x^{m}(1)\right|  ,\left< y^{n}(2)\right| \right\}$, hemos definido las componentes \textit{contravariantes} de un tensor:
\[
T^{ij}=\mathcal{T}\left[  \overset{\left< x^{i}(1)\right|  }{\overset{\downarrow}{\bullet}},\overset{\left< y^{j}(2)\right|
}{\overset{\downarrow}{\bullet}}\right]  \,\,\Longleftrightarrow\,\,
\left\{  T^{ij}\right\}  \equiv\left\{  T^{11},T^{12},\cdots,T^{1n},T^{21},T^{22},\cdots,T^{2n},\cdots,T^{nn}\right\} \,,
\]
en esta visión, las componentes contravariantes en un punto $P$ de coordenadas 
$\left(x^{1},x^{2},\cdots,x^{n}\right) \Leftrightarrow x^{i}=x^{i}\left(\tilde{x}^{j}\right)$ (con $i,j=1,2,3,\cdots,n$) transforman como:
\[
\tilde{T}^{ij}=\frac{\partial \tilde{x}^{i}}{\partial x^{k}}\frac{\partial \tilde{x}^{j}}{\partial x^{m}}\ T^{km} \,\, \Longleftrightarrow \,\, 
T^{ij}=\frac{\partial x^{i}}{\partial \tilde{x}^{k}}\frac{\partial x^{j}}{\partial \tilde{x}^{m}}\ \tilde{T}^{km} \,, \, \text{con: } \,  
\frac{\partial x^{i}}{\partial \tilde{x}^{k}}\frac{\partial \tilde{x}^{k}}{\partial x^{l}}=\delta_{l}^{i} \,,
\]
donde $\frac{\partial \tilde{x}^{i}}{\partial x^{k}}$ y $\frac{\partial x^{i}}{\partial \tilde{x}^{k}}$ deberán ser evaluadas en el punto $P$. 

Esto nos permite construir el caso más general.

\item Si $\left\{  \left|  t_{i}(1)\right>,\left|  u_{j}(2)\right> ,\cdots,\left|  v_{k}(m)\right> \right\}$ y $\left\{  \left< x^{e}(1)\right|  ,\left< y^{f}(2)\right|,\cdots,\left< z^{g}(n)\right|  \right\}$ son bases para los vectores y las formas, respectivamente, las componentes de un tensor:
\[
T_{ijk}^{mn}=\mathcal{T}\left[  
\overset{\left|  t_{i}(1)\right>}{\overset{\downarrow}{\circ}},\overset{\left|  u_{j}(2)\right>}{\overset{\downarrow}{\circ},},\cdots,\overset{\left|  v_{k}(m)\right>}{\overset{\downarrow}{\circ}};
\overset{\left< x^{e}(1)\right|}{\overset{\downarrow}{\bullet}},\overset{\left< y^{f}(2)\right|}{\overset{\downarrow}{\bullet}},\cdots,\overset{\left< z^{g}(n)\right|}{\overset{\downarrow}{\bullet}}
\right]\,,
\]
serán un conjunto de cantidades: $\left\{  T_{1\cdots1}^{1\cdots1},T_{1\cdots1}^{2\cdots1},\cdots,T_{1\cdots1}^{\cdots1},T_{1\cdots1}^{\tilde{n}\cdots1},T_{2\cdots1}^{\tilde{n}\cdots1},\cdots,T_{\tilde{m}\cdots1}^{1\cdots
1},\cdots,T_{\tilde{m}\cdots\tilde{m}}^{\tilde{n}\cdots\tilde{n}}\right\}$ que 
se denominarán las componentes \textit{contravariantes} y \textit{covariantes} respectivamente, de un tensor mixto en un punto $P$ de coordenadas $\left( x^{1},x^{2},\cdots,x^{n}\right)$. 

Bajo una transformación de coordenadas $x^{i}=x^{i}\left(  \tilde{x}^{j}\right) $ (con $i,j=1,2,3,\cdots,n$) estas cantidades transforman como:
\begin{align}
\label{TransfCompTensores}
\tilde{T}_{e\cdots g}^{i\cdots k}  &  =\frac{\partial \tilde{x}^{i}}{\partial x^{p}}\cdots\frac{\partial \tilde{x}^{k}}{\partial x^{q}}
\frac{\partial x^{a}}{\partial \tilde{x}^{e}}\cdots\frac{\partial x^{d}}{\partial \tilde{x}^{g}}\ T_{a\cdots d}^{p\cdots q} 
\,\, \Longleftrightarrow \,\, 
T_{e\cdots g}^{i\cdots k}=\frac{\partial x^{i}}{\partial \tilde{x}^{p}}\cdots\frac{\partial x^{k}}{\partial \tilde{x}^{q}}\frac{\partial \tilde{x}^{a}}{\partial x^{e}}\cdots\frac{\partial \tilde{x}^{d}}{\partial x^{g}}\ T_{a\cdots d}^{p\cdots q} \,,
\end{align}
nuevamente con: 
$\frac{\partial x^{i}}{\partial \tilde{x}^{k}}\frac{\partial \tilde{x}^{k}}{\partial x^{l}}=\delta_{l}^{i}$ y donde las cantidades
$\frac{\partial \tilde{x}^{i}}{\partial x^{k}}$\ y $\frac{\partial x^{i}}{\partial \tilde{x}^{k}}$ deberán ser evaluadas en el punto $P$.
\end{itemize}

\subsection{Un par de tensores en Física: Esfuerzos e Inercia}
\label{unpardetensores}
En esta sección vamos a ejemplificar la utilización de los tensores en varios ámbitos de la Física, en particular de la Mecánica. En la próxima sección consideraremos el tensor de esfuerzos para describir las tensiones internas de cuerpos sometidos a fuerzas externas\footnote{Una presentación más contemporánea del tensor de esfuerzos la pueden encontrar en De Prunelé, E. (2007). Linear strain tensor and differential geometry. American Journal of Physics, 75(10), 881-887.}. Haremos el análisis tanto para el caso de dos como de tres dimensiones. Luego a continuación consideraremos el tensor de inercia y su impacto en la dinámica de cuerpos en movimiento. 

\subsubsection{1. El tensor de esfuerzos (stress)}
\label{TensorEsfuerzos}
\index{Tensor!de esfuerzos}
\index{Esfuerzo!Tensor de}
\index{Tensor!Stress}


\subsubsection{El caso 2D}
\index{Tensor!Esfuerzo 2D}

Supongamos un cuerpo que se encuentra en equilibrio y está sometido a un conjunto de fuerzas externas. Para facilitar las cosas consideremos el efecto de esas fuerzas sobre un plano que contiene a un determinado punto $P$ (ver figura \ref{figtensoresfuerzos2d} cuadrante Ia). Es decir, vamos a considerar los efectos de las componentes de todas las fuerzas sobre ese plano y obviaremos el efecto del resto de las componentes. 

Como observamos en la figura \ref{figtensoresfuerzos2d} Ib y Ic, si cortamos la superficie en dos líneas ($AB$ y $A^{\prime}B^{\prime}$), podemos ver que el efecto del conjunto de fuerzas externas es distinto sobre $P$, en la dirección perpendicular a cada una de esas líneas. De hecho, al ``cortar'' la superficie las fuerzas que aparecen sobre las líneas $AB$ (y $A^{\prime}B^{\prime}$)  eran fuerzas internas y ahora son externas al nuevo cuerpo ``cortado''. Así, estas fuerzas por unidad de longitud\footnote{En el caso tridimensional, las fuerzas que generan los esfuerzos serán definidas como fuerzas por unidad de área. Ese caso lo veremos en la próxima sección.} sobre el punto $P$ existen como un conjunto de fuerzas que generan esfuerzos (\textit{stress}). Por lo tanto, es claro que los esfuerzos sobre un punto dependen del punto, de las fuerzas externas y de la dirección del efecto.

Para irnos aclarando consideremos un elemento de área infinitesimal $\mathrm{d}s$ sobre la cual actúa un conjunto de fuerzas externas, las cuales  podemos descomponer como normales y tangenciales a la línea sobre la cual están aplicadas (ver figura \ref{figtensoresfuerzos2d} cuadrante II). Es costumbre denotar los esfuerzos normales y tangenciales por $\sigma$ y $\tau$ respectivamente.
\[
\mathrm{d}A=\mathrm{d}x\mathrm{d}y \quad \Rightarrow  
\left\{
\begin{array}
[c]{ccc}
&
\begin{array}
[c]{cc}
\uparrow Y_{2}=\sigma_{2}\mathrm{d}x & \longrightarrow X_{2}=\tau
_{2}\mathrm{d}x
\end{array}
& \\
\begin{array}
[c]{c}
Y_{3}=\tau_{3}\mathrm{d}y\uparrow\\
X_{3}=\sigma_{3}\mathrm{d}y\rightarrow
\end{array}
&
\begin{array}
[c]{ccc}
& \mathrm{d}x & \\
\mathrm{d}y & \mathrm{d}s & \mathrm{d}y\\
& \mathrm{d}x &
\end{array}
&
\begin{array}
[c]{c}
\uparrow Y_{1}=\tau_{1}\mathrm{d}y\\
\rightarrow X_{1}=\sigma_{1}\mathrm{d}y
\end{array}
\\
&
\begin{array}
[c]{cc}
\uparrow Y_{4}=\sigma_{4}\mathrm{d}x & \rightarrow X_{4}=\tau_{4}\mathrm{d}x
\end{array}
&
\end{array}
\right.
\]

Consideramos la segunda ley de Newton aplicada a cada diferencial de masa $\mathrm{d}m$ y obtendremos:
\[
\sum{\bf F}_{i}^{ext}=\mathrm{d}m\ {\bf a}=0 \quad \Rightarrow  \left\{
\begin{array}
[c]{c}
\tau_{1}\mathrm{d}y+\sigma_{2}\mathrm{d}x+\tau_{3}\mathrm{d}y+\sigma_{4}\mathrm{d}x=0=\left(  \sigma_{2}+\sigma_{4}\right)  \mathrm{d}x+\left(\tau_{1}+\tau_{3}\right)  \mathrm{d}y \\
\\
\sigma_{1}\mathrm{d}y+\tau_{2}\mathrm{d}x+\sigma_{3}\mathrm{d}y+\tau_{4}\mathrm{d}x=0=\left(  \tau_{2}+\tau_{4}\right) \mathrm{d}x+\left(
\sigma_{1}+\sigma_{3}\right)  \mathrm{d}y
\end{array}
\right.
\]
con lo cual:
\[
\sigma_{2}=-\sigma_{4}\,,\,\,  \tau_{1}=-\tau_{3} \,, \,\,
\tau_{2}=-\tau_{4} \,, \,\,  \sigma_{1}=-\sigma_{3} \,.
\]

Como se trata de una situación en equilibrio, también la sumatoria de
torques se tendrá que anular. Esto significa que:
\[
\left.
\begin{array}
[c]{c}
\left(  \tau_{1}\mathrm{d}y\right)  \frac{\mathrm{d}x}{2}-\left(  \tau
_{2}\mathrm{d}x\right)  \frac{\mathrm{d}y}{2}=0\\
\\
\left(  \tau_{3}\mathrm{d}y\right)  \frac{\mathrm{d}x}{2}-\left(  \tau
_{4}\mathrm{d}x\right)  \frac{\mathrm{d}y}{2}=0
\end{array}
\right\}  \quad \Rightarrow   \tau_{1}=\tau_{2}=\tau_{3}=\tau_{4} \,,
\]
entonces, nos damos cuenta que existen sólo tres cantidades
independientes: dos esfuerzos normales $\sigma_{1}$ y $\sigma_{2};$ y un esfuerzo tangencial $\tau_{1}.$ Adicionalmente notamos que los esfuerzos tienen que ver con la dirección de la fuerza y la superficie sobre la cual va aplicada. Con ello podemos diseñar la siguiente notación para los esfuerzos: $\sigma_{ij}.$ El primer índice indica la dirección de la fuerza y el segundo la dirección de la normal de la superficie donde está aplicada. Así, tal y como muestra la figura (ver figura \ref{figtensoresfuerzos2d} cuadrante II)
\[
\sigma_{1}\equiv\sigma_{xx}\,,\quad
-\sigma_{4}\equiv\sigma_{yy}\,,\quad
\tau_{2}\equiv\sigma_{xy}\equiv\sigma_{yx} \,.
\]

\begin{figure}[t]
\begin{center}
\includegraphics[width=5.8in]{VOLUMEN_1/03_Funciones_Lineales/Figuras/Figura3_1.jpg}
\caption{Tensor de Esfuerzos (\textit{stress}) en 2 dimensiones}
\label{figtensoresfuerzos2d}
\end{center}
\end{figure}

El cambio de signo se debe a lo incómodo de la notación: $\sigma
_{4}\equiv\sigma_{y-y}$ ya que la normal de lado $4$ apunta en la dirección $-y$. Es importante también señalar que los esfuerzos en cualquier punto contenido en el diferencial de área $\mathrm{d} A=\mathrm{d}x\mathrm{d}y$ deben ser considerado constantes, o lo que es lo mismo, que podemos hacer tender a cero el área del diferencial y con ello asociar los esfuerzos $\sigma_{ij}$ a un punto $P$ contenido en $\mathrm{d}A$ sobre la cual hemos calculado los esfuerzos.

En esta misma línea de razonamiento, nos podemos preguntar cuál es la
expresión de los esfuerzos cuando se miden respecto a una superficie
genérica, definida por un vector normal ${\bf n}$ (ver figura
\ref{figtensoresfuerzos2d} cuadrante III). Es decir, queremos conocer los esfuerzos medidos en el punto $P$ y en la dirección ${\bf n}$, es decir, $\sigma_{nn}$. 

Por lo tanto, tendremos que:
\begin{eqnarray*}
x&\rightarrow&\sigma_{xx}\mathrm{d}y+\sigma_{xy}\mathrm{d}x=
\sigma_{nn}\mathrm{d}s\cos(\phi)+\sigma_{sn}\mathrm{d}s \ \mbox{sen}(\phi)\\
y&\rightarrow&\sigma_{yy}\mathrm{d}x+\sigma_{yx}\mathrm{d}y=
\sigma_{nn}\mathrm{d}s \ \mbox{sen}(\phi)-\sigma_{sn}\mathrm{d}s\cos(\phi)
\end{eqnarray*}

Ahora bien, dado que $\mathrm{d}y=\mathrm{d}s\cos(\phi)$ y $\mathrm{d}x=\mathrm{d}s \ \mathrm{sen}(\phi)$, entonces podemos expresar:
\begin{eqnarray*}
\sigma_{nn}  &  = &\sigma_{xx}\cos^{2}(\phi)+\sigma_{xy}\mbox{sen}(\phi)\cos(\phi)+\sigma_{yx}\mbox{sen}(\phi)\cos(\phi)+\sigma_{yy}
\mbox{sen}^{2}(\phi) \\
\sigma_{sn}  &  = &\sigma_{xx}\mbox{sen}(\phi)\cos(\phi)+\sigma_{xy}\mbox{sen}^{2}(\phi)-\sigma_{yx}\cos^{2}(\phi)-\sigma_{yy}\mbox{sen}(\phi)\cos(\phi)
\end{eqnarray*}

y si ahora nos damos cuenta que si construimos una matriz:
\[
A_{j}^{i}=\left(
\begin{array}
[c]{cc}
A_{n}^{x} &  A_{s}^{x}\\
A_{n}^{y}&  A_{s}^{y}
\end{array}
\right)  = 
\left(
\begin{array}
[c]{cc}
\cos(\phi) &  \operatorname{sen}(\phi)\\
\operatorname{sen}(\phi) & -\cos(\phi)
\end{array}
\right) \,,
\]
entonces:
\begin{align*}
\sigma_{nn}  &  =A_{n}^{x}A_{n}^{x}\sigma_{xx}+A_{n}^{x}A_{n}^{y}\sigma
_{xy}+A_{n}^{y}A_{n}^{x}\sigma_{yx}+A_{n}^{y}A_{n}^{y}\sigma_{yy}
\quad \Rightarrow \quad \sigma_{nn}=A_{n}^{i}A_{n}^{j}\sigma_{ij}\quad\text{con
}i,j=n,s\\
& \\
\sigma_{sn}  &  =A_{s}^{x}A_{n}^{x}\sigma_{xx}+A_{s}^{x}A_{n}^{y}\sigma
_{xy}+A_{s}^{y}A_{n}^{x}\sigma_{yx}+A_{s}^{y}A_{n}^{y}\sigma_{yy}
\quad \Rightarrow \quad\sigma_{sn}=A_{s}^{i}A_{n}^{j}\sigma_{ij}\quad\text{con
}i,j=n,s 
\end{align*}
es decir: 
\[
\sigma_{kl}=A_{k}^{i}A_{l}^{j}\sigma_{ij}\,, \quad \mbox{con} \quad  i,j,k,l=n,s \,.
\]

Como ya hemos mencionado, y veremos más adelante con más detalle, cualquier objeto que transforme como $\sigma_{kl}=A_{k}^{i}A_{l}^{j}\sigma_{ij}$ lo llamaremos tensor de segundo orden.

\begin{figure}[t]
\begin{center}
\includegraphics[width=6.0in]{VOLUMEN_1/03_Funciones_Lineales/Figuras/Figura3_2.jpg}
\caption{Tensor de Esfuerzos en 3 dimensiones}
\label{figtensoresfuerzos3d}
\end{center}
\end{figure}

\subsubsection{El caso 3D}
\index{Tensor!Esfuerzo 3D}
Podemos  proceder como en el caso anterior estableciendo las siguientes condiciones de equilibrio:
\[
\sum{\bf F}_{i}^{ext}=0\quad\text{y}\quad\sum{\boldsymbol{\tau}}_{i}^{ext}=0\,,
\]
con ello construimos un volumen (cúbico) diferencial y construimos los
esfuerzos normales y tangenciales, los cuales serán:
\[
\sigma_{xx}\mathrm{d}y\mathrm{d}z\,, \quad
\sigma_{yy}\mathrm{d}x\mathrm{d}z\,, \quad
\sigma_{zz}\mathrm{d}x\mathrm{d}y\,, \quad
\sigma_{xz}\mathrm{d}x\mathrm{d}y\,, \quad
\sigma_{yz}\mathrm{d}x\mathrm{d}y\,, \quad
\sigma_{xy}\mathrm{d}x\mathrm{d}z\,.
\]

Siguiendo el mismo proceso que involucra imponer el equilibrio, es fácil
demostrar que al igual que el caso anterior, el tensor de esfuerzos
$\sigma_{ij}$ cumple con:
\[
\sigma_{xz}=\sigma_{zx}\,, \quad
\sigma_{yz}=\sigma_{zy}\,, \quad
\sigma_{xy}=\sigma_{yx} \,.
\]

Tendremos $6$ componentes (tres normales y tres tangenciales)
independientes. Es decir, si bien el tensor de esfuerzos $\sigma_{ij}$ viene representado por una matriz $3\times3$ y por lo tanto tiene $9$ elementos, sólo 6 son independientes. 

Vayamos ahora el caso general para un tensor de esfuerzos en un medio elástico. Para ello construimos un tetraedro regular tal y como muestra la figura \ref{figtensoresfuerzos3d}, y sobre su cara genérica asociada a un vector normal ${\bf n}$ una fuerza ${\bf F}$:
\[
{\bf F}=F^{i}\text{\bf{i}}_{i}
=F_{x}\text{\bf{{i}}}+F_{y}\text{\bf{{j}}}+F_{z}
\text{\bf{{k}}}\,\, \Rightarrow \,\,  \left\{
\begin{array}
[c]{c}
F_{x}=\sigma_{xn}\mathrm{d}S_{n}\\
\\
F_{y}=\sigma_{yn}\mathrm{d}S_{n}\\
\\
F_{z}=\sigma_{zn}\mathrm{d}S_{n}
\end{array}
\right\} \,\, \Rightarrow \,\,   F^{i}=\sigma_{j}^{i}n^{j}\mathrm{d}S
\,\, \Rightarrow \,\,  {\bf F}=\boldsymbol{\sigma}\cdot\mathrm{d}{\bf S}\,.
\]
De ésta manera se especifica como la fuerza  actúa sobre un determinado elemento de superficie. Es claro que la condición de equilibrio se traduce en:
\begin{align*}
\sum F_{xi}  &  =0\,\, \Rightarrow \,\,  \sigma_{xn}\mathrm{d}S_{n}-\frac{1}{2}\sigma_{xx}\mathrm{d}y\ \mathrm{d}z-\frac{1}{2}\sigma_{xy}\mathrm{d}x\ \mathrm{d}z-\frac{1}{2}\sigma_{xz}\mathrm{d}x\ \mathrm{d}y=0 \,, \\
& \\
\sum F_{yi}  &  =0\,\, \Rightarrow \,\,  \sigma_{yn}\mathrm{d}S_{n}-\frac{1}{2}\sigma_{yx}\mathrm{d}y\ \mathrm{d}z-\frac{1}{2}\sigma_{yy}\mathrm{d}x\ \mathrm{d}z-\frac{1}{2}\sigma_{yz}\mathrm{d}x\ \mathrm{d}y=0 \,,  \\
& \\
\sum F_{zi}  &  =0\,\, \Rightarrow \,\,  \sigma_{zn}\mathrm{d}S_{n}-\frac{1}{2} \sigma_{zx}\mathrm{d}y\ \mathrm{d}z-\frac{1}{2}\sigma_{zy}\mathrm{d} x\ \mathrm{d}z-\frac{1}{2}\sigma_{zz}\mathrm{d}x\ \mathrm{d}y=0 \,.
\end{align*}

Si consideramos  la proyección de $\mathrm{d}S_{n}$ sobre cada uno de los planos del sistema cartesiano tendremos:
\[
\left.
\begin{array}
[c]{c}
\mathrm{d}S^{n}\cos\left(  \text{\bf{{i}}};{\bf n}\right)  =\frac{1}
{2}\mathrm{d}y\ \mathrm{d}z=\mathrm{d}S^{n}\ A_{n}^{x}\\
\\
\mathrm{d}S^{n}\cos\left(  \text{\bf{{j}}};{\bf n}\right)  =\frac{1}
{2}\mathrm{d}x\ \mathrm{d}z=\mathrm{d}S^{n}\ A_{n}^{y}\\
\\
\mathrm{d}S^{n}\cos\left(  \text{\bf{{k}}};{\bf n}\right)  =\frac{1}
{2}\mathrm{d}x\ \mathrm{d}y=\mathrm{d}S^{n}\ A_{n}^{z}
\end{array}
\right\}  \,\, \Rightarrow \,\,  \sigma_{xn}=\sigma_{xx}A_{n}^{x}+\sigma_{xy}
A_{n}^{y}+\sigma_{xz}A_{n}^{z}\,,
\]
y equivalentemente:
\[
\sigma_{yn}=\sigma_{yx}A_{n}^{x}+\sigma_{yy}A_{n}^{y}+\sigma_{yz}A_{n}^{z} \quad\text{y}\quad
\sigma_{zn}=\sigma_{zx}A_{n}^{x}+\sigma_{zy}A_{n}^{y}+\sigma_{zz}A_{n}^{z}\,,
\]
las cuales se conocen como las relaciones de Cauchy. Estas relaciones  representan los
esfuerzos sobre la superficie con normal ${\bf n}$. 

Ahora bien, dado que:
${\bf F}=\boldsymbol{\sigma}\mathrm{d}{S}$ es una relación vectorial
podemos proyectar en la dirección $\hat{\bf u}_{m}$:
\[
\hat{\bf  u}_{m}\cdot {\bf F}   =\hat{\bf u}_{m}\cdot \left( \boldsymbol{\sigma}\mathrm{d}{ S} \right)\,\, \Rightarrow \,\,   F^{m}=\sigma_{n}^{m}\mathrm{d}S^{n}=
\left(\sigma_{i}^{m}A_{n}^{i}\right)  \mathrm{d}S^{n}=\left(  \sigma_{i}^{m}
A_{n}^{i}\right)  \mathrm{d}S^{n}\,,
\]
por lo tanto:
\[
\sigma_{mn}\mathrm{d}S^{n}   =\left(  \sigma_{mi}A_{n}^{i}\right)
\mathrm{d}S^{n}\,\, \Rightarrow \,\,  \sigma_{mn}\mathrm{d}S^{n}=\left(  \sigma_{ki}A_{m}^{k}A_{n}^{i}\right)  \mathrm{d}S^{n}\,,
\quad\text{con } i,j=x,y,z \,.
\]

Una vez más, podemos ver  que transforma como un tensor.

\subsubsection{2. El Tensor de inercia}
\label{TensorInercia}
Consideremos el caso de un sistema de $n$ partículas. La cantidad de
movimiento angular para este sistema vendrá dada por:
\[
{\bf L}=\sum_{i}m_{\left(  i\right)  }\left(  {\bf r}_{\left(  i\right)
}\times{\bf v}_{\left(  i\right)  }\right) \,,
\]
donde hemos indicado que la $i-$ésima partícula que está en la
posición $\bf{r}_{\left(  i\right)  }$ tiene una velocidad ${\bf v}_{\left(  i\right) }$. 

Si las distancias entre las partículas, y entre las partículas con el origen de coordenadas, es constante, podremos expresar la velocidad de cada una de ellas como:
\[
{\bf v}_{\left(  i\right)  }=\boldsymbol{\omega}\times{\bf r}_{\left(  i\right)}\,,\quad \mbox{¿Por qué?}
\]

Donde $\boldsymbol{\omega}$ es la velocidad angular instantánea del sistema. Entonces tendremos que:
\[
{\bf L}=\sum_{i} m_{\left(  i\right)  }\left[  {\bf r}_{\left(  i\right)
}\times\left(  \boldsymbol{\omega}\times{\bf r}_{\left(  i\right)  }\right)  \right]
=\sum_{i}m_{\left(  i\right)  }\left[  \boldsymbol{\omega}\left(  {\bf r}_{\left(
i\right)  }\cdot{\bf r}_{\left(  i\right)  }\right)  -{\bf r}_{\left(
i\right)  }\left(  \boldsymbol{\omega}\cdot{\bf r}_{\left(  i\right)  }\right)
\right] \,,
\]
y para cada partícula se cumple que las componentes de la cantidad de movimiento angular serán:
\[
L^{k}=\sum_{i}m_{\left(  i\right)  }\left[  \omega^{k}\left(  r_{\left(
i\right)  }^{m}r_{\left(  i\right)  m}\right)  -r_{\left(  i\right)  }
^{k}\left(  \omega^{m}r_{\left(  i\right)  m}\right)  \right]\,.
\]

Si vemos que $\omega_{\left(  i\right)  }^{k}=\delta_{l}^{k}\omega_{\left(
i\right)  }^{l}$ entonces:
\[
L^{k}=  \sum_{i}m_{\left(  i\right)  }\left[  \delta_{l}^{k}\omega
^{l}\left(  r_{\left(  i\right)  }^{m}r_{\left(  i\right)  m}\right)
-r_{\left(  i\right)  }^{k}\left(  \omega^{m}r_{\left(  i\right)  m}\right)
\right]   =\omega_{\left(  i\right)  }^{l}\underset{I_{l}^{k}
}{\underbrace{\left[  \sum_{i}m_{\left(  i\right)  }\left(  \delta_{l}
^{k}\left(  r_{\left(  i\right)  }^{m}r_{\left(  i\right)  m}\right)
-r_{\left(  i\right)  }^{k}\left(  r_{\left(  i\right)  l}\right)  \right)
\right] }}
\]
es decir:
\[
L^{k}=\omega_{\left(  i\right)  }^{l}I_{l}^{k}\,, \quad\text{donde: } \quad\, 
I_{l}^{k}=\sum_{i}m_{\left(  i\right)  }\left(  \delta_{l}^{k}\left(  r_{\left(
i\right)  }^{m}r_{\left(  i\right)  m}\right)  -r_{\left(  i\right)  }
^{k}\left(  r_{\left(  i\right)  l}\right)  \right)\,.
\]

El objeto $I_{l}^{k}$ se conoce como el tensor de inercia y corresponde a $9$ cantidades (sólo $6$ son independientes porque es un tensor simétrico). En coordenadas cartesianas se verá de la siguiente forma:
\[
I_{l}^{k}=\left(
\begin{array}
[c]{lllll}
I_{xx}   &  & I_{xy}  &  &I_{xz}   \\
            &  &            &  & \\
I_{yx}   &  & I_{yy}  &  &I_{yz}  \\
            &  &            &  & \\
I_{zx}   &  & I_{zy}  &  & I_{zz}
\end{array}
\right) = 
\left(
\begin{array}
[c]{lllll}
\sum_{i}m_{(i)}\left(y_{(i)} ^{2}+z_{(i)}^{2} \right)  &  & -\sum_{i}m_{(i)  }\left(x_{(i)}y_{(i)}\right)             &  & -\sum_{i}m_{(i)}\left(x_{(i)}z_{(i)}\right)  \\
                                                                             &  &                                                                            &  & \\
-\sum_{i} m_{(i)}\left(x_{(i)}y_{(i)}\right)                &  & \sum_{i}m_{(i)}\left(x_{(i)}^{2}+z_{(i)}^{2}\right)  &  & -\sum_{i}m_{(i)}\left(y_{(i)}z_{(i)}\right)  \\
                                                                             &  &                                                                            &  & \\
-\sum_{i}m_{(i)}\left(x_{(i)}z_{(i)}\right)                 &  &-\sum_{i}m_{(i)}\left(y_{(i)}z_{(i)}\right)                 &  & \sum_{i}m_{(i)}\left(z_{(i)}^{2}+y_{(i)}^{2}\right)
\end{array}
\right) 
\]

Por ahora, nos contentaremos en suponer que esta construcción es un tensor y lo demostraremos más adelante.

\subsection{{\color{Fuchsia}Ejemplos}} 
\begin{enumerate}
\item {\bf Productos tensoriales, vectores y matrices:} 
Consideremos el conjunto de matrices $\left\{\mathbf{M}_{mn}\right\}$ que constituyen un espacio vectorial para la suma y multiplicación por los números reales, además, la dimensión de este espacio vectorial es: dim$(\mathbf{M}_{mn})=m\cdot n$. 
 
En este espacio vectorial tenemos los siguientes elementos:
\[
\left| \mathrm{v}\right> = \left(
\begin{array}
[c]{r}
x_1\\
x_2\\
x_3
\end{array}
\right) \,\, \in \mathbf{M}_{31} \quad \mbox{y} \quad
 \left< \mathrm{u} \right|  = \left(y_1 \,\, y_2\,\, y_3 \right) \,\, \in \mathbf{M}_{13} \, ,
\]
donde $\mathbf{M}_{mn}$ indica matrices $m \times n$ sobre los números reales $\mathds{R}$, y por lo tanto, tenemos que una columna de la matriz, es una matriz $3 \times 1$ y una fila una $1 \times 3$. Es claro --y la notación que utilizamos así lo indica-- que  $\mathbf{M}_{31}$ es un vector y que $\mathbf{M}_{13}$ una $1-$forma.

Podemos entonces definir la siguiente operación: 
\[
 \left|  \mathrm{v} \right. \left. \mathrm{u} \right> =
 \left(
\begin{array}
[c]{r}
x_1\\
x_2\\
x_3
\end{array}
\right) \otimes \left(y_1 \ y_2 \ y_3 \right)=
\left(
\begin{array}
[c]{ccc}
x_1y_1 & x_1y_2 & x_1y_3 \\
x_2y_1 & x_2y_2 & x_2y_3 \\
x_3y_1 & x_3y_2 & x_3y_3 
\end{array}
\right) \,\, \Leftrightarrow \,\,\mathbf{M}_{31}\otimes \mathbf{M}_{13}=\mathbf{M}_{33} \, .
\]

Notemos que este funcional (funcional bilineal) cumple con:
\begin{eqnarray*}
\mathcal{T}\left[ \lambda_1 \left| \mathrm{v}_1\right> +\lambda_2  \left| \mathrm{v}_2\right>, \left< \mathrm{u}\right| \right ] &=& 
\lambda_1\mathcal{T}\left[\left| \mathrm{v}_1\right> , \left< \mathrm{u}\right| \right ] + 
\lambda_2\mathcal{T}\left[\left| \mathrm{v}_2\right> , \left< \mathrm{u}\right| \right ] \,,\\
\mathcal{T}\left[  \left| \mathrm{v}\right>, \lambda_1\left< \mathrm{u}_1\right| + \lambda_2\left< \mathrm{u}_2\right| \right ] &=& 
\lambda_1\mathcal{T}\left[\left| \mathrm{v}\right> , \left< \mathrm{u}_1\right| \right ] + 
\lambda_2\mathcal{T}\left[\left| \mathrm{v}\right> , \left< \mathrm{u}_2\right| \right ] \,,
\end{eqnarray*}
donde $\{\lambda_1, \lambda_2\} \in \mathds{R}$, $\{\left| \mathrm{v}\right>,\left| \mathrm{v}_1\right>,\left| \mathrm{v}_2\right>\} \in \textbf{\em V}^1=\mathbf{M}_{31}$ y
$\{\left< \mathrm{u}\right|,\left< \mathrm{u}_1\right|,\left< \mathrm{u}_2\right| \} \in \textbf{\em V}^{2*}=\mathbf{M}_{13}$.

Veamos que si $\{\left| e_{1}(1)\right>, \left|e_{2}(1)\right>, \left|e_{3}(1)\right> \}$ es una base para $\textbf{\em V}^1$ y $\{\left< e_{1}(2)\right|, \left< e_{2}(2)\right|, \left< e_{3}(2)\right| \}$ es una base para $\textbf{\em V}^{2*}$ definidas como:
\[
\left|e_{1}(1)\right>= \left(
\begin{array}
[c]{r}
1\\
0\\
0
\end{array}
\right), \,
\left|e_{2}(1)\right>= \left(
\begin{array}
[c]{r}
0\\
1\\
0
\end{array}
\right),\,
\left|e_{3}(1)\right>= \left(
\begin{array}
[c]{r}
0\\
0\\
1
\end{array}
\right),\,
\left<e_{1}(2)\right| = \left(1\, 0 \, 0 \right), \,
\left<e_{2}(2)\right| = \left(0\, 1 \, 0 \right), \,
\left<e_{3}(2)\right| = \left(0\, 0 \, 1 \right). 
\]
Con el producto tensorial previamente definido, entonces podemos construir:
\[
\left|e_{1}(1)\right> \otimes \left< e_{1}(2)\right|  = 
\left(
\begin{array}
[c]{ccc}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{array}
\right) ,\,\,
\left|e_{1}(1)\right> \otimes \left<e_{2}(2)\right|  = 
\left(
\begin{array}
[c]{ccc}
0 & 1 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{array}
\right) ,\,\,
\left|e_{1}(1)\right> \otimes \left< e_{3}(2)\right|  = 
\left(
\begin{array}
[c]{ccc}
0 & 0 & 1 \\
0 & 0 & 0 \\
0 & 0 & 0 
\end{array}
\right) ,
\]

\[
\left|e_{2}(1)\right> \otimes \left<e_{1}(2)\right|  = 
\left(
\begin{array}
[c]{ccc}
0 & 0 & 0 \\
1 & 0 & 0 \\
0 & 0 & 0 
\end{array}
\right) ,\,\,
\left|e_{2}(1)\right> \otimes \left< e_{2}(2)\right|  = 
\left(
\begin{array}
[c]{ccc}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0 
\end{array}
\right) ,\,\,
\left|e_{2}(1)\right> \otimes \left< e_{3}(2)\right|  = 
\left(
\begin{array}
[c]{ccc}
0 & 0 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0 
\end{array}
\right),
\]

\[
\left|e_{3}(1)\right> \otimes \left< e_{1}(2)\right|  = 
\left(
\begin{array}
[c]{ccc}
0 & 0 & 0 \\
0 & 0 & 0 \\
1 & 0 & 0 
\end{array}
\right) ,\,\,
\left|e_{3}(1)\right> \otimes \left< e_{2}(2)\right| = 
\left(
\begin{array}
[c]{ccc}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 1 & 0 
\end{array}
\right) ,\,\,
\left|e_{3}(1)\right> \otimes \left< e_{3}(2)\right| = 
\left(
\begin{array}
[c]{ccc}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1 
\end{array}
\right)\,.
\]

Es claro que éste conjunto de matrices (matrices $3\times3$) forman una base para el espacio $\textbf{\em V}^3=\mathbf{M}_{33}$, y que la dim$(\textbf{\em V}^3)=9$.


\item {\bf Un ejemplo detallado de transformación de tensores}
\label{EjercicioDetallado}

Ilustremos ahora las transformaciones de tensores bajo cambios de la base del espacio vectorial. Una vez más consideremos dos bases de vectores coordenados: $\left\{  \left|  \mathrm{e}_{1}\right>, \left| \mathrm{e}_{2}\right>, \left|  \mathrm{e}_{3}\right> \right\}  $ y $\left\{  \left|  \mathrm{\tilde{e}}_{1}\right>, \left| \mathrm{\tilde{e}}_{2}\right>, \left| \mathrm{\tilde{e}}_{3}\right> \right\}$
para el espacio vectorial $\mathds{R}^{3}$. 

La expresión de un determinado tensor en la base: $\left\{  \left|  \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right>, \left|  \mathrm{e}_{3}\right> \right\}\equiv\left\{  \left|  \mathrm{i}\right>, \left|  \mathrm{j}\right>, \left|  \mathrm{k}\right> \right\}$  será:
\[
T_{j}^{i}=\left(
\begin{array}
[c]{ccc}
2 & 1 & 3\\
2 & 3 & 4\\
1 & 2 & 2
\end{array}
\right) \,.
\]

Si consideramos una nueva base:  
$ \left|  \textrm{w}_{1}\right> =\left|  \mathrm{i}\right>$, $\left|  \textrm{w}_{2}\right>=\left|  \mathrm{i}\right> +\left|  \mathrm{j}\right> $ y 
$\left|  \textrm{w}_{3}\right>=\left|  \mathrm{i}\right>+\left|  \mathrm{j}\right> +\left|  \mathrm{k}\right> $.

Entonces, podemos ver que:
\[
\left| \mathrm{\tilde{e}}_{i}\right>=\left|  \textrm{w}_{i}\right> = {\tilde A}^j_i \left| \mathrm{e}_{j}\right>
 \,\, \Longleftrightarrow \,\, {\tilde A}^j_i =
\left(
\begin{array}
[c]{ccc}
1 & 1 & 1\\
&  & \\
0 & 1 & 1\\
&  & \\
0 & 0 & 1
\end{array}
\right) \,.
\]

Para ese mismo espacio $\mathds{R}^{3}$, encontraremos una nueva expresión para $T_{j}^{i}$ en la base $\{\textrm{w}_{i}\}$. Es decir, necesitaremos calcular:
\[
\tilde{T}_{m}^{k} =\frac{\partial \tilde{x}^{k}}{\partial x^{i}}
\frac{\partial x^{j}}{\partial \tilde{x}^{m}}\ T_{j}^{i}\,.
\]

Nótese que la nueva base \textbf{no es ortogonal} ,$\left< \textrm{w}^{k}\right.  \left|  \textrm{w}_{i}\right> \neq\delta_{i}^{k}$, con lo cual no se cumplen muchas cosas, entre ellas: $\left|  \textrm{w}_{k}\right> \left< \textrm{w}^{k}\right|  \neq1$.

Este ejercicio consiste en ver cómo calcular las expresiones de los siguientes tensores: $\tilde{T}_{i}^{j},\tilde{T}_{ij}$ y $\tilde{T}^{ij}$. 

Para encontrar  $\tilde{T}_{j}^{i}$ expresamos los vectores base: $\left\{ \left|\textrm{e}_{  1}\right> = \left|  \textrm{i}\right> ,\left|\textrm{e}_{2}\right> = \left|  \textrm{j}\right> , \left|\textrm{e}_{  3}\right> = \left|  \textrm{k}\right> \right\}$ en términos de la base $\left\{  \left|  \textrm{w}_{1}\right> ,\left| \textrm{w}_{2}\right> ,\left|  \textrm{w}_{3}\right> \right\}$, es decir,   $\left|  \textrm{e}_{i}\right> = {A}^j_i \left| \textrm{w}_{j}\right>$. Esto es:
\[
\left\{
\begin{array}
[c]{l}
\left|  \textrm{e}_{1}\right> =\left|  \textrm{i}\right> = \left|\textrm{w}_{1}\right> \\
\\
\left|  \textrm{e}_{2}\right> =\left|  \textrm{j}\right> = -\left|  \textrm{w}_{1}\right> +
\left|\textrm{w}_{2}\right>  \\
\\
\left|  \textrm{e}_{3}\right> =\left|  \textrm{k}\right> =-\left|  \textrm{w}_{2}\right> +
\left|\textrm{w}_{3}\right> 
\end{array}
\right.
 \,\, \Longleftrightarrow \,\, {A}^j_i =
\left(
\begin{array}
[c]{rrr}
1 & -1 & 0\\
&  & \\
0 & 1 & -1\\
&  & \\
0 & 0 & 1
\end{array}
\right)\,.
\]

Recordamos que un vector genérico  transforma de la siguiente manera:
\[
\left|  a\right>  =a^{j}\left|  \textrm{e}_{j}\right> = \tilde{a}^{j}\left|  \textrm{w}_{j}\right>\,,
\]
por lo tanto:
\begin{align*}
\left|  a\right>  &  = a^{j}\left|  \textrm{e}_{j}\right>
=\tilde{a}^{1}\left|  \textrm{w}_{1}\right> +\tilde{a}
^{2}\left|  \textrm{w}_{2}\right> +\tilde{a}^{3}\left|
\textrm{w}_{3}\right> =\tilde{a}^{1}\left|  \textrm{e}
_{1}\right> +\tilde{a}^{2}\left(  \left|  \textrm{e}_{1}\right>
+\left|  \textrm{e}_{2}\right> \right)  +\tilde{a}^{3}\left(  \left|
\textrm{e}_{1}\right> +\left|  \textrm{e}_{2}\right> +\left|
\textrm{e}_{3}\right> \right)\,,
\end{align*}
con lo cual:
\[
\left|  a\right>=a^{1}\left|  \textrm{e}_{1}\right> +a^{2}\left|  \textrm{e}
_{2}\right> +a^{3}\left|  \textrm{e}_{3}\right> =\left(  \tilde
{a}^{1}+\tilde{a}^{2}+\tilde{a}^{3}\right)  \left|  \textrm{e}_{1}
\right> +\left(  \tilde{a}^{2}+\tilde{a}^{3}\right)  \left|
\textrm{e}_{2}\right> +\tilde{a}^{3}\left|  \textrm{e}_{3}\right>\,.
\]

Lo que nos lleva al siguiente sistema de ecuaciones:
\[
\left.
\begin{array}
[c]{l}
a^{1}=\tilde{a}^{1}+\tilde{a}^{2}+\tilde{a}^{3}\\
a^{2}=\tilde{a}^{2}+\tilde{a}^{3}\\
a^{3}=\tilde{a}^{3}
\end{array}
\right\}  \,\, \Rightarrow \,\,  a^{i}=\frac{\partial x^{i}}{\partial \tilde{x}^{k}}\ \tilde{a}^{k}
\,\, \Rightarrow \,\, 
\left\{
\begin{array}
[c]{lll}
\frac{\partial x^{1}}{\partial \tilde{x}^{1}}=1; & 
\frac{\partial x^{1}}{\partial \tilde{x}^{2}}=1; & 
\frac{\partial x^{1}}{\partial \tilde{x}^{3}}=1\\
&  & \\
\frac{\partial x^{2}}{\partial \tilde{x}^{1}}=0; & 
\frac{\partial x^{2}}{\partial \tilde{x}^{2}}=1; & 
\frac{\partial x^{2}}{\partial \tilde{x}^{3}}=1\\
&  & \\
\frac{\partial x^{3}}{\partial \tilde{x}^{1}}=0; & 
\frac{\partial x^{3}}{\partial \tilde{x}^{2}}=0; & 
\frac{\partial x^{3}}{\partial \tilde{x}^{3}}=1
\end{array}
\right.
\]

Es de hacer notar que dado que la base: $\left\{  \left|\textrm{e}_{1}\right>, \left|  \textrm{e}_{2}\right>, \left|\textrm{e}_{3}\right> \right\}  \equiv\left\{  \left|  {i}\right> ,\left|  {j}\right> ,\left|  {k}\right> \right\}$  es ortonormal, se tiene que:
\[
\left|  a\right> =a^{j}\left|  \textrm{e}_{j}\right> =\tilde{a}^{i}\left|  \textrm{w}_{i}\right> \,\, \Rightarrow \,\,   \left< \textrm{e}^{i}\right|  \left.  a\right> =a^{j}\left< \textrm{e}^{i}\right.  \left|  \textrm{e}_{j}\right> =a^{j}\delta_{j}^{i} = a^{i}=\tilde{a}^{k}\left< \textrm{e}^{i}\right.  \left| \textrm{w}_{k}\right> \,\, \Rightarrow \,\,   \frac{\partial x^{i}}{\partial \tilde{x}^{k}}=\left< \textrm{e}^{i}\right.  \left|\textrm{w}_{k}\right>\,.
\]

Este mismo procedimiento se puede aplicar para expresar el vector $\left| a\right> $ como una combinación lineal de los vectores $\left|\textrm{w}_{j}\right>$: 
\[
\left|  a\right> =\tilde{a}^{j}\left|  {\tilde{\textrm{e}}}_{j}\right> = a^{j}\left|  \textrm{e}_{j}\right> =a^{1}\left|\textrm{e}_{1}\right> +a^{2}\left|  \textrm{e}_{2}\right> +a^{3}\left|  \textrm{e}_{3}\right> =a^{1}\left|  \textrm{w}_{1}\right> +a^{2}\left(  \left|  \textrm{w}_{2}\right> -\left|  \textrm{w}_{1}\right> \right)  +a^{3}\left(  \left|\textrm{w}_{3}\right> -\left|  \textrm{w}_{2}\right> \right) \,,
\]
esto es:
\[
\left.
\begin{array}
[c]{l}
\tilde{a}^{1}=a^{1}-a^{2}\\
\tilde{a}^{2}=a^{2}-a^{3}\\
\tilde{a}^{3}=a^{3}
\end{array}
\right\} \,\, \Rightarrow \,\,   \tilde{a}^{k}=\frac{\partial \tilde{x}^{k}}{\partial x^{i}}a^{i}
\,\, \Rightarrow \,\,   \left\{
\begin{array}
[c]{ccc}
\frac{\partial \tilde{x}^{1}}{\partial x^{1}}=1; & 
\frac{\partial \tilde{x}^{1}}{\partial x^{2}}=-1; & 
\frac{\partial \tilde{x}^{1}}{\partial x^{3}}=0\\
&  & \\
\frac{\partial \tilde{x}^{2}}{\partial x^{1}}=0; & 
\frac{\partial \tilde{x}^{2}}{\partial x^{2}}=1; & 
\frac{\partial \tilde{x}^{2}}{\partial x^{3}}=-1\\
&  & \\
\frac{\partial \tilde{x}^{3}}{\partial x^{1}}=0; & 
\frac{\partial \tilde{x}^{3}}{\partial x^{2}}=0; & 
\frac{\partial \tilde{x}^{3}}{\partial x^{3}}=1
\end{array}
\right.
\]

Nótese que, como era de esperarse:
\[
\frac{\partial x^{i}}{\partial \tilde{x}^{k}}\frac{\partial \tilde{x}^{k}
}{\partial x^{j}}=\delta_{j}^{i} \,\, \Rightarrow \,\,   \left(
\begin{array}
[c]{lll}
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  \left(
\begin{array}
[c]{rrr}
1 & -1 & 0\\
0 & 1 & -1\\
0 & 0 & 1
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right) \,.
\]

Con las expresiones matriciales para las transformaciones, estamos en capacidad de calcular, componente a componente, las representación del tensor dado en la nueva base: 
\[
\tilde{T}_{m}^{k} =\frac{\partial \tilde{x}^{k}}{\partial x^{i}}
\frac{\partial x^{j}}{\partial \tilde{x}^{m}}\ T_{j}^{i}\,.
\]

Veamos:
\begin{eqnarray*}
\tilde{T}_{1}^{1} &=&  \frac{\partial \tilde{x}^{1}}{\partial x^{i}}
\frac{\partial x^{j}}{\partial \tilde{x}^{1}}\ T_{j}^{i} \\
 &=&  \frac
{\partial \tilde{x}^{1}}{\partial x^{1}}\left(  \frac{\partial x^{1}}{\partial \tilde{x}^{1}}\ T_{1}^{1}+\frac{\partial x^{2}}{\partial \tilde{x}^{1}}\ T_{2}^{1}+\frac{\partial x^{3}}{\partial \tilde{x}^{1}}\ T_{3}^{1}\right)  +\frac{\partial \tilde{x}^{1}}{\partial x^{2}}\left(
\frac{\partial x^{1}}{\partial \tilde{x}^{1}}\ T_{1}^{2}+\frac
{\partial x^{2}}{\partial \tilde{x}^{1}}\ T_{2}^{2}+\frac{\partial x^{3}}{\partial \tilde{x}^{1}}\ T_{3}^{2}\right)  \\
&+&\frac{\partial \tilde{x}^{1}}{\partial x^{3}}\left(
\frac{\partial x^{1}}{\partial \tilde{x}^{1}}\ T_{1}^{3}+\frac
{\partial x^{2}}{\partial \tilde{x}^{1}}\ T_{2}^{3}+\frac{\partial x^{3}}{\partial \tilde{x}^{1}}\ T_{3}^{3}\right)\,,
\end{eqnarray*}
es decir:
\begin{eqnarray*}
\tilde{T}_{1}^{1} &=& 1\cdot\left(1\ T_{1}^{1}+0\ T_{2}^{1}+0\ T_{3}^{1}\right) 
-1\cdot\left(  1\ T_{1}^{2}+0\ T_{2}^{2}+0\ T_{3}^{2}\right) 
+0\left(  1\ T_{1}^{3}+0\ T_{2}^{3}+0\ T_{3}^{3}\right)  \\
&=& T_{1}^{1}-T_{1}^{2}=2-2=0 \,.
\end{eqnarray*}

Del mismo modo para $\tilde{T}_{2}^{1}$:
\begin{eqnarray*}
\tilde{T}_{2}^{1}&= & \frac{\partial \tilde{x}^{1}}{\partial x^{i}}
\frac{\partial x^{j}}{\partial \tilde{x}^{2}}\ T_{j}^{i} \\
&=&  \frac{\partial \tilde{x}^{1}}{\partial x^{1}}\left(  \frac{\partial x^{1}}
{\partial \tilde{x}^{2}}\ T_{1}^{1}+\frac{\partial x^{2}}{\partial \tilde{x}^{2}}\ T_{2}^{1}+
\frac{\partial x^{3}}{\partial \tilde{x}^{2}}\ T_{3}^{1}\right) 
+\frac{\partial \tilde{x}^{1}}{\partial x^{2}}\left(
\frac{\partial x^{1}}{\partial \tilde{x}^{2}}\ T_{1}^{2}+\frac
{\partial x^{2}}{\partial \tilde{x}^{2}}\ T_{2}^{2}+\frac{\partial x^{3}}
{\partial \tilde{x}^{2}}\ T_{3}^{2}\right) \\
& + & \frac{\partial \tilde{x}^{1}}{\partial x^{3}}\left(
\frac{\partial x^{1}}{\partial \tilde{x}^{2}}\ T_{1}^{3}+\frac
{\partial x^{2}}{\partial \tilde{x}^{2}}\ T_{2}^{3}+\frac{\partial x^{3}}
{\partial \tilde{x}^{2}}\ T_{3}^{3}\right)\,,
\end{eqnarray*}
resultando:
\begin{eqnarray*}
\tilde{T}_{2}^{1}&= &  1\cdot\left(1\ T_{1}^{1}+1\ T_{2}^{1}+0\ T_{3}^{1}\right) 
-1\cdot\left(  1\ T_{1}^{2}+1\ T_{2}^{2}+0\ T_{3}^{2}\right) 
+0\left(  1\ T_{1}^{3}+1\ T_{2}^{3}+0\ T_{3}^{1}\right) \\
&= &   \left(T_{1}^{1}+T_{2}^{1}\right)  -\left(  T_{1}^{2}+T_{2}^{2}\right)  =
\left(2+1\right)  -\left(  2+3\right)  =-2 \,.
\end{eqnarray*}


Se puede continuar término a término (el lector debe terminar los cálculos) o realizar la multiplicación
de las matrices  provenientes de la transformación de componentes de tensores. Vale decir:
\[
\tilde{T}_{m}^{k}=\left(\frac{\partial \tilde{x}^{k}}{\partial x^{i}}\right) \left(T_{j}^{i}\right) \left(\frac{\partial x^{j}}{\partial \tilde{x}^{m}}\right)
\,\, \Leftrightarrow \,\,
\left(
\begin{array}
[c]{rrr}
1 & -1 & 0\\
0 & 1 & -1\\
0 & 0 & 1
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}
2 & 1 & 3\\
2 & 3 & 4\\
1 & 2 & 2
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  =\left(
\begin{array}
[c]{rrr}
0 & -2 & -3\\
1 & 2 & 4\\
1 & 3 & 5
\end{array}
\right)\,.
\]

Hay que resaltar el especial cuidado que se tuvo en la ubicación de las matrices para su multiplicación. Si bien en la expresión $\tilde{T}_{m}^{k}=\frac{\partial \tilde{x}^{k}}{\partial x^{i}}\frac{\partial x^{j}}{\partial \tilde{x}^{m}}\ T_{j}^{i}$ las cantidades $\frac{\partial \tilde{x}^{k}}{\partial x^{i}}$ son números y no importa el orden con el cual se multipliquen, cuando se escriben como matrices debe respetarse la ``concatenación interna de índices''. Esto es,  cuando queremos expresar $\tilde{T}_{m}^{k}$ como una matriz, donde el índice contravariante $k$ indica filas y el índice covariante $m$ las columnas, fijamos primero estos índices y luego respetamos la ``concatenación de índices'' covariantes con los contravariantes. Esta es la convención para expresar la multiplicación de matrices en la notación de índices\footnote{Quizá una forma de comprobar si los índices están bien concatenados se observa si se ``bajan'' los índices contravariantes pero se colocan  antes que los covariantes. Esto es, $T_{j}^{i}\rightarrow T_{ij}$. Así, la multiplicación de matrices queda representada de la siguiente forma: $C_{j}^{i}=A_{k}^{i}B_{j}^{k} \rightarrow C_{ij}=A_{ik}B_{kj}$ y aquí es claro que índices consecutivos están ``concatenados'' e indican multiplicación.}. Esto es:
\[
\tilde{T}_{m}^{k}=\frac{\partial \tilde{x}^{k}}{\partial x^{i}}
\frac{\partial x^{j}}{\partial \tilde{x}^{m}}\ T_{j}^{i}
\quad \Rightarrow 
\left(\tilde{T}_{m}^{k}\right)=\left(\frac{\partial \tilde{x}^{k}}{\partial x^{i}}\right) \left(T_{j}^{i}\right)\left(\frac{\partial x^{j}}{\partial \tilde{x}^{m}}\right)\,.
\]

Los objetos $\frac{\partial \tilde{x}^{k}}{\partial x^{i}},T_{j}^{i}$ y $\frac{\partial x^{j}}{\partial \tilde{x}^{m}}$ fueron sustituidos (en sus puestos correspondientes) por su representación matricial. 
Con lo cual hemos encontrado la representación matricial $\tilde{T}_{i}^{j}$ de las componentes del tensor $\bf{T}$ en la base $\left\{\left|  \textrm{w}_{1}\right> ,\left|  \textrm{w}_{2}\right> ,\left|  \textrm{w}_{3}\right> \right\}$
\[
\tilde{T}_{i}^{j}=
\left(
\begin{array}
[c]{lll}
\tilde{T}_{1}^{1} & \tilde{T}_{2}^{1} & \tilde{T}_{2}^{1}\\
\tilde{T}_{1}^{2} & \tilde{T}_{2}^{2} & \tilde{T}_{3}^{2}\\
\tilde{T}_{1}^{3} & \tilde{T}_{2}^{3} & \tilde{T}_{3}^{3}
\end{array}
\right) =
\left(
\begin{array}
[c]{rrr}
0 & -2 & -3\\
1 & 2 & 4\\
1 & 3 & 5
\end{array}
\right) \,.
\]

Para encontrar la expresión de $\tilde{T}_{ij}$ ($\tilde{T}_{ij}=\tilde{g}_{ik}\tilde{T}_{j}^{k}$), requeriremos las componentes covariantes y contravariantes del tensor métrico $\tilde {g}_{ik}$ que genera esta base. Para ello recordamos que para una base genérica, $\left\{  \left|  \textrm{w}_{j}\right> \right\}$, no necesariamente ortogonal, de un espacio vectorial con producto interno, podemos definir la expresión de un tensor que denominaremos tensor métrico como:
\[
\mbox{un tensor del tipo }\,\, \left(
\begin{array}
[c]{c}
0\\
2
\end{array} 
\right) \,\, \Rightarrow \,\,   
\tilde{g}_{ij} = \frac{\partial x^{m}}{\partial \tilde{x}^{i}} \frac{\partial x^{n}}{\partial \tilde{x}^{j}} g_{mn} \equiv
\left< \mathrm{e}^{m}\right.\left|\mathrm{w}_{i}\right> \left< \mathrm{e}^{n}\right.\left|\mathrm{w}_{j}\right>  g_{mn} \,.
\]

Recordemos  que la métrica covariante $g_{ij}$ generada por una base ortonormal $\left\{  \left|  \mathrm{e}_{i}\right> \right\} \equiv\left\{  \left|  \mathrm{i}_i\right> \right\}$ es:
\[
g_{11} =1 \,,\,g_{22} =1 \,,\,g_{33} =1 \,,\,
g_{12} =g_{21} =g_{13}=g_{31} =g_{23}=g_{32} =0  \,.
\]
Es decir:
\[
g_{ij}= g^{ij}= g_{j}^{i} \,\,\Longleftrightarrow\,\, \left(
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right)  \,.
\]

Con lo cual, para el caso de la base genérica no ortonormal 
$\left\{  \left| \textrm{w}_{j}\right> \right\}$ tenemos dos formas de calcular las componentes covariantes y contravariantes del tensor
métrico. 

La primera es la forma directa: $\tilde{g}_{ij} = \left< \mathrm{e}^{m}\right.\left|\mathrm{w}_{i}\right> \left< \mathrm{e}^{n}\right.\left|\mathrm{w}_{j}\right>  g_{mn}$. Esto es:
\[
\begin{array}[c]{l}
\tilde{g}_{11} =  \left< \mathrm{e}^{n}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{m}\right.   \left| \textrm{w}_{1}\right> g_{nm} = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{1}\right.   \left| \textrm{w}_{1}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{2}\right.   \left| \textrm{w}_{1}\right> +
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{3}\right.   \left| \textrm{w}_{1}\right>  \\
\quad \,\,\,\, = \left< \mathrm{e}^{1}\right. \left| \textrm{w}_{1}\right>^{2}=1
\\
\tilde{g}_{12} =  \left< \mathrm{e}^{n}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{m}\right.   \left| \textrm{w}_{2}\right> g_{nm} = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right> +
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{2}\right> \\
\quad \,\,\,\, =
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{1}\right> \left< \mathrm{e}^{1}\right.   \left| \textrm{w}_{2}\right> = 1
\\
\tilde{g}_{13} = 
\left< \mathrm{e}^{n}\right. \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{m}\right. \left| \textrm{w}_{3}\right> g_{nm} = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right> +
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{1}\right>  \left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{3}\right> \\
\quad \,\,\,\, =
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{1}\right> \left< \mathrm{e}^{1}\right.   \left| \textrm{w}_{3}\right> = 1
\end{array}
\]
\[
\begin{array}[c]{l}
\tilde{g}_{21} = 
\left< \mathrm{e}^{n}\right. \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{m}\right. \left| \textrm{w}_{1}\right> g_{nm} = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{1}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{1}\right> +
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{1}\right> \\
\quad \,\,\,\, = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{1}\right>  = 1
\\
\tilde{g}_{22} = 
\left< \mathrm{e}^{n}\right. \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{m}\right. \left| \textrm{w}_{2}\right> g_{nm} = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right> +
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{2}\right>\\
\quad \,\,\,\,  =  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right> 
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right>  = 2
\\
\tilde{g}_{23} = 
\left< \mathrm{e}^{n}\right. \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{m}\right. \left| \textrm{w}_{3}\right> g_{nm} = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right> +
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{3}\right>  \\
\quad \,\,\,\, = \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right>  +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right> =2
\end{array}
\]
\[
\begin{array}[c]{l}
\tilde{g}_{31} = 
\left< \mathrm{e}^{n}\right. \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{m}\right. \left| \textrm{w}_{1}\right> g_{nm} = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{1}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{1}\right> +
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{1}\right> \\
\quad \,\,\,\, = \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{1}\right>  =1 
\\
\tilde{g}_{32} = 
\left< \mathrm{e}^{n}\right. \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{m}\right. \left| \textrm{w}_{2}\right> g_{nm} = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right> +
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{2}\right>  \\ 
\quad \,\,\,\, = \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{2}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{2}\right>  = 2
\\
\tilde{g}_{33} = 
\left< \mathrm{e}^{n}\right. \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{m}\right. \left| \textrm{w}_{3}\right> g_{nm} = 
\left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right> +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right> +
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{3}\right> \\
\quad \,\,\,\, = \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{1}\right.  \left| \textrm{w}_{3}\right>  +
\left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{2}\right.  \left| \textrm{w}_{3}\right>  + 
\left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{3}\right>  \left< \mathrm{e}^{3}\right.  \left| \textrm{w}_{3}\right> = 3
\end{array}
\]

De manera que:
\[
\tilde{g}_{ij}\,\,\Rightarrow\,\,
\tilde{g}_{11} =1\,,\,\tilde{g}_{12} =1\,,\,\tilde{g}_{13} =1\,,\,
\tilde{g}_{21} =1\,,\,\tilde{g}_{22} =2\,,\,\tilde{g}_{23} =2\,,\,
\tilde{g}_{31} =1\,,\,\tilde{g}_{32} =2\,,\,\tilde{g}_{33} =3\,.
\] 

Consecuentemente podemos ``arreglarlo como una matriz''\footnote{Recordemos que hemos insistido que las matrices representan tensores mixtos} de la siguiente forma:
\[
\tilde{g}_{ij}\equiv\tilde{g}_{ji}\,\,\Longleftrightarrow\,\,\left(
\begin{array}
[c]{ccc}
1 & 1 & 1\\
1 & 2 & 2\\
1 & 2 & 3
\end{array}
\right) \,\, \Rightarrow \,\,  \tilde{g}^{ij}\equiv\tilde
{g}^{ji}=\left(  \tilde{g}_{ij}\right)  ^{-1}\,\,\Longleftrightarrow\,\,
\left(
\begin{array}
[c]{rrr}
2 & -1 & 0\\
-1 & 2 & -1\\
0 & -1 & 1
\end{array}
\right)\,.
\]

Nótese que también podríamos haber procedido, y en términos ``matriciales'', de la siguiente forma:
\[
\tilde{g}_{km}=\frac{\partial x^{i}}{\partial \tilde{x}^{k}}\ g_{ij}
\ \frac{\partial x^{j}}{\partial \tilde{x}^{m}}\,\, \Rightarrow \,\, 
\left(
\begin{array}
[c]{ccc}
1 & 0 & 0\\
1 & 1 & 0\\
1 & 1 & 1
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}
1 & 1 & 1\\
1 & 2 & 2\\
1 & 2 & 3
\end{array}
\right)\,.
\]

Nuevamente es bueno aclarar que  para conservar la convención de índices y poder representar la multiplicación de matrices, los índices deben estar consecutivos, por lo tanto, hay que trasponer la representación matricial para multiplicarlas. Es por eso que en el cálculo anterior aparece como primera matriz la transpuesta de la última. El cálculo se debe hacer como se muestra a continuación:
\[
\tilde{g}_{km}=\frac{\partial x^{i}}{\partial \tilde{x}^{k}}\ g_{ij}
\ \frac{\partial x^{j}}{\partial \tilde{x}^{m}}\quad \longrightarrow \quad\tilde
{g}_{km}=\Pi_{ik}\ g_{ij}\ \Pi_{jm}\quad\longrightarrow \quad\tilde{g}_{km}=\bar
{\Pi}_{ki}\ g_{ij}\ \Pi_{jm}\,.
\]

Finalmente, estamos en capacidad de obtener las representaciones matriciales para los tensores: $\tilde{T}_{i}^{j}, \tilde{T}_{ij}, \tilde{T}^{ij}$.
\begin{align*}
\tilde{T}_{i}^{j}  &  =(\tilde{T}_{j}^{i})^{\mathrm{T}}\,\, \Longleftrightarrow\,\,
 \tilde{T}_{i}^{j}=
\left(
\begin{array}
[c]{rrr}
0 & -2 & -3\\
1 & 2 & 4\\
1 & 3 & 5
\end{array}
\right) ^{\mathrm{T}}=\left(
\begin{array}
[c]{ccc}
0 & 1 & 1\\
-2 & 2 & 3\\
-3 & 4 & 5
\end{array}
\right) \,.  \\
\tilde{T}_{km}  &  = \tilde{g}_{kn}\tilde{T}_{m}^{n} \,\,\Longleftrightarrow\,\,
\tilde{T}_{km}=
\left(
\begin{array}
[c]{ccc}
1 & 1 & 1\\
1 & 2 & 2\\
1 & 2 & 3
\end{array}
\right)  \left(
\begin{array}
[c]{rrr}
0 & -2 & -3\\
1 & 2 & 4\\
1 & 3 & 5
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}
2 & 3 & 6\\
4 & 8 & 15\\
5 & 11 & 20
\end{array}
\right) \,. \\
\tilde{T}^{kn} &  = \tilde{T}_{m}^{n}\tilde{g}^{mk}\,\,\Longleftrightarrow\,\,
\tilde{T}^{kn}=
\left(
\begin{array}
[c]{rrr}
0 & -2 & -3\\
1 & 2 & 4\\
1 & 3 & 5
\end{array}
\right)  \left(
\begin{array}
[c]{rrr}
2 & -1 & 0\\
-1 & 2 & -1\\
0 & -1 & 1
\end{array}
\right)  =\left(
\begin{array}
[c]{rrr}
2 & -1 & -1\\
0 & -1& 2\\
-1 & 0 & 2
\end{array}
\right)\,.
\end{align*}

Quisiéramos ejemplificar una forma ``rápida y furiosa'' (pero sucia) de calcular la métrica generada por una determinada base genérica. La idea es que, violentando toda nuestra notación e idea de tensores,  construimos la métrica a partir de los vectores base definiéndola como
$
\tilde{g}_{ij}=\left< \mathrm{w}_{i}\right.\left|\mathrm{w}_{i}\right>  
$, de esta manera: 
\begin{eqnarray*}
\tilde{g}_{ij} &=&
\left(
\begin{array}{ccc}
\left<  \mathrm{w}_{1}\right.  \left| \mathrm{w}_{1}\right> & 
\left<  \mathrm{w}_{1}\right.  \left|  \mathrm{w}_{2}\right> & 
\left<  \mathrm{w}_{1}\right.  \left|  \mathrm{w}_{3}\right> \\
\left<  \mathrm{w}_{2}\right.  \left| \mathrm{w}_{1}\right> & 
\left<  \mathrm{w}_{2}\right.  \left|  \mathrm{w}_{2}\right> & 
\left<  \mathrm{w}_{2}\right.  \left|  \mathrm{w}_{3}\right> \\
\left<  \mathrm{w}_{3}\right.  \left| \mathrm{w}_{1}\right> & 
\left<  \mathrm{w}_{3}\right.  \left|  \mathrm{w}_{2}\right> & 
\left<  \mathrm{w}_{3}\right.  \left|  \mathrm{w}_{3}\right> 
\end{array}
\right) \\
&=&
\left(
\begin{array}{ccc}
\left<  \mathrm{i}\right.\left| \mathrm{i}\right> & 
\left< \mathrm{i}\right| \left[ \left|\mathrm{i}\right> +\left| \mathrm{j}\right> \right] & 
\left<  \mathrm{i}\right|  \left[\left|  \mathrm{i}\right> +\left| \mathrm{j}\right> +\left| \mathrm{k}\right> \right] \\
\left[ \left<  \mathrm{i}\right| +\left<  \mathrm{j}\right|  \right]  \left|  \mathrm{i}\right> & 
\left[  \left<  \mathrm{i}\right| +\left<  \mathrm{j}\right|  \right]  \left[  \left|  \mathrm{i} \right> +\left|  \mathrm{j}\right> \right] & 
\left[  \left<  \mathrm{i}\right|+\left<  \mathrm{j}\right|  \right] \left[  \left|  \mathrm{i} \right> +\left| \mathrm{j}\right>+\left|  \mathrm{k}\right> \right] \\
\left[  \left<  \mathrm{i}\right|+\left<  \mathrm{j}\right|  +\left<  \mathrm{k}\right|  \right] \left|  \mathrm{i}\right> & 
\left[  \left<  \mathrm{i}\right|  +\left<  \mathrm{j}\right| +\left<  \mathrm{k}\right|  \right]  \left[  \left|  \mathrm{i}\right> +\left|  \mathrm{j}\right> \right]& 
\left[  \left<  \mathrm{i}\right|+\left<  \mathrm{j}\right|  +\left<  \mathrm{k}\right|  \right] \left[  \left|  \mathrm{i}\right> +\left|  \mathrm{j}\right>
+\left|  \mathrm{k}\right> \right]
\end{array}
\right)
=
\left(
\begin{array}{ccc}
1 &1 & 1 \\
1 & 2 & 2\\
1 & 2 & 3 
\end{array}
\right)\,.
\end{eqnarray*}

Dejamos al lector, la reflexión si esta forma ``rápida de calcular la métrica'' a partir de unos vectores base es general o, si en su defecto, es una coincidencia únicamente válida para este caso. 


\item {\bf Otros ejemplos con tensores cartesianos}
\begin{enumerate}
\item 

Dado los tensores:
\[
T_{ij}= 
\left(
\begin{array}[c]{ccc}
1 &0 &2 \\
3 &4 &1\\
1 &3 & 4
\end{array}
\right) \,,  \quad 
E^i=
\left(
\begin{array}[c]{l}
1\\
4\\
3
\end{array}
\right) \,.
\]

La parte simétrica de $T_{ij}$ es:
\[
S_{ij}=\frac{1}{2}\left[T_{ij}+T_{ji}\right]= 
\frac12\left[
\left(
\begin{array}[c]{ccc}
1 &0 &2 \\
3 &4 &1\\
1 &3 & 4
\end{array}
\right)+\left(
\begin{array}[c]{ccc}
1 &3 & 1 \\
0 &4 & 3\\
2 &1 & 4
\end{array}
\right)\right]=
\frac12
\left(
\begin{array}[c]{ccc}
2 &3 & 3 \\
3 &8 & 4\\
3 &4 & 8
\end{array}
\right)\,.
\]

Para la parte antisimétrica de $T_{ij}$:
\[
A_{ij}=\frac{1}{2}\left[T_{ij}-T_{ji}\right]= 
\frac12\left[
\left(
\begin{array}[c]{ccc}
1 &0 &2 \\
3 &4 &1\\
1 &3 & 4
\end{array}
\right)-\left(
\begin{array}[c]{ccc}
1 &3 & 1 \\
0 &4 & 3\\
2 &1 & 4
\end{array}
\right)\right]=
\frac12
\left(
\begin{array}[c]{ccc}
0 &-3 & 1 \\
3 &0 & -2\\
-1 &2 & 0
\end{array}
\right)\,.
\]

Por lo tanto:
\[
T_{ij}=S_{ij}+A_{ij}= 
\frac12\left[
\left(
\begin{array}[c]{ccc}
2 &3 & 3 \\
3 &8 & 4\\
3 &4 & 8
\end{array}
\right)
+\left(
\begin{array}[c]{ccc}
0 &-3 & 1 \\
3 &0 & -2\\
-1 &2 & 0
\end{array}
\right)\right]=
\left(
\begin{array}[c]{ccc}
1 &0 &2 \\
3 &4 &1\\
1 &3 & 4
\end{array}
\right)\,.
\]

Para calcular $T_{ij}E^j$ hacemos lo siguiente:
\[
C_i=T_{i1}E^1+T_{i2}E^2+T_{i3}E^3=
\left\{
\begin{array}[c]{l}
C_1=T_{11}E^1+T_{12}E^2+T_{13}E^3=(1)(1)+(0)(4)+(2)(3)=7 \\
C_2=T_{21}E^1+T_{22}E^2+T_{23}E^3=(3)(1)+(4)(4)+(1)(3)=22\\
C_3=T_{31}E^1+T_{32}E^2+T_{33}E^3=(1)(1)+(3)(4)+(4)(3)=25
\end{array}
\right. 
\]

\item Dados $T_{j}^{i}$, $a^{i}$ y $b^{i}\in  \mathds{R}^3$ con:
\[
T_{j}^{i}=\left(
\begin{array}
[c]{ccc}
1 & 0 & 3\\
3 & 4 & 1\\
3 & 1 & 4
\end{array}
\right)\,, \quad{\bf a}=\left(
\begin{array}
[c]{r}
5\\
2\\
-5
\end{array}
\right)  \quad\text{y}\quad{\bf b}=\left(
\begin{array}
[c]{r}
-2\\
5\\
4
\end{array}
\right)\,.
\]

Supongamos que $T_{j}^{i},$ ${\bf a}$ y ${\bf b}$ están expresados en coordenadas cartesianas. 

Vamos a calcular: $A_{ij}a^{i}b^{j}$, $S_{ij}a^{i}a^{j}$ y  $A_{ij}b^{i}b^{j}$,  donde $S_{ij}$ y $A_{kl}$ son, respectivamente, las partes simétrica y antisimétrica del tensor $T_{j}^{i}$.
 
La métrica en coordenadas cartesianas viene dada por:
\[
g_{ij}=\left(
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right) \,.
\]

Tendremos que:
\[
S_{ij}=\frac{1}{2}\left(T_{ij}+T_{ji}\right)  =\frac{1}{2}\left(  \left(
\begin{array}
[c]{ccc}
1 & 0 & 3\\
3 & 4 & 1\\
3 & 1 & 4
\end{array}
\right)  +\left(
\begin{array}
[c]{ccc}
1 & 3 & 3\\
0 & 4 & 1\\
3 & 1 & 4
\end{array}
\right)  \right)  =\frac12\left(
\begin{array}
[c]{ccc}
2 & 3 & 6\\
3 & 8 & 2\\
6 & 2 & 8
\end{array}
\right)\,.
\]
Nótese que la expresión matricial para $T_{ij}\equiv
g_{ik}T_{j}^{k}$ es la misma que para $T_{j}^{i}$ debido a la
forma de la métrica en este espacio. Del mismo modo:
\[
A_{ij}=\frac{1}{2}\left(  T_{ij}-T_{ji}\right)  =\frac{1}{2}\left(  \left(
\begin{array}
[c]{ccc}
1 & 0 & 3\\
3 & 4 & 1\\
3 & 1 & 4
\end{array}
\right)  -\left(
\begin{array}
[c]{ccc}
1 & 3 & 3\\
0 & 4 & 1\\
3 & 1 & 4
\end{array}
\right)  \right)  =\frac12\left(
\begin{array}
[c]{ccc}
0 & -3 & 0\\
3 & 0 & 0\\
0 & 0 & 0
\end{array}
\right)\,,
\]
por lo tanto:
\begin{align*}
a^{i}A_{ij}b^{j}  & =\frac{1}{2}\left(
\begin{array}
[c]{ccc}
5 & 2 & -5
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}
0 & -3 & 0\\
3 & 0 & 0\\
0 & 0 & 0
\end{array}
\right)  \left(
\begin{array}
[c]{c}
-2\\
5\\
4
\end{array}
\right)  =-\frac{87}{2} \,,\\
& \\
a^{i}S_{ij}a^{j}  & =\frac{1}{2}\left(
\begin{array}
[c]{ccc}
5 & 2 & -5
\end{array}
\right)  \left(
\begin{array}
[c]{ccc}
2 & 3 & 6\\
3 & 8 & 2\\
6 & 2 & 8
\end{array}
\right)  \left(
\begin{array}
[c]{c}
5\\
2\\
-5
\end{array}
\right)  =1\,, \\
& \\
b^{i}A_{ij}b^{j}  & =0 \,.
\end{align*}


\end{enumerate}

%%%%%%%%
\item \textbf{Sistema genérico de coordenadas}. Dado un sistema genérico de coordenadas oblicuas:
\[
\left|  {\mathrm{e}}_{1} \right\rangle =a\left|  \mathrm{i}\right\rangle
+b\left|  \mathrm{j}\right\rangle \,, \quad\left|  {\mathrm{e}}
_{2}\right\rangle =c\left|  \mathrm{i}\right\rangle +d\left|  \mathrm{j}
\right\rangle \,.
\]

\begin{enumerate}
\item  Para una base arbitraria, $\left\{ \left|  {\mathrm{e}}_{i} \right\rangle \right\}$, 
la métrica viene definida por:
\begin{align*}
g_{ij} &  \equiv g_{ji}=\mathbf{g}\left[ \left|  {\mathrm{e}}_{i} \right\rangle, \left|  {\mathrm{e}}_{j} \right\rangle  \right]  \equiv
\left\langle {\mathrm{e}}^{i} \right.  \left|  {\mathrm{e}}_{j} \right\rangle \\
 g_{ij} &  =\left(
\begin{array}
[c]{cc}
\left\langle {\mathrm{e}}^{1}\right. \left|{\mathrm{e}}_{1} \right\rangle&
\left\langle {\mathrm{e}}^{1}\right. \left|{\mathrm{e}}_{2} \right\rangle \\
\left\langle {\mathrm{e}}^{2}\right. \left|{\mathrm{e}}_{1} \right\rangle &
\left\langle {\mathrm{e}}^{2}\right. \left| {\mathrm{e}}_{2}\right\rangle
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}
a^{2}+b^{2} & ac+bd\\
ac+bd & c^{2}+d^{2}
\end{array}
\right)\,.
\end{align*}

\item  Un vector  genérico: $\left| v \right\rangle =v_{x}\left|  \mathrm{i}\right\rangle +v_{y}\left|  \mathrm{j}\right\rangle $, en estas coordenadas se escribe como:
\[
\left.
\begin{array}
[c]{c}
\left|  {\mathrm{e}}_{1}\right\rangle =a\left|  \mathrm{i}\right\rangle
+b\left|  \mathrm{j}\right\rangle \\
\\
\left|  {\mathrm{e}}_{2}\right\rangle =c\left|  \mathrm{i}\right\rangle
+d\left|  \mathrm{j}\right\rangle
\end{array}
\right\}  \quad \Leftrightarrow\quad \left\{
\begin{array}
[c]{c}
\left|  \mathrm{i}\right\rangle =\frac{1}{\Delta}\left( d\left|
{\mathrm{e}}_{1}\right\rangle -b\left|  {\mathrm{e}}
_{2}\right\rangle \right) \\
\\
\left|  \mathrm{j}\right\rangle =\frac{1}{\Delta}\left( -c\left|
{\mathrm{e}}_{1}\right\rangle + a\left|  {\mathrm{e}}
_{2}\right\rangle \right)
\end{array}
\right.
\]
con $\Delta=ad-bc$, por lo cual:
\begin{align*}
\left| v \right\rangle   &  =v_{x}\left|  \mathrm{i}\right\rangle +v_{y}\left|  \mathrm{j}\right\rangle =\frac{v_{x}}{\Delta}\left(  d\left|  {\mathrm{e}}
_{1}\right\rangle -b\left|  {\mathrm{e}}_{2}\right\rangle \right)
+\frac{v_{y}}{\Delta}\left( - c\left|  {\mathrm{e}}_{1}\right\rangle
+a\left|  {\mathrm{e}}_{2}\right\rangle \right) \\
& \\
&  =\left(  d\frac{v_{x}}{\Delta}-c\frac{v_{y}}{\Delta}\right)  \left|
{\mathrm{e}}_{1}\right\rangle -\left(  b\frac{v_{x}}{\Delta}
-a\frac{v_{y}}{\Delta}\right)  \left|  {\mathrm{e}}_{2}\right\rangle \,.
\end{align*}

\item  Consideremos ahora una base y un tensor de manera concreta.
\[
\left|  {\mathrm{e}}_{1}\right\rangle =\left|  \mathrm{i}\right\rangle \,,\quad
\left|  {\mathrm{e}}_{2}\right\rangle =\frac{\sqrt{2}}{2}\left|\mathrm{i}\right\rangle +
\frac{\sqrt{2}}{2}\left|  \mathrm{j}\right\rangle \,, \quad T_{j}^{i}=\left(
\begin{array}
[c]{cc}
4 & 2\\
1 & 4
\end{array}
\right)\,.
\]

Encontremos primero la expresión matricial para el tensor $\tilde{T}_{ij}$\footnote{\textbf{Ayuda}: dada una matriz genérica 
$A_{j}^{i}=\left(
\begin{array}
[c]{cc}
A & B\\
C & D
\end{array}
\right)  $, su inversa será $\left(
\begin{array}
[c]{cc}
\frac{D}{AD-BC} & -\frac{B}{AD-BC}\\
-\frac{C}{AD-BC} & \frac{A}{AD-BC}
\end{array}
\right).$}.

En general:
\[
\tilde{T}_{ij}={g}_{ik}\tilde{T}_{j}^{k}={g}_{ik}\frac
{\partial \tilde{x}^{k}}{\partial x^{m}}T_{n}^{m}\frac{\partial x^{n}
}{\partial \tilde{x}^{j}} \,,
\]
Identificando:
\begin{align*}
\tilde{v}_{x} &  =\tilde{v}^{1}=\frac{\partial \tilde{x}^{1}}{\partial
 x^{j}}v^{j}=\frac{\partial \tilde{x}^{1}}{\partial x^{1}}v^{1}
+\frac{\partial \tilde{x}^{1}}{\partial x^{2}}v^{2}= 
{{\frac{d}{\Delta}}} v_{x}-{{\frac{c}{\Delta}}} v_{y} \,, \\
& \\
\tilde{v}_{y} &  =\tilde{v}^{2}=\frac{\partial \tilde{x}^{2}}{\partial
 x^{j}}v^{j}=\frac{\partial \tilde{x}^{2}}{\partial x^{1}}v^{1}
+\frac{\partial \tilde{x}^{2}}{\partial x^{2}}v^{2}=  
-{{\frac{b}{\Delta}}}v_{x}+{{\frac{a}{\Delta}}}v_{y} \,,
\end{align*}
donde: $\frac{\partial \tilde{x}^{1}}{\partial x^{1}}=\frac{d}{\Delta}$,  
$\frac{\partial \tilde{x}^{1}}{\partial x^{2}}=-\frac{c}{\Delta}$, 
$\frac{\partial \tilde{x}^{2}}{\partial x^{1}}=-\frac{b}{\Delta}$ y
$\frac{\partial \tilde{x}^{2}}{\partial x^{2}}=\frac{a}{\Delta}$. 

Como $a=1, b=0, c=\frac{\sqrt{2}}{2}, d=\frac{\sqrt{2}}{2}$, entonces:
\begin{eqnarray*}
{g}_{ik} \,\, \Rightarrow \,\, \left(
\begin{array}
[c]{cc}
1 & \frac{\sqrt{2}}{2}\\
\frac{\sqrt{2}}{2} & 1
\end{array}
\right) \,, \quad 
 \frac{\partial \tilde{x}^{k}}{\partial x^{m}}
\,\, \Rightarrow \,\, \left(
\begin{array}
[c]{cc}
1 & -1\\
0 & \sqrt{2}
\end{array}
\right)\,, \quad  
\frac{\partial x^{n}}{\partial \tilde{x}^{j}}= \left(\frac{\partial \tilde{x}^{n}}{\partial x^{j}}\right)^{-1}
\,\, \Rightarrow \,\, 
\left(
\begin{array}
[c]{cc}
1 & \frac{1}{\sqrt{2}}\\
0 & \frac{1}{\sqrt{2}}
\end{array}
\right)\,.
\end{eqnarray*}

Finalmente:
\[
\tilde{T}_{ij}=\left(
\begin{array}
[c]{cc}
1 & \frac{\sqrt{2}}{2}\\
\frac{\sqrt{2}}{2} & 1
\end{array}
\right)  \left(
\begin{array}
[c]{cc}
1 & -1\\
0 & \sqrt{2}
\end{array}
\right)  \left(
\begin{array}
[c]{cc}
4 & 2\\
1 & 4
\end{array}
\right)  \left(
\begin{array}
[c]{cc}
1 & \frac{1}{\sqrt{2}}\\
0 & \frac{1}{\sqrt{2}}
\end{array}
\right)  =\left(
\begin{array}
[c]{cc}
4 & 3\sqrt{2}\\
\frac{5}{\sqrt{2}} & \frac{11}{2}
\end{array}
\right) \,.
\]
\end{enumerate}

%%%%%%%%

\item {\bf Cartesianas y polares, otra vez}
\label{CartesianasPolaresOtra}
\index{Coordenadas!cartesianas}
\index{Coordenadas!polares}
\index{Polares!Coordenadas}
\index{Cartesianas!Coordenadas}

El ejemplo más simple, y por ello clásico y emblemático de cambio de coordenadas, lo constituye las expresiones de un mismo vector en dos sistemas de coordenadas en el plano: Cartesianas $\left\{  \left|  \mathrm{i}\right>, \left|  \mathrm{j}\right> \right\}$ y polares $\left\{  \left| {u}_{r}\right> ,\left|  {u}_{\theta}\right> \right\}$. Esto es, para un vector $\left|  a\right>$:
\[
\left|  a\right> =a_{x}\left|  \mathrm{i}\right> +a_{x}\left| \mathrm{j}\right> =
a^{1}\left|  \mathrm{e}_{1}\right> +a^{2}\left|  \mathrm{e}_{2}\right> \qquad\text{y} \qquad \left| a\right> = 
a_{r}\left|  {u}_{r}\right> +a_{\theta}\left|{u}_{\theta}\right> =\tilde{a}^{1}\left|  {\mathrm{\tilde{e}}}_{1}\right> +\tilde{a}^{2}\left|  {\mathrm{\tilde{e}}}_{2}\right> \,.
\]

Al expresar una base en términos de la otra obtenemos:
\[
\left| {u}_{r}\right> =\cos(\theta) \left| \mathrm{i} \right> +\mathrm{sen}(\theta) \left|  \mathrm{j}\right>
\qquad \text{y} \qquad\left|  {u}_{\theta}\right> =-\mathrm{sen}(\theta) \left|  \mathrm{i}\right> +\cos(\theta) \left|  \mathrm{j}\right> \,,
\]
con lo cual:
\begin{align*}
\left< {\mathrm{\tilde{e}}}^{i}\right.  \left|  \mathrm{e}_{j}\right>  &  =\tilde{A}_{j}^{i} \,\, \Longleftrightarrow \,\,
\tilde{A}_{j}^{i}=\left(
\begin{array}[c]{cc}
\left< {u}_{r}\right.  \left|  \mathrm{i}\right>  & \left< {u}_{r}\right.  \left|  \mathrm{j}\right> \\
\left< {u}_{\theta}\right.  \left|  \mathrm{i}\right>  & \left< {u}_{\theta}\right.  \left|  \mathrm{j}\right>
\end{array}
\right)  \equiv
\left(
\begin{array}[c]{cc}
\cos(\theta) & \mathrm{sen}(\theta)\\
-\mathrm{sen}(\theta) & \cos(\theta)
\end{array}
\right)\,.
\end{align*}

Para la transformación inversa se tiene:
\begin{align*}
\left< \mathrm{e}^{i}\right.  \left|  {\mathrm{\tilde{e}}}_{j} \right>  &  =A_{j}^{i} \,\, \Longleftrightarrow \,\, 
A_{j}^{i}=\left(
\begin{array}[c]{cc}
\left< \mathrm{i}\right.  \left|  {u}_{r}\right>  &
\left< \mathrm{i}\right.  \left|  {u}_{\theta}\right> \\
\left< \mathrm{j}\right.  \left|  {u}_{r}\right>  &
\left< \mathrm{j}\right.  \left|  {u}_{\theta}\right>
\end{array}
\right)  \equiv 
\left(
\begin{array}[c]{cc}
\cos(\theta) & -\mathrm{sen}(\theta)\\
\mathrm{sen}(\theta) & \cos(\theta)
\end{array}
\right) \,.
\end{align*}
y por lo tanto:
\[
\left| \mathrm{i}\right> = \cos(\theta) \left| {u}_{r} \right> - \mathrm{sen}(\theta) \left|  {u}_{\theta} \right>
\qquad \text{y} \qquad \left| \mathrm{j} \right> =
\mathrm{sen}(\theta) \left| {u}_{r} \right> +\cos(\theta) \left|  {u}_{\theta} \right> \,.
\]

Cumpliendo además con:
\[
\left(
\begin{array}[c]{cc}
\cos(\theta) & -\mathrm{sen}(\theta)\\
\mathrm{sen}(\theta) & \cos(\theta)
\end{array}
\right)  
\left(
\begin{array}[c]{cc}
\cos(\theta) & \mathrm{sen}(\theta)\\
-\mathrm{sen}(\theta) & \cos(\theta)
\end{array}
\right)  =
\left(
\begin{array}[c]{cc}
1 & 0\\
0 & 1
\end{array}
\right)  \,\, \Longleftrightarrow \,\, 
A_{k}^{i}\tilde{A}_{j}^{k}=\delta_{j}^{i} \,.
\]

Por otro lado,  si:
\[
\left|  a\right> =a_{r}\left|  {u}_{r}\right> +a_{\theta}\left|  {u}_{\theta}\right> \equiv
\tilde{a}^{1}\left|  {\tilde{e}}_{1}\right> +\tilde{a}^{2}\left|  {\mathrm{\tilde{e}}}_{2}\right> =
a_{x}\left|  \mathrm{i}\right> +a_{x}\left| \mathrm{j}\right> \equiv
a^{1}\left|  \mathrm{e}_{1}\right> +a^{2}\left|  \mathrm{e}_{2}\right>\,,
\]
tendremos que:
\[
\tilde{a}^{i}=\tilde{A}_{j}^{i}a^{j}\,\, \Longleftrightarrow \,\,
\left(
\begin{array}[c]{c}
a_{r}\\
a_{\theta}
\end{array}
\right) = 
\left(
\begin{array}[c]{cc}
\cos(\theta) & \mathrm{sen}(\theta)\\
-\mathrm{sen}(\theta) & \cos(\theta)
\end{array}
\right)  \left(
\begin{array}[c]{c}
a_{x}\\
a_{y}
\end{array}
\right)  =\left(
\begin{array}[c]{c}
a_{x}\cos(\theta)+a_{y}\mathrm{sen}(\theta)\\
-a_{x}\mathrm{sen}(\theta)+a_{y}\cos(\theta)
\end{array}
\right) \,,
\]
con lo cual:
\[
a_{r}=a_{x}\cos(\theta)+a_{y}\mathrm{sen}(\theta) \quad \text{y} \quad a_{\theta}=-a_{x}\mathrm{sen}(\theta)+a_{y}\cos(\theta) \,.
\]

Del mismo modo:
\[
a^{i}=A_{j}^{i}\tilde{a}^{j}\,\,\Longleftrightarrow\,\, 
\left(
\begin{array}
[c]{c}
a_{x}\\
a_{y}
\end{array}
\right)  =
\left(
\begin{array}
[c]{cc}
\cos(\theta) & -\mathrm{sen}(\theta)\\
\mathrm{sen}(\theta) & \cos(\theta)
\end{array}
\right)  \left(
\begin{array}
[c]{c}
a_{r}\\
a_{\theta}
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
a_{r}\cos(\theta)-a_{\theta}\mathrm{sen}(\theta)\\
a_{r}\mathrm{sen}(\theta)+a_{\theta}\cos(\theta)
\end{array}
\right)
\]
y
\[
a_{x}=a_{r}\cos(\theta)-a_{\theta}\mathrm{sen}(\theta)\qquad\text{y}\qquad
a_{y}=a_{r}\mathrm{sen}(\theta)+a_{\theta}\cos(\theta) \,.
\]

\item {\bf Pensando en componentes}
\label{Repensandolascomponentes}

Arriba consideramos los cambios de base entre cartesianas y polares. A partir de cambios de la base construimos la matriz de transformación entre sus componentes.  Ahora pondremos la atención en las componentes.  En general, podemos pensar que las componentes de los vectores pueden ser funciones de las otras. Consideremos el ejemplo anterior con esta visión para estudiarlo con más detalle en dos y tres dimensiones.

\begin{itemize}
\item Caso bidimensional

Tendremos que un punto en el plano viene representado en coordenadas cartesianas por dos números $\left(x,y\right)$ y en coordenadas polares por otros dos números $\left( r,\theta\right)$. Siguiendo el ejemplo anterior un punto $P$, en el plano lo describimos como:
\[
\left|  P\right> =r_{P}\left| {u}_{r}\right> = x_{P}\left| {i}\right> +y_{P}\left|  {j}\right> \,.
\]

Veamos como están relacionadas estas dos descripciones, para este caso en el cual las ecuaciones de transformación son:
\[
\left.
\begin{array}[c]{c}
x_{P}=x_{P}\left(  r,\theta\right)  =x^{1}=x^{1}\left(  \tilde{x}^{1} ,\tilde{x}^{2}\right) \\
y_{P}=y_{P}\left(  r,\theta\right)  =x^{2}=x^{2}\left(  \tilde{x}^{1} ,\tilde{x}^{2}\right)
\end{array}
\right\}  \quad \Longleftrightarrow \quad 
\left\{
\begin{array}[c]{c}
r_{P}=r_{P}\left(  x,y\right)  =\tilde{x}^{1}=\tilde{x}^{1}\left(  x^{1} , x^{2}\right) \\
\theta=\theta_{P}\left(  x,y\right)  =\tilde{x}^{2}=\tilde{x}^{2}\left( x^{1}, x^{2}\right)
\end{array}
\right. 
\]
y explícitamente:
\[
\left.
\begin{array}[c]{ccl}
x_{P}=r_{P}\ \cos(\theta) &\Rightarrow &x^{1}=\tilde{x}^{1}\ \cos(\tilde{x}^{2})\\
y_{P}=r_{P}\ \mbox{sen}(\theta) &\Rightarrow&x^{2}= \tilde{x}^{1}\ \mbox{sen}(\tilde{x}^{2})
\end{array}
\right\}  \,\, \mbox{y} \,\,\left\{
\begin{array}[c]{ccl}
r_{P}=\sqrt{x_{P}^{2}+y_{P}^{2}} &\Rightarrow&\tilde{x}^{1}=\sqrt{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}}\\
\theta=\arctan\left(  \frac{y_{P}}{x_{P}}\right) & \Rightarrow &\tilde{x}^{2}=\arctan\left(  \frac{x^{2}}{x^{1}}\right)
\end{array}
\right. 
\]

Es claro que ambas coordenadas están relacionadas y que se puede invertir la relación.
\[
\left.
\begin{array}[c]{c}
\tilde{x}^{1}=\tilde{x}^{1}\left(  x^{1},x^{2}\right) \\
\tilde{x}^{2}=\tilde{x}^{2}\left(  x^{1},x^{2}\right)
\end{array}
\right\}  \,\, \Longleftrightarrow\,\,\left\{
\begin{array}[c]{c}
x^{1}=x^{1}\left(  \tilde{x}^{1},\tilde{x}^{2}\right) \\
x^{2}=x^{2}\left(  \tilde{x}^{1},\tilde{x}^{2}\right)
\end{array}
\right. \,.
\]

Se debe pedir, eso si, dos cosas razonables:
\begin{enumerate}
\item  que las funciones $x^{i}=x^{i}\left(  \tilde{x}^{m}\right)$ y
$\tilde{x}^{j}=\tilde{x}^{j}\left(  x^{m}\right) $ sean al menos
$\mathcal{C}^{2}$  (función y derivada continua)

\item  que el determinante de la matriz Jacobiana sea finito,
\[
\det\left|  \frac{\partial x^{i}\left(  \tilde{x}^{k} \right)}{\partial \tilde{x}^{j}}\right| \neq  0\,.
\]
\end{enumerate}

Más aún, si:
\[
x^{i}=x^{i}\left(  \tilde{x}^{j}\left(  x^{k}\right)  \right)  
\,\, \Rightarrow \,\, 
\frac{\partial x^{i}}{\partial x^{k}}= \frac{\partial x^{i} }{\partial \tilde{x}^{j}}\frac{\partial \tilde{x}^{j}}{\partial x^{k} }=\delta_{k}^{i} 
\,\, \Rightarrow \,\,  \mathrm{d}x^{i}=\frac{\partial x^{i}}{\partial \tilde{x}^{j}}\mathrm{d} \tilde{x}^{j} \,,
\]
con lo cual intuimos dos cosas:
\begin{enumerate}
\item  que las componentes de un vector, $\left|P \right> $, deben transformar bajo un cambio de coordenadas como: 
\[
p^{i}=\frac{\partial x^{i}\left(  \tilde{x}^{k}\right)  }{\partial \tilde{x}^{j}}\tilde{p}^{j} \,.
\]
\item  Las matrices jacobianas $\frac{\partial x^{i}}{\partial \tilde{x}^{k}}$ y $\frac{\partial \tilde{x}^{i}}{\partial x^{k}}$ son una la inversa de la otra.
\end{enumerate}


Veamos si es cierto para el caso de vectores en el plano. Para ello calculamos la matriz jacobiana: 

\[
 \frac{\partial x^{i}\left(  \tilde{x}^{1},\tilde{x}^{2}\right)}{\partial \tilde{x}^{j}}\,\, \Rightarrow \,\,  
 \left(
\begin{array}[c]{cc}
\frac{\partial x^{1}\left(  \tilde{x}^{1},\tilde{x}^{2}\right)  }{\partial \tilde{x}^{1}} & \frac{\partial x^{1}\left(  \tilde{x}^{1},\tilde{x}^{2}\right)  }{\partial \tilde{x}^{2}}\\ 
\\
\frac{\partial x^{2}\left(  \tilde{x}^{1},\tilde{x}^{2}\right)  }{\partial \tilde{x}^{1}} & \frac{\partial x^{2}\left(  \tilde{x}^{1},\tilde{x}^{2}\right)  }{\partial \tilde{x}^{2}}
\end{array}
\right)  =
\left(
\begin{array}[c]{cc}
\cos(\tilde{x}^{2}) & -\tilde{x}^{1}\mathrm{sen}(\tilde{x}^{2})\\
\mathrm{sen}(\tilde{x}^{2}) & \tilde{x}^{1}\cos(\tilde{x}^{2})
\end{array}
\right)\,,
\]
y seguidamente, identificando $\left|  P\right>  = x_{P}\left| {i}\right> +y_{P}\left|  {j}\right> =r_{P}\left| {u}_{r}\right>$
\[
p^{i}=\frac{\partial x^{i}\left(  \tilde{x}^{1},\tilde{x}^{2}\right)}{\partial \tilde{x}^{j}}\tilde{p}^{j} 
\,\, \Rightarrow \,\,  
\left(
\begin{array}[c]{c}
p^{1}\\
p^{2}
\end{array}
\right)  =
\left(
\begin{array}[c]{cc}
\cos(\tilde{x}^{2}) & -\tilde{x}^{1}\mathrm{sen}(\tilde{x}^{2})\\
\mathrm{sen}(\tilde{x}^{2}) & \tilde{x}^{1}\cos(\tilde{x}^{2})
\end{array}
\right)  
\left(
\begin{array}[c]{c}
\tilde{p}^{1}\\
0
\end{array}
\right) \,.
\]

Igualmente, si calculamos la inversa de la matriz jacobiana:
\[
\left(  \frac{\partial x^{i}\left(  \tilde{x}^{1},\tilde{x}^{2}\right)}{\partial \tilde{x}^{j}}\right)^{-1}=
\left(
\begin{array}[c]{cc}
\cos(\tilde{x}^{2}) & \mathrm{sen}(\tilde{x}^{2})\\
\frac{-\mathrm{sen}(\tilde{x}^{2})}{\tilde{x}^{1}} & \frac{\cos(\tilde{x}^{2})}{\tilde{x}^{1}}
\end{array}
\right)  =
\left(
\begin{array}[c]{cc}
\frac{x^{1}}{\sqrt{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}}} &
\frac{x^{2}}{\sqrt{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}}}\\
\frac{-x^{2}}{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}} &
\frac{x^{1}}{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}}
\end{array}
\right)\,,
\]
tendremos:
\[
\left(
\begin{array}[c]{c}
\tilde{p}^{1}\\
0
\end{array}
\right)  =
\left(
\begin{array}[c]{cc}
\frac{x^{1}}{\sqrt{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}}} & \frac{x^{2}}{\sqrt{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}}} \\
\frac{-x^{2}}{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}} & \frac{x^{1}}{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}}
\end{array}
\right)  
\left(
\begin{array}[c]{c}
p^{1}\\
p^{2}
\end{array}
\right)  \quad \Rightarrow  \tilde{p}^{i}=\frac{\partial \tilde{x}^{i}\left(  x^{1},x^{2}\right)  }{\partial x^{j}} p^{j} \,.
\]


\item Caso tridimensional 

Consideremos nuevamente dos sistemas de coordenadas: uno cartesiano $\left(  x^{1}=x,\, x^{2}=y,\, x^{3}=z\right)$ y otro esférico $\left(  \tilde{x}^{1}=r,\,\tilde {x}^{2}=\theta,\,\tilde{x}^{3}=\phi\right)$. 

Tal y como hemos supuesto anteriormente el punto $P$ vendrá descrito por:
\[
\left|  P\right> =r_{P}\left| {u}_{r}\right> =x_{P}\left| {i}\right> +y_{P}\left|  {j}\right> +z_{P}\left| {k}\right> \,,
\]
de nuevo
\[
\left.
\begin{array}[c]{c}
x=x\left(  r,\theta,\phi\right)  =x^{1}=x^{1}\left(  \tilde{x}^{1},\tilde{x}^{2},\tilde{x}^{3}\right) \\
y=y\left(  r,\theta,\phi\right)  =x^{2}=x^{2}\left(  \tilde{x}^{1},\tilde{x}^{2},\tilde{x}^{3}\right) \\
z=z\left(  r,\theta,\phi\right)  =x^{3}=x^{3}\left(  \tilde{x}^{1},\tilde{x}^{2},\tilde{x}^{3}\right)
\end{array}
\right\}  \,\, \Longleftrightarrow \,\, 
\left\{
\begin{array}[c]{c}
r=r\left(  x,y,z\right)  =\tilde{x}^{1}=\tilde{x}^{1}\left(  x^{1},x^{2},x^{3}\right) \\
\theta=\theta\left(  x,y,z\right)  =\tilde{x}^{2}=\tilde{x}^{2}\left(x^{1},x^{2},x^{3}\right) \\
\phi=\phi\left(  x,y,z\right)  =\tilde{x}^{3}=\tilde{x}^{3}\left(  x^{1},x^{2},x^{3}\right)
\end{array}
\right.
\]

Las ecuaciones de transformación serán:
\[
\left\{
\begin{array}[c]{lcl}
x_{P}=r_{P}\mbox{sen}(\theta)\cos(\phi) &\Rightarrow &
x^1=\tilde{x}^{1}\mbox{sen}(\tilde{x}^{2}) \cos(\tilde{x}^{3})\\
y_{P}=r_{P}\mbox{sen}(\theta)\mbox{sen}(\phi) &\Rightarrow&
x^2=\tilde{x}^{1}\mbox{sen}(\tilde {x}^{2}) \mbox{sen}(\tilde{x}^{3})\\
z_{P}=r_{P} \cos(\theta) &\Rightarrow&
x^{3}=\tilde{x}^{1} \cos(\tilde{x}^{2})
\end{array}
\right.
\]
y las transformaciones inversas:
\[
\left\{
\begin{array}[c]{lcl}
r_{P}=\sqrt{x_{P}^{2}+y_{P}^{2}+z_{P}^{2}} &\Rightarrow &
\tilde{x}^{1}=\sqrt{\left(  x^{1}\right)^{2}+\left(  x^{2}\right)^{2}+\left(  x^{3}\right)^{2}}\\
\phi=\arctan\left(\frac{y_{P}}{x_{P}}\right) &\Rightarrow&
\tilde{x}^{2}=\arctan\left(  \frac{x^{2}}{x^{1}}\right)\\
\theta=\arctan\left(\frac{\sqrt{x_{P}^{2}+y_{P}^{2}}}{z_{P}}\right) &\Rightarrow&
\tilde{x}^{3}=\arctan\left(  \frac{\sqrt{\left(x^{1}\right)^{2}+\left(  x^{2}\right)^{2}}}{x^{3}}\right)
\end{array}
\right.
\]

Con lo cual la matriz de las derivadas para esta transformación en particular será:
\begin{eqnarray*}
\frac{\partial x^{i}\left(  \tilde{x}^{1},\tilde{x}^{2},\tilde{x}^{3}\right)}{\partial \tilde{x}^{j}}&=&
\left(
\begin{array}[c]{ccc}
\mathrm{sen}\left(  \theta\right)  \cos\left(  \phi\right)  & -r\mathrm{sen}\left(  \theta\right)  \mathrm{sen}\left(
\phi\right)  & r\cos\left(  \theta\right)  \cos\left(  \phi\right) \\
\mathrm{sen}\left(  \theta\right)  \mathrm{sen}\left( \phi \right)  & r\mathrm{sen}\left(  \theta\right)  \cos\left(
\phi\right)  & r\cos\left(  \theta\right)  \operatorname{sen}\left(  \phi\right) \\
\cos\left(  \theta\right)  & 0 & -r\mathrm{sen}\left(  \theta\right)
\end{array}
\right) \\
&=&
\left(
\begin{array}[c]{ccc}
\mathrm{sen}\left(  \tilde{x}^{2}\right)  \cos\left(  \tilde{x}^{3}\right)  & -\tilde{x}^{1}\mathrm{sen}\left(  \tilde{x}^{2}\right)
\mathrm{sen}\left(  \tilde{x}^{3}\right)  & \tilde{x}^{1}\cos\left( \tilde{x}^{2}\right)  \cos\left(  \tilde{x}^{3}\right) \\
\operatorname{sen}\left(  \tilde{x}^{2}\right)  \operatorname{sen}\left(  \tilde{x}^{3}\right)  & \tilde{x}^{1}\mathrm{sen}\left(  \tilde{x}^{2}\right)  \cos\left( \tilde{x}^{3}\right)  & \tilde{x}^{1}\cos\left(  \tilde{x}^{2}\right) \mathrm{sen}\left(  \tilde{x}^{3}\right) \\
\cos\left(  \tilde{x}^{2}\right)  & 0 & -\tilde{x}^{1}\mathrm{sen}\left(  \tilde{x}^{2}\right)
\end{array}
\right)\,.
\end{eqnarray*}


Mientras que la inversa es:
\begin{eqnarray*}
\frac{\partial \tilde{x}^{i}\left(  x^{1},x^{2},x^{3}\right)  }
{\partial x^{j}} &=&
\left(
\begin{array}[c]{ccc}
\operatorname{sen}\left(  \theta\right)  \cos\left(  \phi\right)  & \operatorname{sen}\left( \theta\right)  \operatorname{sen}\left(  \phi\right)  & \cos\left(  \theta\right) \\ 
-{\frac{\mathrm{sen}\left(  \phi\right)  }{r\mathrm{sen}\left(\theta\right)  }} & {\frac{\cos\left(  \phi\right)  }{r\mathrm{sen} \left(  \theta\right)  }} & 0\\ 
{\frac{\cos\left(  \theta\right)  \cos\left(  \phi\right)  }{r}} & {\frac {\cos\left(  \theta\right)  \mathrm{sen}\left(  \phi\right)  }{r}} &
-{\frac{\mathrm{sen}\left(  \theta\right)  }{r}}
\end{array}
\right) \\
&=&
\left(
\begin{array}[c]{ccc}
\frac{{x}}{\sqrt{{x}^{2}+{y}^{2}+z^{2}}} & \frac{{y}}{\sqrt{{x}^{2}+{y}^{2}+z^{2}}} & \frac{{z}}{\sqrt{{x}^{2}+{y}^{2}+z^{2}}}\\ 
\frac{-{y}}{{x}^{2}+{y}^{2}} & \frac{{x}}{{x}^{2}+{y}^{2}} & 0\\ 
\frac{{xz}}{\left(  {x}^{2}+{y}^{2}+z^{2}\right)  \sqrt{{x}^{2}+{y}^{2}}} & \frac{{yz}}{\left(  {x}^{2}+{y}^{2}+z^{2}\right)  \sqrt{{x}^{2}+{y}^{2}}} & \frac{{-}\sqrt{{x}^{2}+{y}^{2}}}{\left(  {x}^{2}+{y}^{2}+z^{2}\right)  }
\end{array}
\right)\,.
\end{eqnarray*}

Dejaremos al lector comprobar que, efectivamente,
\[
p^{i}=\frac{\partial x^{i}\left(  \tilde{x}^{1},\tilde{x}^{2},\tilde{x}^{3}\right)  }{\partial \tilde{x}^{j}}\tilde{p}^{j}
\quad \Longleftrightarrow \quad \tilde{p}^{i}=\frac{\partial \tilde{x}^{i}\left( x^{1},x^{2},x^{3}\right)  }{\partial x^{j}} p^{j} \,.
\]

\end{itemize}

\item {\bf Energía libre para un medio elástico} 

Consideremos el siguiente par de tensores provenientes  de la teoría de elasticidad:
\[
u_{ik}=\frac{1}{2}\left(  \partial_{k}\ u_{i}+\partial_{i}\ u_{k}\right)
\equiv\frac{1}{2}\left(  \frac{\partial u_{i}}{\partial x^{k}}
+\frac{\partial u_{k}}{\partial x^{i}}\right)\,, \quad\left(  u_{k}
^{i}\right)  ^{0}=u_{k}^{i}-\frac{1}{3}u_{m}^{m}\ \delta_{k}^{i}\,,
\]
y construyamos el tensor de esfuerzos como: $p_{j}^{i}=2\lambda\left(  u_{j}^{i}\right)  ^{0}+Ku_{l}^{l}\ \delta_{j}^{i}$. 

Calculemos la energía libre para el medio elástico, definida como: $F=\frac{1}{2}p_{j}^{i}u_{i}^{j}$.  

Tenemos que:
\[
p_{j}^{i}=2\lambda\left(  u_{j}^{i}\right)  ^{0}+Ku_{l}^{l}\ \delta_{j}
^{i}=2\lambda\left(  u_{j}^{i}-\frac{1}{3}u_{m}^{m}\ \delta_{j}^{i}\right)
+Ku_{l}^{l}\ \delta_{j}^{i}=2\lambda u_{j}^{i}+u_{l}^{l}\delta_{j}^{i}\left(
K-\frac{2\lambda}{3}\right)\,,
\]
donde: $u_{m}^{m}=\frac{1}{2}\left(  \partial^{m}\ u_{m}+\partial_{m}
\ u^{m}\right)  =\partial^{m}\ u_{m}$, con lo cual:

\begin{align*}
F  & =\frac{1}{2}p_{j}^{i}u_{i}^{j}=\frac{1}{2}\left(  2\lambda u_{j}
^{i}+u_{l}^{l}\delta_{j}^{i}\left(  K-\frac{2\lambda}{3}\right)  \right)
u_{i}^{j}=\left(  \lambda u_{j}^{i}u_{i}^{j}+u_{l}^{l}\delta_{j}^{i}u_{i}
^{j}\left(  \frac{1}{2}K-\frac{\lambda}{3}\right)  \right)  \\
& =\lambda\left(  u_{1}^{i}u_{i}^{1}+u_{2}^{i}u_{i}^{2}+u_{3}^{i}u_{i}
^{3}\right)  +\left(  \frac{1}{2}K-\frac{\lambda}{3}\right)  \left(  u_{l}
^{l}\right)  ^{2}\\
& =\lambda\left(  \left(  u_{1}^{1}u_{1}^{1}+u_{1}^{2}u_{2}^{1}+u_{1}^{3}
u_{3}^{1}\right)  +\left(  u_{2}^{1}u_{1}^{2}+u_{2}^{2}u_{2}^{2}+u_{2}
^{3}u_{3}^{2}\right)  +\left(  u_{3}^{1}u_{1}^{3}+u_{3}^{2}u_{2}^{3}+u_{3}
^{3}u_{3}^{3}\right)  \right)  +\left(  \frac{1}{2}K-\frac{\lambda}{3}\right)
\left(  u_{l}^{l}\right)  ^{2}\\
& =\lambda\left(  \left(  u_{1}^{1}\right)  ^{2}+\left(  u_{2}^{2}\right)
^{2}+\left(  u_{3}^{3}\right)  ^{2}+2\left(  u_{2}^{1}u_{1}^{2}+u_{3}^{2}
u_{2}^{3}+u_{1}^{3}u_{3}^{1}\right)  \right)  +\left(  \frac{1}{2}
K-\frac{\lambda}{3}\right)  \left(  u_{l}^{l}\right)  ^{2}\\
& =\left(  \frac{1}{2}K+\frac{2\lambda}{3}\right)  \left(  u_{l}^{l}\right)
^{2}+2\lambda\left(  u_{2}^{1}u_{1}^{2}+u_{3}^{2}u_{2}^{3}+u_{1}^{3}u_{3}
^{1}\right)  \\
& =\left(  \frac{1}{2}K+\frac{2\lambda}{3}\right)  \left(  \partial_{x}
u_{x}+\partial_{y}u_{y}+\partial_{z}u_{z}\right)  ^{2}+2\lambda\left(
\partial_{x}u_{y}\partial_{y}u_{x}+\partial_{y}u_{z}\partial_{z}u_{y}
+\partial_{z}u_{x}\partial_{x}u_{z}\right)\,.
\end{align*}



\end{enumerate}

\subsection{{\color{red}Practicando con Maxima}}
\subsubsection{Tensores y Maxima}
{\bf Maxima}  presenta dos paquetes para la manipulación de tensores, paquetes en constante desarrollo todavía. Estos paquetes son {\bf ctensor}, manipulación de componentes y el paquete {\bf itensor}, manipulación indexada. En la manipulación por componentes los tensores se representan por arreglos o matrices. En la manipulación indexada los tensores se representan por funciones de sus índices covariantes, contravariantes y de derivadas.

Los índices covariantes se especifican mediante una lista que será el primer argumento del objeto indexado, siendo los índices contravariantes otra lista que será el segundo argumento del mismo objeto indexado. Si al objeto indexado le falta cualquiera de estos grupos de índices, entonces se le asignará al argumento correspondiente la lista vacía $[\ ]$. Así, $g([a,b],[c])$ representa un objeto indexado llamado $g$, el cual tiene dos índices covariantes $(a,b)$, un índice contravariante $(c)$ y no tiene índices de derivadas, es decir, $g_{ab}^c$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(itensor)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
imetric(g);
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ishow(g([j,k],[])*g([i,l],[])*a([],[i,j]))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t3) }
a^{i\,j}\,g_{j\,k}\,g_{i\,l}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ishow(contract(%))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t4) }
a_{k\,l}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ishow(kdelta([i],[k])*a([k,l],[]))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t5) }
\delta_{i}^{k}\,a_{k\,l}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ishow(contract(%))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t6) }
a_{i\,l}
\end{math}
\newline

La identidad $\delta_i^j\delta_j^i=3$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ishow(contract(kdelta([i], [j])*kdelta([j], [i])))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t7) }
kdelta
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ishow(ev(%,kdelta))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t8) }
3
\end{math}
\newline

El símbolo de Levi-Civita:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
levi_civita([1, 2, 3]);levi_civita([1, 3, 2]);levi_civita([3, 3, 2]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
1
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
-1
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
0
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
expr:ishow('levi_civita([],[i,j,k])*'levi_civita([i,j,k],[]))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t12) }
\varepsilon^{i\,j\,k}\,\varepsilon_{i\,j\,k}
\end{math}
\newline

Aquí es necesario utilizar la función {\bf lc2kdt( )} que 
simplifica expresiones que contengan el símbolo de Levi-Civita, para convertirlas en expresiones con la delta de Kronecker siempre que esto sea posible.\footnote{En ocaciones, la función  {\bf lc2kdt( )}  hace uso del tensor métrico. Si el tensor métrico no fue previamente definido con {\bf imetric}, se obtiene un mensaje de error.}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ishow(lc2kdt(expr))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t13) }
\left(\delta_{i}^{k}\,\delta_{j}^{i}-3\,\delta_{j}^{k}\right)\,
 \delta_{k}^{j}+\left(\delta_{i}^{j}\,\delta_{j}^{k}-3\,\delta_{i}^{k
 }\right)\,\delta_{k}^{i}-3\,\delta_{i}^{j}\,\delta_{j}^{i}+27
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ishow(contract(expand(%)))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t14) }
27-7\,\delta
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ishow(ev(%,kdelta))$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t15) }
6
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}

\subsubsection{Transformación de tensores y Maxima}
Como ya mencionamos, {\bf Maxima} contiene un paquete para manipular componentes de tensores: {\bf ctensor}. La manipulación en componentes se realiza en términos de matrices. La métrica se almacena en la matriz $\mathrm{lg}$ y la métrica inversa se obtiene y almacena en la matriz $\mathrm{ug}$.

En un ejemplo visto con anterioridad se especificaba las componentes de un tensor en coordenadas cartesianas, esto es:
\[
\mathrm{To}=T_{j}^{i}=\left(
\begin{array}
[c]{ccc}
2 & 1 & 3\\
2 & 3 & 4\\
1 & 2 & 2
\end{array}
\right)\,, \quad \mbox{en la base: }
\left\{  \left|  \mathrm{e}_{1}\right> ,\left|  \mathrm{e}_{2}\right> ,\left|  \mathrm{e}_{3}\right> \right\}
\equiv\left\{  \left|  \mathrm{i}\right> ,\left|  \mathrm{j}\right> ,\left|  \mathrm{k}\right> \right\} \,.
\]

Para luego representarlo en la nueva base:   
$ \left|  \textrm{w}_{1}\right> =\left|  \mathrm{i}\right>$, $\left|  \textrm{w}_{2}\right>=\left|  \mathrm{i}\right> +\left|  \mathrm{j}\right> $ y 
$\left|  \textrm{w}_{3}\right>=\left|  \mathrm{i}\right>+\left|  \mathrm{j}\right> +\left|  \mathrm{k}\right> $.
Entonces necesitamos  calcular:
\[
\tilde{T}_{m}^{k} =\frac{\partial \tilde{x}^{k}}{\partial x^{i}}
\frac{\partial x^{j}}{\partial \tilde{x}^{m}}\ T_{j}^{i}
\,\, \Rightarrow \,\, \mathrm{Tn}=\alpha\beta\mathrm{To}=
\]

Como vimos cuando hacíamos los cálculos:
\[
\alpha=\frac{\partial \tilde{x}^{k}}{\partial x^{i}}=
\left(
\begin{array}
[c]{rrr}
1 & -1 & 0\\
0 & 1 & -1\\
0 & 0 & 1
\end{array}
\right)\,, \qquad
\beta=\frac{\partial x^{i}}{\partial \tilde{x}^{k}}=
\left(
\begin{array}
[c]{lll}
1 & 1 & 1\\
0 & 1 & 1\\
0 & 0 & 1
\end{array}
\right)\,.
\]

Respetando ``la concatenación interna de índices'' podemos realizar la multiplicación de matrices.  

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
To:matrix([2,1,3],[2,3,4],[1,2,2]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\begin{pmatrix}2 & 1 & 3 \\ 2 & 3 & 4 \\ 1 & 2 & 2 \\ \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
alpha:matrix([1,-1,0],[0,1,-1],[0,0,1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\begin{pmatrix}1 & -1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 1 \\ 
 \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
beta:matrix([1,1,1],[0,1,1],[0,0,1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\begin{pmatrix}1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \\ \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Tn:alpha.To.beta;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\begin{pmatrix}0 & -2 & -3 \\ 1 & 2 & 4 \\ 1 & 3 & 5 \\ 
 \end{pmatrix}
 \end{math}
\newline

Una vez que hemos calculado el nuevo tensor $\mathrm{Tn}={\tilde T}_i^j$ en la nueva base, podemos calcular: ${\tilde T}^{ij}$, ${\tilde T}_{ij}$. Pero podemos utilizar la métrica para las coordenadas nuevas y con el paquete {\bf ctensor} hacer las respectivas contracciones. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(ctensor)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ct_coordsys(cartesian3d)$
\end{verbatim}}
\end{minipage}
\newline

Introducimos la métrica ${\tilde g}_{ik}$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
lg:matrix([1,1,1],[1,2,2],[1,2,3]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }\begin{pmatrix}1 & 1 & 1 \\ 1 & 2 & 2 \\ 1 & 2 & 3 \\ \end{pmatrix} \end{math}
\newline

Para tener la métrica inversa ${\tilde g}^{ik}$ escribimos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
cmetric()$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ug;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }\begin{pmatrix}2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 1 \\ 
 \end{pmatrix}
\end{math}
\newline

Para calcular ${\tilde T}_{ij}={\tilde g}_{ik}{\tilde T}_j^{k}$ hacemos lo siguiente:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
lg.Tn;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }\begin{pmatrix}2 & 3 & 6 \\ 4 & 8 & 15 \\ 5 & 11 & 20 \\ 
 \end{pmatrix}
\end{math}
\newline

Y para calcular ${\tilde T}^{ij}={\tilde T}^i_{k}{\tilde g}^{kj}$ procedemos así:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Tn.ug;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }\begin{pmatrix}2 & -1 & -1 \\ 0 & -1 & 2 \\ -1 & 0 & 2 \\ 
 \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}
\newline

Es posible, y a manera de completar esta primer acercamiento con la librería de tensores,  calcular la métrica a partir de una transformación de coordenadas, como vimos en la sección anterior, ejemplo \ref{Repensandolascomponentes}. Escribimos las coordenadas que queremos utilizar, en este caso esféricas, utilizando la función {\bf ct coordsys}.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(ctensor)$
\end{verbatim}}
\end{minipage}
\newline

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ct_coordsys([r*sin(theta)*cos(phi),r*sin(theta)*sin(phi),r*cos(theta),[r,theta,phi]])$
\end{verbatim}}
\end{minipage}
\newline

Y le pedimos al programa que calcule el tensor métrico, al que le asigna el nombre {\bf lg}.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
cmetric()$
\end{verbatim}}
\end{minipage}
\newline

El tensor métrico $g_{ij}$ es entonces

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
lg:trigsimp(lg);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }\begin{pmatrix}1 & 0 & 0 \\ 0 & r^2 & 0 \\ 0 & 0 & r^2\,\sin ^2
 \theta \\ \end{pmatrix}
\end{math}
\newline

Y la métrica inversa $g^{ij}$, el programa la identifica con la etiqueta  {\bf ug}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
trigsimp(ug);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }\begin{pmatrix}1 & 0 & 0 \\ 0 & \frac{1}{r^2} & 0 \\ 0 & 0 & \frac{
 1}{r^2\,\sin ^2\theta} \\ \end{pmatrix}
\end{math}


\subsection{{\color{OliveGreen}Ejercicios}}

\begin{enumerate}
\item Si $A_{ijk}$ es un tensor covariante de orden 3 y $B^{lmno}$ un tensor contravariante de orden 4, pruebe que $A_{ijk}B^{jkno}$ es un tensor mixto de orden 3.

\item Consideremos un tensor $\textbf{B}$ con componentes $B_{i}^j$, un tensor $\textbf{A}$ con componentes  $A_{i}$ y el producto:
\[
B_{i}^jA_{j}=C_i \,.
\]
Esta última expresión se puede interpretar como la acción de ${\bf B}$ sobre $\textbf{A}$ que consiste en producir un nuevo tensor ${\bf C}$ que tendrá una magnitud y una dirección diferente a $\textbf{A}$. Podremos estar interesados en encontrar todos los vectores que NO son rotados por ${\bf B}$, es decir, que estaríamos interesados en resolver la ecuación:
\[
B_{i}^jA_{j}=\lambda A_i\,,\,\, 
\mbox{donde}\,\,\lambda \,\, \mbox{es un escalar}.
\]
Si estos vectores existen se denominan vectores característicos o ``autovectores'' de ${\bf B}$ y sus direcciones: direcciones principales o característicos. Más aún, los ejes determinados por las direcciones principales se denominan ejes principales de ${\bf B}$. Los valores de las componentes $B_{i}^j$ en el sistema de coordenadas determinado por los ejes principales se denominan valores característicos o ``autovalores'' de ${\bf B}$. 

Ahora, consideremos un sistema conformado por $n$ partículas de masas iguales: $m_1, m_2, \dots m_n=m$, distribuidas en el plano $xy$,  y sea $I_{ij}$ el tensor de inercia con respecto a un sistema rectangular de coordenadas. Por ser un caso en 2D, el tensor de inercia  tendrá solamente cuatro componentes. 

\begin{enumerate}
\item Encuentre: $I_{11}=I_{xx}$, $I_{22}=I_{yy}$ y $I_{12}=I_{xy}=I_{21}=I_{yx}$. 
\item Si un vector $\mathbf{A}$ coincide con el eje principal de $I_{i}^j$ entonces debe satisfacer la ecuación: 
\[
I_{i}^jA_{j}=\lambda A_i \,\, \Rightarrow \,\,  (I_{i}^j-\lambda\delta^j_i) A_j =0 \,\, \Rightarrow \,\,   
\left\{
\begin{array}
[c]{r}
(I_{11}-\lambda) A_1+I_{12}A_2=0 \\
I_{12} A_1+(I_{22}-\lambda) A_2 =0  \\
\end{array}
\right.
\]
Encuentre la solución (no trivial) para $\lambda$, es decir, resuelva:  
$\lambda^2-\lambda(I_{11}+I_{22})+I_{11}I_{22}-(I_{12})^2=0$.
\item ¿Cómo se interpreta el hecho de que $I_{12}=0$?
\item Si  $I_{12}\neq 0$ y $\lambda_1\neq \lambda_2$, entonces, para cada valor de $\lambda$ se puede encontrar un vector (autovector) $\textbf{A}^{(\lambda_1)}$ y $\textbf{A}^{(\lambda_2)}$ resolviendo el sistema de dos ecuaciones. Demuestre que las direcciones de estos vectores tienen pendientes, respecto al sistema de coordenada, dadas por:
\[
\tan(\theta_1)= \frac{\lambda_1-I_{11}}{I_{12}} \,,\quad
\tan(\theta_2)= \frac{\lambda_2-I_{11}}{I_{12}} \,.
\]  
\item Demuestre que:
\[
\tan(2\theta_1)=\tan(2\theta_2)= \frac{2I_{12}}{I_{11}-I_{22}} \,,
\,\, \mbox{donde}\,\, \theta_2= \theta_1+\frac{\pi}{2} \,,
\]
es decir: $\textbf{A}^{(\lambda_1)} \perp \mathbf{A}^{(\lambda_2)}$.
\item ¿Cuáles son las componentes del tensor de inercia en el sistema de coordenadas determinado por los ejes principales?
\end{enumerate}

\item Demuestre que si $S^{i}_j$ representa un tensor simétrico y $A^{i}_j$ uno antisimétrico, entonces, $S^{i}_jA^{j}_i=0$.

\item Dados los tensores:
\[
R^{i}_j=\left(
\begin{array}
[c]{ccc}
1/2  & 1    & 3/2   \\
2     & 5/2 &  3   \\
7/2  & 4    & 9/2 
\end{array}
\right) \,, \quad 
T^i= 
\left(
\begin{array}
[c]{c}
1/ 3     \\
2/3    \\
1   
\end{array}
\right) \,, \quad 
g^{i}_j=g^{ij}=g_{ij}=\left(
\begin{array}
[c]{ccc}
1 & 0 & 0   \\
0 & 1 & 0   \\
0 & 0 & 1 
\end{array}
\right) \,.
\] 
Encuentre:
\begin{enumerate}
\item La parte simétrica $S^{i}_j$ y antisimétrica $A^{i}_j$ de $R^{i}_j$.
\item $R_{kj}=g_{ik}R^i_j$, $R^{ki}=g^{jk}R^i_j$, $T_{j}=g_{ij}T^i$ ¿Qué se concluye de estos cálculos?
\item $R^i_jT_i$, $R^i_jT^j$, $R^i_jT_iT^j$.
\item $R^i_j S_i^j$, $R^i_j A_i^j$, $A_i^jT^i$, $A_i^jT^iT_j$.
\item $R^i_j-2\delta^i_jR^l_l$, $(R^i_j-2\delta^i_jR^l_l)T_i$, $(R^i_j-2\delta^i_jR^l_l)T_iT^j$.
\end{enumerate}

\item  Dado $F_{ijk}$ un tensor totalmente antisimétrico respecto a sus índices $ijk$, demuestre que el rotor de $F_{ijk}$ definido como sigue, también será un tensor.
\[
\operatorname*{rot}\left[  F_{ijk}\right]  =\partial_{m}F_{ijk}-\partial_{i}F_{jkm}+\partial_{j}F_{kmi}-\partial_{k}F_{mij}\equiv
\frac{\partial F_{ijk}}{\partial x^{m}}-\frac{\partial F_{jkm}}{\partial x^{i}}+\frac{\partial_{j}F_{kmi}}{\partial x^{j}}-\frac{\partial_{k}F_{mij}}{\partial x^{k}} \,.
\]

\item  El momento de inercia se define como:
\[
I_{j}^{i}=\int_{V}\mathrm{d}v\rho\left( {\bf r}\right)  \left(  \delta_{j}^{i}\left(  x^{k}x_{k}\right)  -x^{i}x_{j}\right)\,,  \quad\text{con \ }x^{i}=\left\{  x,y,z\right\}  \ \text{y}\ \mathrm{d}v=\mathrm{d}x\ \mathrm{d}y\ \mathrm{d}z \,.
\]

\begin{enumerate}
\item  Muestre que $I_{j}^{i}$ es un tensor.

\item  Considere un cubo de lado $l$ y masa total $M$ tal que tres de sus aristas coinciden con un sistema de coordenadas cartesiano. Encuentre el tensor momento de inercia, $I_{j}^{i}$.
\end{enumerate}

\item  Para un sistema de $n$ partículas rígidamente unidas, la
cantidad de movimiento ${\bf p}$ y cantidad de movimiento angular ${\bf L}$ vienen definidas por:
\[
{\bf p}_{\alpha} =m_{\alpha}\ {\bf v}_{\alpha}=m_{\alpha}\left( \boldsymbol{\omega}\times{\bf r}_{\alpha}\right)  \equiv\epsilon^{ijk}\omega_{j}x_{k}\left|  {e}_{i}\right>\,, \quad
{\bf L}  =\sum_{\alpha}\left( {\bf r}\times{\bf p}\right)_{\alpha} \equiv \epsilon^{ijk}x_{k}p_{k}\left|  {e}_{i}\right> \,,
\]
con $\alpha=1,2,\cdots,n$;  $\left|{e}_{i}\right> =\left\{{\bf i}, {\bf j},{\bf k}\right\}$ y $x^{i}=\left\{x,y,z\right\}$.

Muestre que:
\begin{enumerate}
\item ${\bf L}=\sum_{\alpha}m_{\alpha}\left[  \left( {\bf r} \cdot {\bf r}\right) _{\alpha} \boldsymbol{\omega}- {\bf r}_{\alpha}\left( {\bf r}_{\alpha}\cdot\boldsymbol{\omega}\right)  \right]$.

\item $L^{i}=I_{j}^{i}\omega^{j}$, donde $I_{j}^{i}=\sum_{\alpha}m_{\alpha}\left(  \delta_{j}^{i}\left(  x^{k}x_{k}\right)  -x^{i}x_{j}\right) $ es el tensor momento de inercia para un sistema de $n$ partículas rígidamente unidas.
\end{enumerate}

\item  Dados dos sistemas de coordenadas ortogonales 
$O\rightleftharpoons\left( x,y,z\right) $ y $\tilde{O}\rightleftharpoons\left(  \tilde{x},\tilde{y},\tilde{z}\right)$, donde el sistema de coordenadas $\tilde{O}$ se obtiene rotando a $O$, ${\pi}/{6}$ alrededor del eje $z$ y  ${\pi}/{2}$ alrededor del eje $\tilde{x}$, con lo cual los ejes  $\tilde{y}$ y $z$ coinciden.

\begin{enumerate}
\item  Si tenemos los vectores:
\[
{\bf A}={\bf{i}}+2{\bf{j}}+3{\bf{k}}\,,\quad{\bf B}=2{\bf{i}}+{\bf{j}}+3{\bf{k}}\,.
\]
Expréselos en el sistema de coordenadas $\tilde{O}\rightleftharpoons \left(  \tilde{x},\tilde{y},\tilde{z}\right)$.

\item  El tensor de esfuerzos (tensiones normales y tangenciales a una determinada superficie) se expresa en el sistema $O\rightleftharpoons\left(x,y,z\right)$ como:
\[
P_{j}^{i}=\left(
\begin{array}
[c]{ccc}
P_{1} & 0 & P_{4}\\
0 & P_{2} & 0\\
0 & 0 & P_{3}
\end{array}
\right)\,.
\]
¿Cuál será su expresión en el sistema de
coordenadas $\tilde{O}\rightleftharpoons\left(  \tilde{x},\tilde{y},\tilde
{z}\right)$?
\end{enumerate}

\item  Suponga un sistema de coordenadas ortogonales generalizadas $\left(q^{1},q^{2},q^{3}\right) $ las cuales tienen las siguiente relación funcional con las coordenadas cartesianas:
\[
q^{1}=x+y;\qquad q^{2}=x-y;\qquad q^{3}=2z\,.
\]

\begin{enumerate}
\item  Compruebe que el sistema $\left(  q^{1},q^{2},q^{3}\right) $ conforma un sistema de coordenadas ortogonales.

\item  Encuentre los vectores base para este sistema de coordenadas.

\item  Encuentre el tensor métrico y el elemento de volumen en estas coordenadas.

\item  Encuentre las expresiones en el sistema $\left(q^{1},q^{2},q^{3}\right) $ para los vectores:
\[
{\bf A}=2{\bf{j}}\,, \quad{\bf B}={\bf{i}}+2{\bf{j}}\,,\quad{\bf C}={\bf{i}}+7{\bf{j}}+3{\bf{k}}\,.
\]

\item  Encuentre en el sistema $\left(q^{1},q^{2},q^{3}\right)$ las expresiones para las siguientes relaciones vectoriales:
\[
{\bf A}\times{\bf B}\,, \quad{\bf A}\cdot{\bf C}\,, \quad\left({\bf A}\times{\bf B}\right)  \cdot{\bf C} \,.
\]
¿Qué puede decir si compara esas expresiones en ambos sistemas de coordenadas?
\end{enumerate}

\item  La relación entre las coordenadas cartesianas $\left(x,y\right)$ y las coordenadas bipolares $\left(  \xi,\zeta\right)$ viene dada por:
\[
x=\frac{a\sinh(\xi)}{\cosh(\xi)+\cos(\zeta)}\,, \quad 
y=\frac{a\sin(\zeta)}{\cosh(\xi)+\cos(\zeta)}\,,\quad \text{con } a=\mbox{const}\,.
\]

\begin{enumerate}
\item  Compruebe si los vectores base para las coordenadas bipolares son ortogonales.

\item  Encuentre el tensor métrico para las coordenadas bipolares.

\end{enumerate}

\end{enumerate}


\section{Vectores, tensores y espacios pseudoeuclidianos}
\label{Pseudoeuclidianos}
\index{Espacios vectoriales pseudo-euclidianos}
\index{Pseudo-euclidianos!Espacios Vectoriales}
Hasta este punto la descripción de formas representadas por un \textit{bra}: $\left< {a} \right| \equiv a_{k} \left< {e}^{k} \right|$ ha sido casi estética. Hemos insistido que las componentes de las formas tienen subíndices, mientras que sus vectores bases, $\left< {e}^{k} \right|$, deben tener superíndices, pero no hemos visto clara la necesidad de esa definición. Un ejemplo, un tanto tímido lo desarrollamos en en la sección \ref{BasesReciprocas},  \pageref{BasesReciprocas}, donde mencionamos que las bases recíprocas de vectores podrían jugar el papel de bases para el espacio dual. Quizá el ejemplo más emblemático y simple, donde se observa la diferencia entre formas (\textit{bras}) y vectores (\textit{kets}) es el caso de los espacios minkowskianos. Estos espacios, también llamados pseudoeuclidianos, presentan una variante en la definición de producto interno, de tal forma que: $\left< {x}\right|  \left.  {x}\right>$ no es necesariamente positivo, y si $\left< {x}\right|  \left.  {x}\right> =0$ no necesariamente implica que $\left|{x}\right> \equiv\left|  {0}\right> $. 

La consecuencia inmediata es que la definición de norma $\mathcal{N}\left(  \left|  {v}_{i}\right> \right)  \equiv \left\|\left|  {v}_{i}\right> \right\|$, que vimos anteriormente, no es necesariamente positiva. Vale decir que tendremos vectores con norma positiva, $\left\|\left| {v}_{i}\right> \right\|  > 0$, pero también vectores con norma negativa o cero: $\left\|\left| {v}_{i}\right> \right\| \leq 0$. Con lo cual la definición de distancia, entendida como la norma de la resta de vectores, 
$d\left(\left|  {x} \right>,\left|  {y}\right> \right) \equiv \left\|\left|  {x} \right> -\left|  {y}\right> \right\|$, 
tampoco será necesariamente positiva. Esto es, que las distancias serán negativas, positivas o nulas:   
$d\left(  \left|  {x}\right> ,\left|  {y}\right> \right)  <  0$,  
$\,  d\left(  \left|  {x}\right> ,\left|  {y}\right> \right)  =  0$ y 
$\, d\left(  \left|  {x}\right> ,\left|  {y}\right> \right)  >  0$. 

Si extendemos la noción de distancia para que albergue las posibilidades de distancias nula y negativas, entonces la definición del tensor métrico para espacios pseudoeuclidianos también debe cambiar. 
\[
\mathbf{g}\left[  \left| {x}_{i}\right> ,\left|  {x}_{j}\right> \right]
= g_{ij} \equiv g_{ji} 
\left\{ 
\begin{array}{l}
      < 0    \\
       = 0    \\
        > 0 
\end{array}
\right.
\]

En resumen
\[
\left< {x}\right|  \left.  {x}\right> =
\left\{ 
\begin{array}{l}
      < 0    \\
       = 0    \\
        > 0 
\end{array}
\right\} \,\, \Rightarrow  \,\,  
d\left(  \left|  {x}\right> ,\left|  {y}\right> \right) = 
\left\{ 
\begin{array}{l}
      < 0    \\
       = 0    \\
        > 0 
\end{array}
\right\} \,\, \Rightarrow  \,\,   
\mathbf{g}\left[  \left| {x}_{i}\right> ,\left|  {x}_{j}\right> \right] =
\left\{ 
\begin{array}{l}
      < 0    \\
       = 0    \\
        > 0 
\end{array}
\right.
\]

Este tipo de espacios luce como un excentricidad más de los matemáticos y una curiosidad de estudio es ver como organizar los conceptos que aprendimos de los espacios euclidianos y extenderlos a otros espacios. Quizá se hubiera quedado así, como una curiosidad matemática si los físicos no hubieran sacado partido de estas particularidades para describir el comportamiento de la naturaleza. En la próxima sección analizaremos el caso de espacios minkowskianos de dimensión $4$, que denominaremos $\mathds{M}^{4}$. 

\subsection{Espacios minkowskianos}
\label{EspaciosMinkowskianos}
\index{Espacios!Minkowskianos}
\index{Minkowski!Espacios vectoriales}
Consideremos un espacio tetradimensional expandido por una base ortonormal: 
$\left\{  \left|  \mathrm{e}_{0}\right>, \left|  \mathrm{e}_{1}\right>,
\left| \mathrm{e}_{2}\right>, \left| \mathrm{e}_{3}\right> \right\}$. Los vectores $\left\{ \left|  \mathrm{e}_{1}\right>,  \left| \mathrm{e}_{2}\right>, \left| \mathrm{e}_{3}\right> \right\}$ corresponden con la base canónica de $\mathds{R}^{3}$. 

Este espacio vectorial $\mathds{M}^{4}$ tendrá asociado un espacio dual: 
$\left\{ \left<  \mathrm{e}^{0}  \right|, \left<  \mathrm{e}^{1}  \right|, \left<  \mathrm{e}^{2}  \right|, \left<  \mathrm{e}^{3}  \right| \right\}$ a través de una métrica: 
\[
\eta_{\alpha \beta}\left< \mathrm{e}^{\alpha}\right|
\otimes\left< \mathrm{e}^{\beta}\right|  \equiv \eta_{\beta \alpha}\left<
\mathrm{e}^{\beta}\right|  \otimes\left< \mathrm{e}^{\alpha}\right|
\quad \text{y} \quad 
\eta^{\alpha \beta}\left|  \mathrm{e}_{\alpha }\right> \otimes\left|  \mathrm{e}_{\beta}\right> \equiv \eta^{\beta \alpha}\left|  \mathrm{e}_{\beta}\right>
\otimes\left|  \mathrm{e}_{\alpha}\right>\,,
\]
con $\alpha, \beta = 0,1,2,3$, y donde: $\eta_{0 0} =\eta^{0 0} = 1$, $\eta_{1 1} = \eta^{1 1} = -1$, $\eta_{2 2} = \eta^{2 2} = -1$, $\eta_{3 3} = \eta^{3 3} = -1$ (con $\eta_{\alpha \beta} =0$ para $\alpha \neq \beta$). Se dice que $\eta$ tiene signo $-2$.\footnote{Realmente el signo $-2$ es una convención, se puede también considerar $\eta_{\mu \nu}$ de signo $+2$, con $\eta_{0 0} = -1$, $\eta_{1 1} = +1$,  $\eta_{2 2} = +1$, $\eta_{3 3} = +1$.}   

Tal y lo como presentamos en la sección \ref{TensorMetrico}, podemos asociar componentes covariantes y contravariantes a través de la métricas. Si  
$\left|a\right> = a^{\sigma}\left|  \mathrm{e}_{\sigma}\right>$, entonces:
\[
\left(  \eta_{\alpha \beta}\left< \mathrm{e}^{\alpha}\right|  \otimes \left<\mathrm{e}^{\beta}\right|  \right)  \left|a\right> =
a^{\sigma}\left( \eta_{\alpha \beta}\left< \mathrm{e}^{\alpha}\right|  \otimes\left< \mathrm{e}^{\beta}\right|  \right)  \left|  \mathrm{e}_{\sigma}\right> =a^{\sigma}\eta_{\alpha \beta}\left< \mathrm{e}^{\beta}\right.  \left|  \mathrm{e}_{\sigma}\right>
\left< \mathrm{e}^{\alpha}\right|  =a^{\sigma}\eta_{\alpha \beta}\delta_{\sigma}^{\beta}\left< \mathrm{e}^{\alpha}\right|  =a^{\sigma}\eta_{\alpha \sigma}\left< \mathrm{e}^{\alpha}\right|  \equiv
a_{\alpha} \left< \mathrm{e}^{\alpha}\right| \,.
\]

Lo interesante del caso es que:
\[
a_{\alpha}=a^{\sigma}\eta_{\sigma \alpha}  \,\, \Rightarrow  \,\, 
a^{0} = a_{0}\,, \quad a^{1} = -a_{1}\,, \quad a^{2} = -a_{2}\,, \quad 
a^{3} = -a_{3}.  
\]

Es decir, en este caso, porque la métrica tiene signo $-2$,  bajar los índices espaciales ($\mu= i =1,2,3$) es cambiar el signo a las componentes\footnote{Otra vez, para la métrica con signo $-2$, el cambio de signo entre componentes covariantes y contravariantes se da para la componente, $\mu = 0$}. Dicho con más propiedad, las componentes espaciales  contravariantes ($\mu= i =1,2,3$) tienen signos contrarios a las componentes covariantes.

De la misma manera que se expuso anteriormente en la sección \ref{TensorMetrico}:
\[
\left< a\right|\left(  \eta^{\alpha \beta}\left|  \mathrm{e}_{\alpha }\right> \otimes\left|\mathrm{e}_{\beta}\right> \right)  = 
a_{\sigma}\left<  \mathrm{e}^{\sigma}\right|  \left(  \eta^{\alpha \beta}\left|  \mathrm{e}_{\alpha }\right> \otimes\left|\mathrm{e}_{\beta}\right> \right) =
a_{\sigma}\eta^{\alpha \beta}\left< \mathrm{e}^{\sigma}\right.  \left|\mathrm{e}_{\alpha }\right> \left|  \mathrm{e}_{\beta}\right>=
a_{\sigma}\eta^{\alpha \beta}\delta^\sigma_\alpha \left|  \mathrm{e}_{\beta}\right> =
a_{\sigma}\eta^{\sigma \beta}\left|\mathrm{e}_{\beta}\right> \equiv a^{\beta}\left|  \mathrm{e}_{\beta}\right>\,.
\] 
y otra vez, $a^{\sigma} = \eta^{\sigma \alpha} a_{\alpha}$, y habría cambio de signo cuando se bajan los índices $1,2,3$ para la métrica con signo $-2$ que hemos considerado anteriormente. 

Del mismo modo se ``suben'' y se ``bajan'' índices para componentes de tensores:
\[
\eta^{\alpha \beta}P_{\alpha }^{\gamma \sigma \epsilon}    \equiv P^{\beta \gamma \sigma \epsilon} \,.
\]

Por su parte, el producto interno de dos vectores en un espacio de  Minkowski involucra, de manera natural, la métrica del espacio. Esto es:
\[
\left< a \right.  \left|  b \right> =\left< b \right.  \left| a\right> = 
a^{\alpha}b_{\alpha}=b^{\alpha}a_{\alpha}=
a^{\alpha}b^{\beta} \eta_{\alpha \beta}=
a_{\alpha}b_{\beta}\eta^{\alpha \beta} = 
a^{0}b^{0} - a^{1}b^{1} -a^{2}b^{2} -a^{3}b^{3} =  
a_{0}b_{0} - a_{1}b_{1} -a_{2}b_{2} -a_{3}b_{3}  \,.
\]

Una vez más, la norma de un vector, también incluirá al tensor métrico:
\[
\left\|  \left|a\right> \right\|^{2} = \left< a\right.  \left| a \right> =a_{\alpha}a^{\beta} \left< \mathrm{e}^{\alpha}\right.  \left|
\mathrm{e}_{\beta}\right> =a_{\alpha}a^{\alpha}=a_{\alpha}a_{\beta} \ \eta^{\alpha \beta}=
a^{\alpha}a^{\beta}\ \eta_{\alpha \beta} = a^{0}a^{0} - a^{1}a^{1} -a^{2}a^{2} -a^{3}a^{3} \,.
\]

El caso más conocido lo constituye la norma de un desplazamiento infinitesimal, en un espacio tetradimensional. Para una base genérica, $\left\{  \left| {\mathrm w}_{\beta}\right> \right\}$ (no necesariamente ortogonal) de un espacio vectorial con producto interno. El desplazamiento infinitesimal puede expresarse como:
\[
\mathrm{d}s^{2} \equiv 
\left< \mathrm{d}{r}\right.  \left|  \mathrm{d}{r}\right> =
\left(  \mathrm{d} {x}_{\alpha}\ \left< {\mathrm w}^{\alpha}\right|  \right)  
\left( \mathrm{d} {x}^{\beta}\ \left|  {\mathrm w}_{\beta}\right>\right)  =
\mathrm{d} {x}_{\beta}\ \mathrm{d} {x}^{\beta}=
{\eta}_{\alpha \beta}\ \mathrm{d} {x}^{\alpha}\mathrm{d} {x}^{\beta} = 
\mathrm{d}t^{2} - \mathrm{d}{x}^{2} \,,
\] 
con: $ \mathrm{d}{x}^{2} = \left(\mathrm{d}x^{1}\right)^{2} + \left(\mathrm{d}x^{2}\right)^{2} + \left(\mathrm{d}x^{3}\right)^{2} $.

\subsection{Un toque de Relatividad Especial}
La genialidad de Albert Einstein fue haber entendido que tenía que incorporar el tiempo como otra coordenada más, vale decir, que los eventos que ocurren en la naturaleza están etiquetados por cuatro números: $(t, x, y, z) \equiv (x^{0}, x^{1}, x^{2}, x^{3})$\footnote{Una discusión sobre la necesidad de incorporar los conceptos de relatividad especial en los programas de estudio de Física mediante la utilización del álgebra geométrica la pueden encontrar en Baylis, W. E. (2004). Relativity in introductory physics. Canadian journal of physics, 82(11), 853-873.}. El rápido desarrollo de la comprensión de las ideas relativistas muestra que estaban en el ambiente de la época de comienzos de 1900, y una vez más la simplicidad como prejuicio se impuso. 

Sólo dos suposiciones están en el corazón de la Relatividad Especial:
\begin{enumerate}
  \item \textit{El principio de la Relatividad:} Las leyes de la Física son idénticas en todos los sistemas de referencia inerciales. 
  \item \textit{La universalidad de la velocidad de la luz en el vacío}: La velocidad de la luz en el vacío es siempre la misma, y es independiente de la velocidad de la fuente de luz respecto a un observador en particular.
\end{enumerate}

En términos matemáticos estas dos audaces suposiciones se concretan en una simple suposición matemática: el producto interno entre dos elementos de este espacio tetradimensional, debe conservarse para una familia de vectores base. Luego vendrá la asociación de observadores físicos -o sistemas de coordenadas- con los miembros de la familia de vectores base, pero la idea es la misma que planteamos para los espacios euclidianos en \ref{ProductoInterno}: el producto interno -y consecuentemente, la norma de los elementos del espacio vectorial y la distancia entre éstos- es el mismo independientemente de la base en la cual expanda el espacio vectorial. 

La primera de las interpretaciones es el cómo representamos los eventos en el espacio-tiempo. Supongamos el caso unidimensional en el espacio, vale decir los eventos ocurren en un punto de la recta real $x = x^{1}$, y en un tiempo determinado, por lo tanto podremos asociar al evento un vector evento: $\rightarrow (x^{0},x^{1})$. 

A continuación nos preguntamos que representan las distancias (espacio-temporales) entre estos dos eventos. Tal y como vimos, las distancias entre dos elementos de un espacio vectorial puede ser construida a partir de la norma (de la resta de coordenadas)  y la norma a partir del producto interno:
\[
||\left|y-x\right>||^{2} \equiv \left< y - x\right.\left|y-x\right> 
\left\{
\begin{array}{ll}
 < 0     & \text{conexión tipo espacio: } \text{eventos desconectados causalmente}.   \\
 &\\
 = 0     & \text{conexión tipo luz: } \text{posible conexión causal a través de rayos de luz}.    \\
  &\\
> 0       &  \text{conexión tipo tiempo: } \text{posible conexión causal}.  
\end{array}\right.
\]

Con esta primera interpretación de los valores de la norma y la visión tetradimensional, el espacio-tiempo, dividido en pasado, presente y futuro, se puebla de eventos que pueden estar o no relacionados causalmente tal y como muestra la figura \ref{Figura3_3}.

%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{minipage}{7.4cm}
La preservación del producto interno para todos los observadores era intuitiva en los espacios euclidianos y, al mantenerla para los pseudoeuclidianos nos traerá consecuencias nada intuitivas en nuestra idea intuitiva de ``realidad''.

Para el caso de la formulación de la Relatividad Especial, añadimos un supuesto más: las componentes del tensor métrico son invariantes bajo transformaciones de coordenadas, esto es:
\[
\mathbf{g}\left[  \left| \mathrm{e}_{\mu}\right> ,\left|  \mathrm{e}_{\nu}\right> \right] \equiv \mathbf{\tilde g}\left[  \left| \mathrm{\mathrm{\tilde{e}}}_{\mu}\right> ,\left|  \mathrm{\mathrm{\tilde{e}}}_{\nu}\right> \right] \quad \Leftrightarrow \quad   \eta_{\alpha \beta} = \tilde{\eta}_{\alpha \beta} \,,
\]
con: $\{ \left| \mathrm{e}_{\mu}\right> \}$ y  $\{ \left| \mathrm{\mathrm{\tilde{e}}}_{\mu}\right> \} $  dos bases que se conectan a través de una transformación de coordenadas: 
\[
x^{\mu}= x^{\mu}\left( \tilde{x}^{\alpha} \right)\Leftrightarrow \tilde{x}^{\mu}=\tilde{x}^{\mu}\left( x^{\alpha} \right)\,.
\] 
\end{minipage} \hfill 
\begin{minipage}{8.0cm} 
\includegraphics[width=2.6in]{VOLUMEN_1/03_Funciones_Lineales/Figuras/Figura3_3.jpg}
\caption{Cono de luz, espacio-tiempo y eventos.}
\label{Figura3_3}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%

Construyamos ahora el tipo de transformación de coordenadas que mantiene estos dos supuestos\footnote{Estamos suponiendo que observadores, sistemas de coordenadas y sistemas de referencia son conceptos equivalentes.}:
\begin{enumerate}
\item El producto interno de dos vectores es independiente de la base que expande el espacio vectorial.
 \item Las componentes del tensor métricos son invariantes bajo transformaciones de coordenadas.
\end{enumerate}

Si el producto interno de dos vectores es independiente de la base que expanda el espacio vectorial, tendremos:
\[
\left< x \right.  \left|  y \right> =\left< \tilde{x} \right.  \left| \tilde{y}\right> \quad \Leftrightarrow \quad
 x^{\alpha}y_{\alpha}=\tilde{x}^{\alpha}\tilde{y}_{\alpha} \quad \Leftrightarrow \quad  x^{\alpha}y^{\beta}\eta_{\alpha \beta}=\tilde{x}^{\alpha}\tilde{y}^{\beta}\tilde{\eta}_{\alpha \beta}\, ,
\]
y como lo vimos en \ref{TransformaVectoresTensores} las componentes de vectores, bajo cambio de coordenadas, transforman como:
\[
\tilde{a}^{i}=\frac{\partial \tilde{x}^{i}}{\partial x^{k}}\ a^{k} 
\,\, \Rightarrow  \,\,  
 x^{\alpha}y_{\alpha}=\tilde{x}^{\alpha}\tilde{y}_{\alpha} 
 \,\, \Leftrightarrow \,\,  
 x^{\alpha}y^{\beta}\eta_{\alpha \beta}= \frac{\partial \tilde{x}^{\nu}}{\partial x^{\alpha}} x^{\alpha} \frac{\partial \tilde{x}^{\mu}}{\partial x^{\beta}}y^{\beta}\tilde{\eta}_{\nu \mu} = 
 x^{\alpha} y^{\beta} \frac{\partial \tilde{x}^{\nu}}{\partial x^{\alpha}}  \frac{\partial \tilde{x}^{\mu}}{\partial x^{\beta}} \tilde{\eta}_{\nu \mu} \,,
\]
con lo cual concluimos que:
\begin{equation}
\eta_{\alpha \beta}= \frac{\partial \tilde{x}^{\nu}}{\partial x^{\alpha}}  \frac{\partial \tilde{x}^{\mu}}{\partial x^{\beta}} \tilde{\eta}_{\nu \mu} 
\equiv \frac{\partial \tilde{x}^{\nu}}{\partial x^{\alpha}}\frac{\partial\tilde{x}^{\mu}}{\partial x^{\beta}} \eta_{\nu \mu} \,. 
\label{etatransformado}
\end{equation}

Ahora bien, si derivamos (\ref{etatransformado}) respecto a $x^{\gamma}$ tendremos que: 
\[
0 = \eta_{\nu \mu} \left( \frac{\partial^2 \tilde{x}^{\nu}}{\partial x^{\alpha}\partial x^{\gamma}}  \frac{\partial \tilde{x}^{\mu}}{\partial x^{\beta}}  +
\frac{\partial \tilde{x}^{\nu}}{\partial x^{\alpha}}  \frac{\partial^2 \tilde{x}^{\mu}}{\partial x^{\beta}\partial x^{\gamma}} \right) \, .
\]

Como la cantidad dentro del paréntesis se anula podemos jugar con ésta para descubrir algunas consecuencias ocultas. Es de hacer notar que esa cantidad tiene tres índices libres y por lo tanto son 64 ecuaciones que se anulan. Eso significa que le podemos añadir y sustraer cualesquiera otras con los índices intercambiados. 

Supongamos que al paréntesis anulado le añadimos una con los índices $\alpha$ y $\gamma$ intercambiados y, adicionalmente, le sustraemos una con los índices $\gamma$ y $\beta$ intercambiados. Claramente, estamos añadiendo y sustrayendo ceros.
\[
0 = \eta_{\nu \mu} \left( \frac{\partial^2 \tilde{x}^{\nu}}{\partial x^{\alpha}\partial x^{\gamma}}  \frac{\partial \tilde{x}^{\mu}}{\partial x^{\beta}}  +
\frac{\partial \tilde{x}^{\nu}}{\partial x^{\alpha}}  \frac{\partial^2 \tilde{x}^{\mu}}{\partial x^{\beta}\partial x^{\gamma}} +
 \frac{\partial^2 \tilde{x}^{\nu}}{\partial x^{\gamma}\partial x^{\alpha}}  \frac{\partial \tilde{x}^{\mu}}{\partial x^{\beta}}  +
\frac{\partial \tilde{x}^{\nu}}{\partial x^{\gamma}}  \frac{\partial^2 \tilde{x}^{\mu}}{\partial x^{\beta}\partial x^{\alpha}} -
 \frac{\partial^2 \tilde{x}^{\nu}}{\partial x^{\alpha}\partial x^{\beta}}  \frac{\partial \tilde{x}^{\mu}}{\partial x^{\gamma}}  -
\frac{\partial \tilde{x}^{\nu}}{\partial x^{\alpha}}  \frac{\partial^2 \tilde{x}^{\mu}}{\partial x^{\gamma}\partial x^{\beta}} 
\right) \, .
\] 
Con este truco, vemos que el último término anula el segundo y el penúltimo el cuarto, de forma y manera que nos queda: 
\[
0 = 2\eta_{\nu \mu}\frac{\partial^2 \tilde{x}^{\nu}}{\partial x^{\alpha}\partial x^{\gamma}}  \frac{\partial \tilde{x}^{\mu}}{\partial x^{\beta}} \,,
\]

Con lo cual la única posibilidad que resulta es la siguiente: 
\begin{equation}
0 = \frac{\partial^2 \tilde{x}^{\nu}}{\partial x^{\alpha}\partial x^{\gamma}} 
\,\, \Rightarrow\,\, \tilde{x}^{\nu}=\Lambda^{\nu}_{\mu} x^{\mu} + a^{\nu} \,,
\label{translorentz}
\end{equation}
con: $\Lambda^{\nu}_{\mu}$  y $a^{\nu}$ constantes.

Las transformaciones lineales (\ref{translorentz}) se conocen como \textit{las transformaciones, inhomogéneas, de Lorentz} o también las transformaciones de Poincaré. Estas transformaciones forman un grupo y, uno de los posibles subgrupos lo constituye el conjunto de transformaciones propias de Lorentz de la forma:
\[
\Lambda^{0}_{0} =1, \quad \Lambda^{i}_{0} = \Lambda^{0}_{j} =0, \quad \mathrm{y} \quad \Lambda^{i}_{j} = R^{i}_{j}  \,,
\]
con: $i, j = 1,2,3$, y donde $R^{i}_{j}$ es una matriz de rotación. 

Supongamos el caso más sencillo de este grupo de transformaciones: $a^{\nu} = 0$, en la ecuación (\ref{translorentz}). 

Explícitamente hemos identificado una transformación de la forma: 
\[
\tilde{x}^{\alpha} = \Lambda^{\alpha}_{0}x^{0} + \Lambda^{\alpha}_{1}x^{1} + \Lambda^{\alpha}_{2}x^{2} +\Lambda^{\alpha}_{3}x^{3} \, ,
\]
la cual, por construcción, deja invariante el intervalo tetradimensional:
\[
\mathrm{d}s^{2} = c^2\mathrm{d}t^{2} - \mathrm{d}x^{1} -\mathrm{d}x^{2} - \mathrm{d}x^{3} = \eta_{\mu \nu} \mathrm{d}x^{\mu}{d}x^{\nu} \,, 
\]
con $\eta_{\mu \nu}$ el tensor métrico. Aquí $c$ es la constante que representa la velocidad de la luz en el vacío. Por razones de conveniencia se escoge un sistema de unidades\footnote{Un sistema de unidades denominado {\it sistema de unidades geometrizado} en el cual la velocidad de la luz y la constante de gravitación universal se toman como la unidad: $c=G=1$.} donde $c=1$. 

Es inmediato demostrar que este tipo de transformaciones deja invariante el intervalo. Primero, notemos que: 
\[
\tilde{\eta}^{\mu \nu} = \Lambda^{\mu}_{\alpha}\Lambda^{\nu}_{\beta} \eta^{\alpha \beta} 
\,\, \Rightarrow  \,\,  \eta^{\mu \nu} \eta_{\nu \gamma} = \delta^{\mu}_{\gamma} = \Lambda^{\mu}_{\alpha}\Lambda^{\nu}_{\beta} \eta^{\alpha \beta} \eta_{\nu \gamma} \,\, \Rightarrow  \,\,   
\Lambda^{\mu}_{\alpha}\Lambda_{\gamma}^{\alpha} = \delta^{\mu}_{\gamma}
\]
y como $\mathrm{d}\tilde{x}^{\mu} = \Lambda^{\mu}_{\alpha} \mathrm{d}x^{\alpha}$, entonces: 
\[
\mathrm{d}\tilde{s}^{2} = \tilde{\eta}_{\mu \nu}  \mathrm{d}\tilde{x}^{\mu}\mathrm{d}\tilde{x}^{\nu} \equiv
\eta_{\mu \nu}  \Lambda^{\mu}_{\alpha} \mathrm{d}x^{\alpha}\Lambda^{\nu}_{\beta} \mathrm{d}x^{\beta} =  
\eta_{\alpha \beta}   \mathrm{d}x^{\alpha}\mathrm{d}x^{\beta} = \mathrm{d}s^{2} \,.
\]

Para construir una de las expresiones más utilizadas del grupo de Lorentz consideramos la siguiente situación: un observador, $\tilde{x}^{\mu}$, ve moverse una partícula con una velocidad $\mathbf{v}$, mientras que un segundo observador, $x^{\mu}$, la percibe en reposo.  Entonces, para el observador que registra la partícula en reposo resulta que $\mathrm{d}x^{i}=0$, y: 
\[
\mathrm{d}\tilde{x}^{\mu} = \Lambda^{\mu}_{\alpha} \mathrm{d}x^{\alpha} \,\, \Rightarrow  \,\,   
\left\{
\begin{array}{ll}
\mathrm{d}\tilde{t}     & =  \Lambda^{0}_{0} \, \mathrm{d}t  \\
    &    \\
  \mathrm{d}\tilde{x}^{i}      & = \Lambda^{i}_{\alpha} \mathrm{d}x^{\alpha} = \Lambda^{i}_{0} \, \mathrm{d}t \qquad \text{con } i = 1,2,3. 
\end{array}\right.
\label{dxtilde2}
\]
Ahora bien, combinado (\ref{dxtilde2}), se tiene: 
\[
\mathbf{v} = \frac{\mathrm{d}\tilde{\mathbf{x}}}{\mathrm{d}\tilde{t}} 
\,\, \Rightarrow  \,\,  
v^{i}=\frac{\mathrm{d}\tilde{x}^{i}}{\mathrm{d}\tilde{t}} 
\quad \Rightarrow  \Lambda^{i}_{0} = v^{i}\,\Lambda^{0}_{0} \,.
\]

Además:
\[
\tilde{\eta}_{\alpha \beta} = \Lambda^{\mu}_{\alpha}\Lambda^{\nu}_{\beta} \eta_{\mu \nu}
\,\, \Rightarrow  \,\, 
1 = \Lambda^{\mu}_{0}\Lambda^{\nu}_{0} \eta_{\mu \nu} = 
\left(\Lambda^{0}_{0}\right)^{2} -\left(\Lambda^{1}_{0}\right)^{2} - \left(\Lambda^{2}_{0}\right)^{2} - \left(\Lambda^{3}_{0}\right)^{2} \,, 
\] 
con una solución de la forma:
\[
\Lambda^{0}_{0} = \gamma\,, \quad \Lambda^{i}_{0} = \gamma \, v^{i} \,,
\]
donde:
\[
\gamma = \frac{1}{\sqrt{1 - (\mathbf{v})^2}} \equiv \frac{1}{\sqrt{1 - v^{i}v_{i}} }  \equiv \frac{1}{\sqrt{1 - \left[\left(v^{1}\right)^{2} + \left(v^{2}\right)^{2} +\left(v^{3}\right)^{2}\right]} } \,,
\]
los otros términos $\Lambda^{i}_{j}$ no quedan unívocamente determinados porque está de por medio la arbitrariedad de una rotación $R^{i}_{j}$. 

Por ello, una selección arbitraria pero razonable de todos los términos $\Lambda^{i}_{j}$ es:
\[
\Lambda^{i}_{j} = \delta^{i}_{j} + v^{i}v_{j} \frac{\gamma -1}{ (\mathbf{v})^2} \equiv  
\delta^{i}_{j} + v^{i}v_{j}\frac{\gamma -1}{ v^{k}v_{k} }\,.
\]
De esta forma quedan determinados todos los elementos de las transformaciones de Lorentz.

Los observadores lorentzianos son los equivalentes a los observadores galileanos en las teorías newtonianas: son observadores que se mueven uno respecto al otro con una velocidad constante y, desempeñan el mismo papel que los observadores inerciales. Quizá la consecuencia más impactante de la necesidad de vincular mediciones de distintos observadores lorentzianos a través de transformaciones de Lorentz, lo ilustra la evolución distinta del tiempo medido por los diferentes observadores. Un observador en reposo respecto a un reloj, ve avanzar el tiempo con \textit{tic} separados por $\mathrm{d}t = \Delta t$, ya que su reposo respecto al reloj implica:  $\mathrm{d}x^{i}=0$, por lo tanto la separación espacio temporal será:
\[
\mathrm{d}s^{2} = \mathrm{d}t^{2} - (\mathrm{d}{x}^i)^{2} =\left(\Delta t \right)^{2} \,\, \Rightarrow  \,\, \mathrm{d}t = \Delta t \,,
\]
mientras que un segundo observador, en movimiento,  tendrá el mismo elemento de línea pero expresado como:
\[
\mathrm{d}\tilde{s}^{2} = \mathrm{d}\tilde{t}^{2} - 
(\mathrm{d}{\tilde x}^i)^{2} = \left(1 -\mathbf{v}^{2}\right)  \mathrm{d}\tilde{t} \,\, \Rightarrow  \,\,  
\mathrm{d}\tilde{t} = \frac{\Delta t}{\sqrt{1 -\mathbf{v}^{2}}} \,,
\]
y ésta última ecuación claramente indica que el tiempo evoluciona más lento para relojes en movimiento.


\subsection{{\color{Fuchsia}Ejemplos}} 
\label{ejemploslorentz}
\begin{enumerate}
\item  Como vimos, las transformaciones de Lorentz  relacionan las coordenadas del espacio-tiempo medidas por un observador $O$ de un evento, con las coordenadas medidas por otro observador $\tilde{O}$ del mismo evento. El observador $O$ lo representaremos por las coordenadas $\{t,x,y,z\}=\{ {x}^{0}, {x}^{1}, {x}^{2}, {x}^{3}\}$, mientras que el observador $\tilde{O}$ por $\{\tilde{t},\tilde{x},\tilde{y}, \tilde{z}\}=\{ \tilde{x}^{0}, \tilde{x}^{1}, \tilde{x}^{2}, \tilde{x}^{3}\}$. Para lo que sigue, supondremos que cuando los observadores coincidan los relojes de ambos marcarán $t=t'=0$.

La ecuación (\ref{translorentz}), 
\[
\tilde{x}^{\nu}=\Lambda^{\nu}_{\mu} x^{\mu} \,,
\] 
la podemos expandir en lo que realmente es:
\begin{eqnarray*}
\tilde{t}=\tilde{x}^{0}&=&\Lambda^{0}_{0} x^{0} + 
\Lambda^{0}_{1} x^{1} + \Lambda^{0}_{2} x^{2} + 
\Lambda^{0}_{3} x^{3} \\
\tilde{x}=\tilde{x}^{1}&=&\Lambda^{1}_{0} x^{0} + 
\Lambda^{1}_{1} x^{1} + \Lambda^{1}_{2} x^{2} + 
\Lambda^{1}_{3} x^{3} \\
\tilde{y}=\tilde{x}^{2}&=&\Lambda^{2}_{0} x^{0} + 
\Lambda^{2}_{1} x^{1} + \Lambda^{2}_{2} x^{2} + 
\Lambda^{2}_{3} x^{3} \\
\tilde{z}=\tilde{x}^{3}&=&\Lambda^{3}_{0} x^{0} + 
\Lambda^{3}_{1} x^{1} + \Lambda^{3}_{2} x^{2} + 
\Lambda^{3}_{3} x^{3} 
\end{eqnarray*}

Vamos a suponer que $\tilde{O}$ se mueve respecto a ${O}$, con velocidad $v$, únicamente en la dirección $x$, es decir, $\tilde{y}=y$ y $\tilde{z}=z$. Se supone entonces que $\tilde{t}$ no dependerá ni de $y$ ni de $z$ y que cuando $\tilde{x}=0$ entonces ${x}=vt$. 
Todo esto hace que el sistema de ecuaciones anterior se simplifique de manera significativa: 
\begin{eqnarray*}
\tilde{t}=\tilde{x}^{0}&=&\Lambda^{0}_{0} x^{0} +\Lambda^{0}_{1} x^{1} =
\Lambda^{0}_{0} t +\Lambda^{0}_{1} x  \\
\tilde{x}=\tilde{x}^{1}&=&\Lambda^{1}_{1}(x^1-vt)=\Lambda^{1}_{1} (x-vt)\\
\tilde{y}=\tilde{x}^{2}&=& x^{2} = y\\
\tilde{z}=\tilde{x}^{3}&=& x^{3} = z 
\end{eqnarray*}

Debemos determinar los coeficiente $\Lambda^{0}_{0}$, $\Lambda^{0}_{1}$ y $\Lambda^{1}_{1}$. Para tal fin vamos a suponer que cuando $\tilde{t}=t=0$ un pulso de luz es emitido desde el origen de coordenadas. Recordemos que en ese instante ambos observadores coinciden. Como suponemos además que la velocidad de la luz es constante, la onda de luz se propagará en todas las direcciones de manera que cada observador podrá describirla mediante la ecuación de una esfera cuyo radio aumenta con el tiempo a velocidad $c$. Esto es:
\[
\tilde{x}^2+\tilde{y}^2+\tilde{z}^2=c^2\tilde{t}^2  \quad \mbox{y} \quad
{x}^2+{y}^2+{z}^2=c^2{t}^2 \,.
\]
Por lo tanto:
\[
(\Lambda^{1}_{1} (x-vt))^2+{y}^2+{z}^2=c^2(\Lambda^{0}_{0} t +\Lambda^{0}_{1} x)^2 \,.
\]

Al desarrollar esta última ecuación obtenemos:
\begin{eqnarray*}
\left(\Lambda^{1}_{1}\right)^2\left(x^2-2xvt+v^2t^2\right)+{y}^2+{z}^2&=&c^2\left(\left(\Lambda^{0}_{0}\right)^2 t^2+2\Lambda^{0}_{0}\Lambda^{0}_{1}xt+\left(\Lambda^{0}_{1}\right)^2 x^2 \right) \\
\left(\left(\Lambda^{1}_{1}\right)^2 -c^2\left(\Lambda^{0}_{1}\right)^2 \right)x^2 +{y}^2+{z}^2-2xt\left( \left(\Lambda^{1}_{1}\right)^2 v+c^2 \Lambda^{0}_{0}\Lambda^{0}_{1}   \right)
&=& \left(c^2 \left(\Lambda^{0}_{0}\right)^2 -v^2\left(\Lambda^{1}_{1}\right)^2 \right)t^2 \,.
\end{eqnarray*}
Como ambos observadores registran la misma onda de luz, entonces:
\begin{eqnarray*}
\left(\Lambda^{1}_{1}\right)^2 -c^2\left(\Lambda^{0}_{1}\right)^2 &=&1 \\
\left(\Lambda^{1}_{1}\right)^2 v+c^2 \Lambda^{0}_{0}\Lambda^{0}_{1}&=&0\\
c^2\left(\Lambda^{0}_{0}\right)^2 -v^2\left(\Lambda^{1}_{1}\right)^2&=&c^2 \,.
\end{eqnarray*}

Al resolver este sistema de tres ecuaciones con tres incógnitas resulta:
\[
\Lambda^{1}_{1}=\Lambda^{0}_{0}= \frac{1}{\sqrt{1-\frac{v^2}{c^2}}}\,,\quad \Lambda^{0}_{1}=\frac{v}{c^2\sqrt{1-\frac{v^2}{c^2}}} \,.
\]

Si definimos $\gamma=\frac{1}{\sqrt{1-\frac{v^2}{c^2}}}$, las transformaciones de Lorentz quedan entonces de la forma:
\[
\tilde{t}= \left(t -\frac{v}{c^2} x\right) \gamma \,,\quad
\tilde{x}=\left(x-vt\right)\gamma \,,\quad
\tilde{y}= y\,,\quad
\tilde{z}= z \,.
\]

Existen las transformaciones inversas que se pueden obtener de la misma manera que las anteriores.
\[
{t}= \left(\tilde{t} +\frac{v}{c^2} \tilde{x}\right) \gamma \,,\quad
{x}=\left(\tilde{x}+ v \tilde{t}\right)\gamma \,,\quad
{y}=  \tilde{y}\,,\quad
{z}=  \tilde{z}\,.
\]

Notemos que cuando el observador $\tilde{O}$ se mueve a velocidades muy pequeñas en comparación a $c$, entonces el factor $\gamma$  tiende a la unidad:
\[
v \ll c \,\, \Rightarrow  \,\,   \gamma \rightarrow 1 \,,
\]
por lo tanto: 
\[
\tilde{t}= t   \,,\quad
\tilde{x}=x-vt \,,\quad
\tilde{y}= y\,,\quad
\tilde{z}= z \,.
\]
que no son más que la transformación de Galileo. 

\item  Si un observador $\tilde{O}$ determina que dos eventos ocurren en el mismo lugar, pero  separados en el tiempo; entonces, un observador  ${O}$ ¿cómo los apreciaría? Veamos dos situaciones diferentes. 
\begin{itemize}
\item Consideremos a una persona sentada en un tren en movimiento, observador $\tilde{O}$, esta persona enciende y apaga rápidamente una linterna dos veces, con una diferencia de 15 minutos entre un evento y el otro. Entonces para $\tilde{O}$ será dos eventos que ocurren en el mismo lugar. Pero para el observador ${O}$ que se encuentra en tierra los eventos habrán ocurrido en lugares diferentes, y esto no parece tener ningún conflicto con nuestra experiencia e intuición. 

\item Si $\tilde{O}$ sigue en su asiento del tren y observa 
que dos personas, uno a cada extremo del vagón (a unos 30 mts), encienden las linternas al mismo tiempo, entonces para $\tilde{O}$ será un evento simultáneo, pero para el observador en tierra ${O}$ que ve al tren alejarse determinará que la persona de la parte trasera encendió la linterna un poco antes que la persona apostada en la parte delantera. Esto tal vez si nos resulte algo extraño. 
\end{itemize}

Consideremos que el tren viaja a $36$ m/s ($\approx 130$ Km/h) entonces,  esta pequeña velocidad se refleja en el factor $\gamma$  de la manera siguiente: 
\[
\frac{v}{c}=1.2\times 10^{-7} \rightarrow  \frac{v^2}{c^2}=1.44\times 10^{-14} \rightarrow \gamma=\frac{1}{\sqrt{1-1.44 \times 10^{-14}} }
\approx 1.000000000000007 \,.
\]

Cuando consideramos la primera situación, la diferencia entre un evento y el otro era de $15$ minutos ($900$ seg), y para el observador en tierra ambos eventos ocurren en dos lugares diferentes: $x_1$ y $x_2$,
\[
{x}_1=\left(\tilde{x}_1+ v \tilde{t}_1\right)\gamma \,,\quad
{x}_2=\left(\tilde{x}_2+ v \tilde{t}_2\right)\gamma \,,
\]
de manera que:
\[
{x}_2-{x}_1=\left(\tilde{x}_2+ v \tilde{t}_2\right)\gamma -\left(\tilde{x}_1+ v \tilde{t}_1\right)\gamma= \left(\tilde{x}_2- \tilde{x}_1\right)\gamma +
\left(\tilde{t}_2- \tilde{t}_1\right)v \gamma \,.
\]

Pero: $\tilde{x}_2=\tilde{x}_1$ y $\tilde{t}_2- \tilde{t}_1=15$ min. Por lo tanto:
\[
{x}_2-{x}_1= (900\ \mbox{s}) (36\ \mbox{m}.\mbox{s}^{-1})\gamma = 32400 \mbox{m}= 32.4 \mbox{Km} \,.
\]

Para la segunda situación, el observador en tierra puede dar fe que los dos eventos ocurren en tiempos diferentes: $t_1$ y $t_2$,
\[
t_1=\left(\tilde{t}_1 +\frac{v}{c^2} \tilde{x}_1\right) \gamma\,,\quad
t_2=\left(\tilde{t}_2 +\frac{v}{c^2} \tilde{x}_2\right) \gamma \,,
\]
de manera que:
\[
t_2-t_1=\left(\tilde{t}_2 +\frac{v}{c^2} \tilde{x}_2\right) \gamma -\left(\tilde{t}_1 +\frac{v}{c^2} \tilde{x}_1\right) \gamma =\left(\tilde{t}_2 -\tilde{t}_1\right) \gamma + \left(\tilde{x}_2 -\tilde{x}_1\right) \frac{v}{c^2} \gamma \,,
\]
con: $\tilde{t}_2 =\tilde{t}_1$ y $\tilde{x}_2 -\tilde{x}_1=30$ metros, resulta:
\[
t_2-t_1=\left(30\ \mbox{m}\right)\left[ \frac{ 36\ \mbox{m}.\mbox{s}^{-1} }{(3\times 10^8 \ \mbox{m}.\mbox{s}^{-1})^2}\right] \gamma= 1.2\times 10^{-14}\ \mbox{s} \,.
\]
\end{enumerate}

Intervalo de tiempo muy pequeño pero diferente de cero: el observador en tierra no aprecia que los eventos fueron simultáneos. 

Este valor tan pequeño, obviamente, es debido a que el factor $\gamma$ es casi la unidad. Notemos que si el tren viajara a una velocidad cercana a $c$, digamos $v=0.99999999 \ c$, entonces:
\[
\frac{v}{c}=0.99999999 \rightarrow  \frac{v^2}{c^2}=0.99999998 \rightarrow \gamma=\frac{1}{\sqrt{1-0.99999998}}
\approx 7071.06780 \,.
\]
por lo que:
\[
t_2-t_1=\left(30\ \mbox{m}\right) \left[\frac{ 36\ \mbox{m}.\mbox{s}^{-1} }{(3\times 10^8 \ \mbox{m}.\mbox{s}^{-1})^2}\right] \gamma= 8.49\times 10^{-11}\ \mbox{s} \,.
\]

En vista de lo anterior, se podría esperar que ocurran cosas curiosas con las longitud de los objetos.  Supongamos que el observador que viaja en el tren coloca su laptop en el piso y mide su longitud. Esto significa que la computadora está en reposo para $\tilde{O}$ y, en la dirección del eje $x$, la laptop mide $\tilde{x}_2-\tilde{x}_1$ según $\tilde{O}$. Para el observador que está en tierra la laptop viaja a una velocidad $vt$, de manera que:
\[
\tilde{x}_2-\tilde{x}_1= \left[({x}_2-{x}_1)-v({t}_2-{t}_1) \right]\gamma\,,
\]

Para el observador que se encuentra en reposo la laptop mide ${x}_2-{x}_1$, medida en el instante ${t}_2={t}_1$. Por lo tanto:
\[
{x}_2-{x}_1=\frac{\tilde{x}_2-\tilde{x}_1}{\gamma}\,.
\]
¡La laptop se contrae por el factor $\gamma$!



\subsection{{\color{red}Practicando con Maxima}}

\begin{enumerate}
\item En los grandes aceleradores de partículas se puede producir piones cargados cuando se hace colisionar protones  de gran energía con algún blanco preparado para este fin. Se conoce que los piones tienen una vida media muy corta: $1.77 \times 10^{-8}$ seg. Lo que esto significa es, que una vez que se producen los piones, la mitad de ellos se habrá desintegrado en un tiempo de $1.77 \times 10^{-8}$ seg. 

Los experimentos en el acelerador reflejan el hecho de que la intensidad del haz de piones que emerge del blanco, y que viaja a $0.99 \ c$, se reduce a la mitad una vez que recorren $37.2$ mts. 
Pero sucede que si los piones viajan a $0.99 \ c= 2.968\times 10^8$ m/s cuando estos hayan decaído a la mitad la distancia que habrán recorrido es $d=vt$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
d:(2.968*10^8*(m/s))*(1.77*10^(-8)*s);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
5.25336\,m
\end{math}
\newline

¿Y entonces por qué el experimento mide que viajaron $37.2$ mts? Bueno, no estamos considerando los efectos relativistas que están contenidas en las transformaciones de Lorentz:
\[
t_2-t_1= (\tilde{t}_2-\tilde{t}_1)\gamma \,\, \Rightarrow  \,\,  
\Delta t= \Delta \tilde{t} \gamma \,,
\]
donde $\Delta \tilde{t}$ es el tiempo propio, el tiempo medido por un reloj respecto a la partícula en movimiento, es decir, el sistema de referencia donde el pion se encuentra en reposo. 

Calculemos, primeramente el factor $\gamma$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
gamma:1/sqrt(1-(0.99)^2);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
7.088812050083354
\end{math}
\newline

Por lo tanto, la vida media de los piones medida desde el laboratorio es:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Delta_t:(1.77*10^(-8)*s)*gamma;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
1.254719732864754 \times 10^{-7}\,s
\end{math}
\newline

Según las personas que miden el experimento, y que están en el sistema en reposo, los piones viven $\approx 1.255 \times 10^{-7}$ s,  y por lo tanto viajan una distancia: $x=(0.99 c) \Delta t$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
(2.968*10^8*(m/s))*Delta_t;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
37.24008167142589\,m
\end{math}
\newline 

\item Procederemos ahora a resolver con  {\bf Maxima} el sistema que nos condujo a las transformaciones impropias de Lorentz. 

Primero, escribamos una lista que contiene el sistema de ecuaciones:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sis:[L11^2-c^2*L01^2=1,L11^2*v+c^2*L01*L00=0,c^2*L00^2-v^2*L11^2=c^2];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ {\tt L_{11}}^2-c^2\,{\tt L_{01}}^2=1 , v\,{\tt L_{11}}^2+c^2
 \,{\tt L_{00}}\,{\tt L_{01}}=0 , c^2\,{\tt L_{00}}^2-v^2\,
 {\tt L_{11}}^2=c^2 \right]
\end{math}
\newline

Es importante agregar la información de que $v>0$, y que además $v<c$, para evitar soluciones complejas. El comando {\bf assume} nos permite restringir el valor de los parámetros.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
assume(v>0,c>v);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\left[ v>0 , c>v \right]
\end{math}
\newline

Procedemos a resolver el sistema de ecuaciones, lo podemos hacer con ayuda del comando {\bf solve}:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
S:solve(sis,[L11,L00,L01]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\left[ \left[ {\tt L_{11}}=\frac{c}{\sqrt{c^2-v^2}} , {\tt L_{00}}=
 \frac{c}{\sqrt{c^2-v^2}} , {\tt L_{01}}=\frac{\sqrt{c-v}\,v\,\sqrt{v
 +c}}{c\,v^2-c^3} \right]  \right. , \\ 
\quad \quad  \left[\left[ {\tt L_{11}}=\frac{c}{\sqrt{c^2-v
 ^2}} , {\tt L_{00}}=-\frac{c}{\sqrt{c^2-v^2}} , {\tt L_{01}}=-\frac{
 \sqrt{c-v}\,v\,\sqrt{v+c}}{c\,v^2-c^3} \right] \right. , \\ 
 \quad \quad \left[
 \left[  {\tt L_{11}}=-\frac{c}{\sqrt{c^2-v^2}} , {\tt L_{00}}=\frac{c}{
 \sqrt{c^2-v^2}} , {\tt L_{01}}=\frac{\sqrt{c-v}\,v\,\sqrt{v+c}}{c\,v
 ^2-c^3} \right]  \right. , \\ 
\quad \quad  \left[ 
  \left[ {\tt L_{11}}=-\frac{c}{\sqrt{c^2-v^2}} , 
 {\tt L_{00}}=-\frac{c}{\sqrt{c^2-v^2}} , {\it L_{01}}=-\frac{\sqrt{c
 -v}\,v\,\sqrt{v+c}}{c\,v^2-c^3} \right]  \right] 
\end{math}
\newline

Seleccionamos la primera de las listas anteriores y factorizamos con la rutina {\bf factor}:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
map(factor,S[1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\left[ {\tt L_{11}}=\frac{c}{\sqrt{c^2-v^2}} , {\tt L_{00}}=\frac{c
 }{\sqrt{c^2-v^2}} , {\tt L_{01}}=\frac{\sqrt{c-v}\,v}{c\,\left(v-c
 \right)\,\sqrt{v+c}} \right] 
\end{math}

\end{enumerate}


\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}


\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}

\item En el espacio euclidiano 3D, y en coordenadas cartesianas, no distinguimos entre vectores y $1-$formas debido a que sus componentes transforman de la misma manera. Demuestre que:
\begin{enumerate}
\item ${\tilde a}^{i}=\Lambda^i_j a^j$  $\,\ \wedge \,\,$
${\tilde b}_{j}=\Lambda^i_j b_i$, son la misma transformación si
la matriz $\Lambda^i_j $ es igual a la transpuesta de su inversa, es decir, si es ortogonal. 

\item Considere dos observadores $O:{x, y} \leftrightarrow {x^{1}, x^{2}}$ y $\tilde{O}:{\tilde{x}, \tilde{y}} \leftrightarrow {\tilde{x}^{1}, \tilde{x}^{2} }$ y sus sistemas de coordenadas asociados. 
\begin{enumerate}
\item Considere la siguiente transformación de coordenadas de Galileo:
\[
\tilde{x}^{1} = V^{1}t + \frac{\sqrt{2}}{2}x^{1} - \frac{\sqrt{2}}{2}x^{2} \qquad \text{y} \qquad 
\tilde{x}^{2} = \frac{\sqrt{2}}{2}x^{1} + \frac{\sqrt{2}}{2}x^{2} \,,
\] 
con $V^{1}$ una constante que representa la velocidad relativa entre  $O-\tilde{O}$, y $t$ al tiempo (parámetro de esta transformación).  

A continuación suponga una partícula que describe un movimiento respecto a $O$ siguiendo una trayectoria recta, esto es $x^{2} = \alpha x^{1}$, donde $\alpha$ es una constante. Encuentre cómo lo describiría el observador $\tilde{O}$ respecto a sus coordenadas (${\tilde{x}^{1}, \tilde{x}^{2} }$).

\item Considere ahora la generalización de la transformación de coordenadas anterior:
\[
\tilde{x}^{1} = V^{1}t + \frac{\sqrt{2}}{2}x^{1} - \frac{\sqrt{2}}{2}x^{2} \qquad \text{y} \qquad 
\tilde{x}^{2} = V^{2}t +  \frac{\sqrt{2}}{2}x^{1} + \frac{\sqrt{2}}{2}x^{2} \,,
\] 
con $V^{1}$ y $V^{2}$ las componentes de una velocidad relativa entre  $O-\tilde{O}$, y $t$ al tiempo. Muestre que la norma de cualquier vector queda invariante respecto a una transformación de coordenadas como la anterior y encuentre la matriz de transformación.
\end{enumerate}
\end{enumerate}

\item Un acelerador produce partículas que tienen una vida promedio de $5\mu$s. Estas logran alcanzar  una velocidad de $0.6 c$. 
\begin{enumerate}
\item Un observador que se encuentra en reposo en el laboratorio 
¿qué tiempo de vida media le atribuirá a las partículas?
\item ¿Qué distancia promedio viajan las partículas en el acelerador? 
\item Si suponemos que existe un observador en reposo con respecto a la partícula ¿Qué distancia se desplazará éste observador antes de que la partícula se desintegre? 
\end{enumerate}

\item  Un elemento de área cuadrada se encuentra en reposo para un observador $O$. Encuentre el área medida por un observador en movimiento $\tilde{O}$ si éste lleva una velocidad de $0.85 c$ a lo largo de la diagonal del cuadrado.

\item Consideremos un vehículo se mueve con una velocidad $V$ con respecto a un observador fijo en tierra $O$, y un pasajero dentro del vehículo que se mueve con una velocidad $\tilde{v}$ con respecto al vehículo. Sabemos de los cursos básicos que la velocidad del pasajero respecto al observador fijo en tierra es simplemente la suma: $v=\tilde{v}+V$. Nos podemos preguntar sobre la forma en que se suman las velocidades en la teoría especial de la relatividad, es decir, ¿Cómo se suman las velocidades si consideramos las transformaciones de Lorentz? 
Demuestre que se suman como:
\[
v=\frac{\tilde{v}+V}{1+\tilde{v}\frac{V}{c^2}} \,.
\]

\item Dado un espacio minkowskiano y un observador $O$ que describe los eventos en el espacio-tiempo respecto a un sistema de coordenadas $\{x^{\alpha} \}$, donde $\alpha = 0,1,2$, y $\eta = \mathrm{diag} [ -1,1,1]$ el tensor métrico. Considere entonces la siguiente transformación de coordenadas:
\[
\tilde{x}^{0} = \gamma(x^{0} - \beta x^{1})\,, \quad 
\tilde{x}^{1} = \gamma(x^{1} - \beta x^{0}) \quad \text{y} \quad
\tilde{x}^{2} = x^{2}\,, \quad \text{con  } \gamma = \frac{1}{\sqrt{1 - \beta^{2}}}\,. 
\] 
Donde $\beta = v/c$ es la velocidad relativa entre $O$ y $\tilde{O}$.
\begin{enumerate}
\item Otra vez, suponga que una partícula describe una linea recta respecto a $O$: $x^{2} = \alpha x^{1}$, con $\alpha$ igual a una constante. Encuentre cómo lo describiría el otro observador $\tilde{O}$ respecto a sus coordenadas (${\tilde{x}^{0}, \tilde{x}^{1}, \tilde{x}^{2} }$).
  \item Encuentre la expresión para la transformación de coordenadas, $  \frac{\partial \tilde{x}^{\alpha}}{\partial x^{\beta}} = \Lambda^{\tilde{\alpha}}_{\beta}$ (transformación de Lorentz) entre estos sistemas relativistas  y muestre como la norma: $x^{\alpha}x_{\alpha} = x^{\alpha}x^{\beta} \eta_{\alpha \beta}$,  de cualquier vector  se conserva.

\item Considere el tensor de Maxwell definido como:
  \[ 
F_{\mu \alpha } = \left(\begin{array}{ccc}
  0 & E^{x} & E^{y}  \\
  -E^{x} & 0 & -cB^{z} \\
  -E^{y} & cB^{z} & 0 
  \end{array}\right)\,,
 \quad \text{otra vez con: } \quad
\eta_{\mu \nu } = \left(\begin{array}{cccc}
  -1 & 0 & 0  \\
  0 & 1 & 0  \\
  0 & 0 & 1  
  \end{array}\right)\,.
  \]
donde $\textbf{\em E} = (E^{x}, E^{y}, 0 )$ y $\mathbf{B} = (0, 0, B^{z})$ son los campos eléctricos y magnéticos (respectivamente), medidos por un observador $O$. Si un observador mide un campo eléctrico $\textbf{\em E} = E^{x} \mathbf{i}$ y ningún campo magnético ¿Cuáles campos, $F_{\mu \alpha }$,  medirá otro observador que viaja con una velocidad  ${\boldsymbol \beta} = v\mathbf{i}$?
\end{enumerate}

\item Muestre que las ecuaciones de Maxwell:

\[
\nabla \times \mathbf{B}  -\frac{\partial}{ \partial t}  \textbf{\em E}  = 4 \pi \mathbf{J} \,,  \quad  
\nabla \times \textbf{\em E}  -\frac{\partial }{ \partial t}  \mathbf{B}  = 0  \,, \quad  
\nabla \cdot \mathbf{B}  = 0 \quad \text{y} \quad  \nabla \cdot \textbf{\em E}  =   4 \pi \rho \,,
\] 

\begin{enumerate}
\item se pueden expresar como:
\[
\frac{\partial}{ \partial x^{\nu}} F^{\mu \nu} \equiv F^{\mu \nu},_{\nu} =  
4 \pi J^{\mu}\,, \quad \mathrm{donde} \; J^{\mu} = (\rho, J^{1}, J^{2}) \quad \mathrm{y } \,\, \mathbf{J} = (J^{1}, J^{2}) \,,
\]
\item y la identidad de Bianchi de la forma:
\[
\frac{\partial F_{\mu \nu}}{\partial x^{\gamma}} +\frac{\partial F_{\nu \gamma}}{\partial x^{\mu}} +\frac{\partial F_{\gamma \mu}}{\partial x^{\nu}} \equiv
\partial_{\gamma} F_{\mu \nu} + \partial_{\mu} F_{\nu \gamma} + \partial_{\nu} F_{\gamma \mu} \equiv 
F_{\mu \nu \, ,\gamma} + F_{\nu \gamma \, ,\mu} +F_{\gamma \mu \, ,\nu} = 0 \,.
\]
\end{enumerate}

\item El  dual de un tensor $\mathbf{B}$ de rango 2, cuadridimensional, puede ser definido de manera que sus componentes vienen dadas por:
\[
\mathcal{B}^{ij}=\frac{1}{2!} \epsilon^{ijkl}B_{kl}\,.
\]
Muestre que $\mathcal{B}$ transforma como:
\begin{enumerate}
\item Un tensor de rango 2 bajo rotaciones.
\item Un pseudotensor bajo inversiones.
\end{enumerate}

\item Construya $\mathcal{F}^{\mu \alpha}$, el dual del tensor de Maxwell ${F}^{\mu \alpha}$, dado en el ejercicio 5-c. 
 
\item Demuestre que $c^2 \mathbf{B}^2-\mathbf{E}^2$ es un escalar de Lorentz. 

\item Resuelva los ejercicios anteriores utilizando {\bf Maxima}.

\end{enumerate}
\newpage

\section{Espacios de funciones}
\label{espaciosdefunciones}

Los conceptos de espacios vectoriales discretos que hemos estudiado hasta ahora, finitos o infinitos, se pueden extender a espacios de funciones, en donde los vectores del espacio vectorial y las bases son funciones $\{|{F}_n(x)\rangle\}$. La generalización de bases discretas a continuas se hace transformando el índice de las sumatorias en la variable de una integral, de manera que los índices se convierten en variables continuas definidas en intervalos finitos o infinitos. 

Como primer paso para el estudio de estos espacios introduciremos un nuevo objeto matemático, que como veremos más adelante, es de gran importancia y utilidad. 

\subsection{La función delta de Dirac}
Recordemos que la delta de Kronecker $\delta_{ij}$ que definimos con anterioridad es una cantidad que toma valores discretos: $\delta_{ij}=1$ si $i=j$ y $\delta_{ij}=0$ si $i\neq j$. Es posible construir una extensión de este objeto matemático para que los índices puedan tomar cualquier valor real.
%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{minipage}{7.4cm}
Veamos la siguiente sucesión de funciones gaussianas:

\[
\delta_n(x)=\sqrt{\frac{n}{{\pi}}}e^{-n x^2}\,,
\]

tal que:

\[
\int_{-\infty}^{\infty} \delta_n(x)\mathrm{d}x = 1 \,,\,\,  n>0 \,.
\]

A medida que el parámetro $n$ aumenta su valor, la curva gaussiana $\delta_n(x)$ será cada vez más alta  y más angosta, y si $n \rightarrow \infty$ la curva será entonces infinitamente más alta e infinitamente más angosta, pero siempre conservando que el área bajo la curva será igual a la unidad. 
\end{minipage} \hfill 
\begin{minipage}{8.0cm} 
\includegraphics[width=2.6in]{VOLUMEN_1/03_Funciones_Lineales/Figuras/Figura3_4}
\caption{ $\delta_n(x)$ para tres valores diferentes de $n$.}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%

Esto nos permite definir la ``función'':
\[
\delta(x)=\lim_{n\rightarrow \infty} \sqrt{\frac{n}{{\pi}}}e^{-nx^2}
\,\, \Rightarrow \,\,
\left\{
\begin{array}{lcl}
\delta(x) \rightarrow \infty & \mbox{si} & x \rightarrow 0  \\
& &\\
\delta(x)=0 & \mbox{si} & x \neq 0
\end{array}
\right.
\]

Por tener este comportamiento tan particular no se le suele llamar función, sino una {\it distribución},  y si el punto del máximo es un punto diferente de $x=0$, podemos escribir:
\[
\delta(x-x_0)=\lim_{n\rightarrow \infty} \sqrt{\frac{n}{{\pi}}}e^{-n(x-x_0)^2} \,,
\]
donde $x_0$ es el punto donde la gaussiana alcanza su valor máximo. 

En general, este proceso de limite puede aplicarse a funciones cuya área bajo la curva sea la unidad:
\[
\int_{-\infty}^{\infty} G(x)\mathrm{d}x = 1 \,,
\]
de manera que:
\begin{eqnarray*}
\lim_{n\rightarrow \infty}\int_{-\infty}^{\infty} nG(n(x-x'))f(x')
\mathrm{d}x'&=&
\lim_{n\rightarrow \infty}\int_{-\infty}^{\infty} G(n(x-x'))f\left(\frac{nx'}{n}\right)\mathrm{d}(nx')\\
&=&\lim_{n\rightarrow \infty}\int_{-\infty}^{\infty} G(\tau)f\left(x-\frac{\tau}{n}\right)\ \mathrm{d}\tau=f(x)\,.
\end{eqnarray*}

Si se intercambian los procesos de límite e integración:
\[
f(x)= \int_{-\infty}^{\infty}\left[\lim_{n\rightarrow \infty}
nG(n(x-x'))\right]f(x') \mathrm{d}x' \,\, \Rightarrow\,\,
\delta(x-x')=\lim_{n\rightarrow \infty}nG(n(x-x'))\equiv \delta(x,x')\,,
\]
por lo tanto:
\[
f(x)= \int_{-\infty}^{\infty} \delta(x-x')f(x') \mathrm{d}x'\,\, \Rightarrow\,\,
\left\{
\begin{array}{lcl}
\delta(x,x') \rightarrow \infty & \mbox{si} & x \rightarrow x'  \\
& &\\
\delta(x,x')=0 & \mbox{si} & x \neq x'
\end{array}
\right.
\]

La principal propiedad de la delta de Dirac, $\delta(x-x_0)$,  es que:
\[
\int_{-\infty}^{\infty} f(x) \delta(x-x_0) \mathrm{d}x= f(x_0) \,,
\]
es decir, actúa parecido a como lo hace la delta de Kronecker cuando se contrae uno de sus índices con las componentes de un vector: $a_j=\delta_j^i a_i$, en el sentido que de todos lo valores selecciona sólo uno. Además, si $f(x)=1$ entonces: 
\[
\int_{-\infty}^{\infty}  \delta(x-x_0) \mathrm{d}x= 1\,. 
\]

Existen algunas representaciones integrales basadas en el hecho de que $\delta(x-x_0)$ únicamente existe en $x=x_0$, por ejemplo:
\begin{eqnarray*}
\int_{-\infty}^{\infty}  f(x)\delta(x-x_0) \mathrm{d}x &=& 
\int_{x=x_0-\varepsilon}^{x=x_0+\varepsilon}   f(x)\delta(x-x_0)\mathrm{d}x \,, \\
\int_{a}^{b}  f(x)\delta(x-x_0) \mathrm{d}x &=& 
\left\{
\begin{array}{lcl}
f(x_0) & \mbox{si} & a\leq x_0 \leq b  \\
& &\\
0 & \mbox{si} & x_0 <a \,\,\, \mbox{ó}\,\,\, x_0 >b 
\end{array}
\right.
\end{eqnarray*}
Y la función escalón de Heaviside:  
\[
H(x-x_0)=\int_{-\infty}^{x} \delta(x-x_0) \mathrm{d}x=
\left\{
\begin{array}{lcl}
1 & \mbox{si} & x\geq  x_0   \\
& &\\
0 & \mbox{si} &  x\leq  x_0 
\end{array}
\right. \,\, \Rightarrow\,\,
\delta(x-x_0)=\frac{\mathrm{d}H(x-x_0)}{\mathrm{d}x}\,.
\]

Algunas de las tantas propiedades de la delta de Dirac son las siguientes:
\begin{center}
\begin{tabular}{ll}
$\bullet $ $\delta(x-x_0)=\delta(x_0-x)$ & 
$\bullet $ $\delta(ax)=\frac{\delta(x)}{|a|}$ \\
& \\
$\bullet $ $x\delta(x)=0$ & 
$\bullet $ $f(x)\delta(x-x_0)=f(x_0)\delta(x-x_0)$ \\
& \\
$\bullet $ $\delta(x^2-a^2)=\frac{1}{2|a|}\left[\delta(x-a)+\delta(x+a)\right]$ & 
$\bullet $ $x^{n+1} \frac{\mathrm{d}^n\delta(x)}{\mathrm{d}x^n}=0$\\
& \\
$\bullet $ $\delta({\bf r}-{\bf r}_0)=\delta(x-x_0)\delta(y-y_0)\delta(z-z_0)$ & 
$\bullet $ $\int f({\bf r}) \delta({\bf r}-{\bf r}_0)\mathrm{d}V=f({\bf r}_0)$
\end{tabular}
\end{center}

\subsection{Bases continuas}
Tal y como vimos anteriormente, la representación de un vector $|{F}\rangle$ en un espacio
vectorial abstracto $\textbf{\em V}$ puede darse en término de una base
ortonormal de vectores 
(discreta y finita $B_{DF} = \left\{  \left|  \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right>, \left|\mathrm{e}_{3} \right>, \cdots \left|  \mathrm{e}_{n}\right> \right\}  $ o discreta e infinita $B_{DI}=\left\{  \left|  \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right>, \left|\mathrm{e}_{3} \right> \cdots \left|  \mathrm{e}_{n}\right> \cdots\right\}  $) de la forma:
\[
|{F}\rangle=
\left\{
\begin{array} [c]{l}
c^{i}\ \left|  \mathrm{e}_{i}\right> =
\left< \mathrm{e}^{i}\right|  \left.  {F}\right> \ \left|  \mathrm{e}_{i}\right> 
\,\, \Leftarrow \,\ B_{DF}=\left\{  \left|  \mathrm{e}_{1}\right> ,\ \left|  \mathrm{e}_{2}\right> ,\ \left| \mathrm{e}_{3}\right> \cdots\left|  \mathrm{e}_{n}\right> \right\} \\ \\
c^{i}\ \left|  \mathrm{e}_{i}\right>  =\left< \mathrm{e}^{i}\right|  \left.  {F}\right \rangle \ \left| \mathrm{e}_{i}\right> \,\, \Leftarrow \,\, 
B_{DI}=\left\{  \left|  \mathrm{e}_{1}\right> ,\ \left|  \mathrm{e}_{2}\right> , \left| \mathrm{e}_{3} \right> \cdots \left|  \mathrm{e}_{n} \right> \cdots\right\}
\end{array}
\right.
\]
donde en ambos casos:
\[
c^{i}=\left< \mathrm{e}^{i}\right|  \left.  {F}\right> = c^{j}\ \left< \mathrm{e}^{i}\right.  \left|  \mathrm{e}_{j}
\right> =c^{j}\ \delta_{j}^{i}\,.
\]

Recapitulemos ahora algunos puntos que hemos tratado con anterioridad, y con el fin de aclarar conceptos, para el caso de bases discretas de funciones $\{\left|\mathrm{e}_i(x)\right>\}$.

La relación de ortogonalidad viene dada, como hemos mencionado, por:
\[
\langle{\mathrm{e}^n(x)}|{\mathrm{e}_m(x)}\rangle=\int_{a}^{b}e^{*}_{n}(x)e_{m}(x)\mathrm{d}x=\Theta_n\delta^n_{m} \,,
\]
y con una norma definida como:
\[
\left\|  \left|\mathrm{e}_i(x)\right> \right\|^{2}=
\langle{\mathrm{e}_n(x)}|{\mathrm{e}_n(x)}\rangle=
\int_{a}^{b}\left|e_{n}(x)\right|^2 \mathrm{d}x=\Theta_n \,.
\]
La base será ortonormal si $\Theta_n=1$. 

Con una base $\{\left|\mathrm{e}_i(x)\right>\}$ definida, es posible representar cualquier función $\left|f(x)\right>$ cuadrado integrable, es decir que cumpla con:
\[
\int_{a}^{b}\left|f(x)\right|^2 \mathrm{d}x \neq \infty \,,
\]
en una combinación lineal de la base dada:
\[
\left|f(x)\right>= C^i\left|\mathrm{e}_i(x)\right>
\]

Las cantidades $C^i$, las componentes de $\left|f(x)\right>$, se obtienen de la siguiente manera:
\[
\left< \mathrm{e}^{i}(x) \right|\left. f(x)\right>=
 C^j\left< \mathrm{e}^{i}(x) \right|\left. \mathrm{e}_j(x)   \right>=
 C^j \delta^i_{j}=C^i=
\int_{a}^{b} f(x) e^{*}_{i}(x) \mathrm{d}x  \,,
\]

Resulta conveniente para los cálculos que vienen a continuación cambiar 
$i \rightarrow n$ y $x\rightarrow x'$, de manera que: 
\[
C^n=\int_{a}^{b} f(x') e^{*}_{n}(x') \mathrm{d}x'\,.
\]

Por lo tanto:
\begin{eqnarray*}
\left|f(x)\right>= C^n\left|\mathrm{e}_n(x)\right> \,\, \Rightarrow \,\, 
\left|f(x)\right>&=& \sum_{n}\left[\int_{a}^{b} f(x') e^{*}_{n}(x') \mathrm{d}x' \right]\left|\mathrm{e}_n(x)\right>= \sum_{n} \int_{a}^{b} f(x') e^{*}_{n}(x')\mathrm{e}_n(x) \mathrm{d}x'\\
&=&  \int_{a}^{b} f(x') \sum_{n} e^{*}_{n}(x')\mathrm{e}_n(x) \mathrm{d}x' \,.
\end{eqnarray*}
Utilizando la delta de Dirac:
\[
\left|f(x)\right>=  \int_{a}^{b} f(x') \delta(x-x')\mathrm{d}x'= 
\int_{a}^{b} f(x') \left[ \sum_{n} e^{*}_{n}(x')\mathrm{e}_n(x) \right]\mathrm{d}x'
\,\, \Rightarrow \,\, \delta(x-x')=  \sum_{n} e^{*}_{n}(x')\mathrm{e}_n(x)\,.
\]

Esta última relación viene a ser una generalización de la relación de cierre mencionada en la sección \ref{OrtogonalidadBases}, y se conoce como {\it condición de completitud} para la base  $\{\left|\mathrm{e}_i(x)\right>\}$, además,  también resulta ser una representación en serie para la delta de Dirac.

De todo esto surgen algunas propiedades fundamentales:

\begin{itemize}
\item Se dice que el conjunto $\{\left|\mathrm{e}_i(x)\right>\}$ es {\it completo} si para $\left|f(x)\right>= C^i\left|\mathrm{e}_i(x)\right>$, entonces se tiene que: 
\[  
\int_{a}^{b} \left| f(x)\right|^2=\sum_{n}  \left| C^n\right|^2 \,.
\]
\item Si $\left|f(x)\right>= C^i\left|\mathrm{e}_i(x)\right> $ y 
$ \left| g(x)\right>= E^i\left|\mathrm{e}_i(x)\right>$, entonces $\langle{f(x)}|{g(x)}\rangle= C_n^{*} E^n$.
\end{itemize}

Es posible pasar de estos espacios vectoriales discretos, donde las bases son un conjunto numerable de elementos $\{\mathrm{e}_{n}(x)\}$, a espacios vectoriales de funciones con las bases  dadas por funciones y donde el índice $n$ pasa a ser una variable continua. 

Recordemos que en la sección \ref{OrtogonalidadBases}  hablábamos de un conjunto de funciones $\left\{  \left|  \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right>, \left|  \mathrm{e}_{3}\right>, \cdots ,\left|  \mathrm{e}_{n}\right> \cdots\right\}$ definidas por:
\[
\left|  \mathrm{e}_{0}\right> =1,\quad\left|  \mathrm{e}_{2n-1} \right> =\cos(nx)\quad\text{y}\quad\left|  \mathrm{e}_{2n}\right>= \operatorname{sen}(nx),\quad\text{con } n=1,2,3,\cdots \,,
\]
la base discreta de Fourier. Base que también podemos escribir de la forma compleja, como se muestra a continuación:
\[
\left|  \mathrm{e}_{n}(x)\right>= \frac{1}{\sqrt{2L}}e^{i\frac{n\pi}{L}x}= 
\frac{1}{\sqrt{2L}}e^{i k x} \,,\quad
\mbox{con  } n=-\infty\dots \infty \,\, \mbox{y} \,\, -L\leq x\leq L\,.
\]

Cuando $n$ aumenta indefinidamente, también lo hará la cantidad $k=\frac{n\pi}{L}$,  convirtiéndose entonces en una variable continua, es decir, $\Delta k=\frac{\Delta n\pi}{L}\rightarrow 0$. Notemos que los índices de la delta de Kronecker $\delta_{nm}=\delta_{nn'}$ pueden tomar los valores: $n=\frac{Lk}{\pi}, n'=\frac{Lk'}{\pi}$, y en el proceso de  $L \rightarrow \infty$ se convertirán en: 
\[
\delta_{nn'} \,\,\rightarrow \,\,\delta\left(\frac{Lk}{\pi} - \frac{Lk'}{\pi}\right) =
\delta\left(\frac{L}{\pi}( k- k')\right)  = \frac{\pi}{L}\delta\left( k- k' \right)\,.
\]

De esta manera, la base ortonormal quedará escrita en función de las variables continuas $k$ y $x$, como se muestra a continuación: 
\[
\left|  \mathrm{e}(k,x)\right>= \frac{1}{\sqrt{2\pi}}e^{i k x}\,.
\]

El hecho de que sean ortonormales se refleja en la siguiente condición  (que es una extensión de la expresión para el producto interno de bases discretas) 
\[
\langle \mathrm{e}(k,x) | \mathrm{e}(k,x)\rangle=
\int_a^b  \mathrm{e}(k,x)^{\ast} \mathrm{e}(k',x)\  \mathrm{d}x=\delta(k-k') \,,\quad
\mbox{con } \quad \alpha \leq k \leq \beta \,,\,\, a \leq x \leq b
\] 

Al ser el conjunto de funciones $\{\left|  \mathrm{e}(k,x)\right>\}$ una base, toda función del mismo espacio vectorial se puede expandir en esa base como:
\[
\left| f(x)\right>=\int_\alpha^\beta \mathrm{C}(k) \mathrm{e}(k,x)\mathrm{d}k \,.
\]

En correspondencia con el caso de las bases discretas, el coeficiente $\mathrm{C}(k)$ viene a ser algo parecido a las componentes de $ \left| f(x)\right>$. Y para calcularlas se procede como se muestra a continuación:
\[
 \left<\mathrm{e}(k',x)\right.\left|f(x)\right>=
\int_a^b \mathrm{e}(k',x)^{\ast} f(x)\ \mathrm{d}x =
\int_\alpha^\beta \mathrm{C}(k) \left[ \int_a^b  \mathrm{e}(k',x)^{\ast}  \mathrm{e}(k,x)\   \mathrm{d}x \right] \mathrm{d}k =
\int_\alpha^\beta \mathrm{C}(k) \delta(k-k') \mathrm{d}k = \mathrm{C}(k')\,.
\]

Volvamos a cambiar la notación para los índices: $k' \rightarrow k$, $x \rightarrow x'$ en la última ecuación:
\[
\mathrm{C}(k)=\int_a^b \mathrm{e}(k,x')^{\ast} f(x') \mathrm{d}x' \,,
\]
por lo tanto:
\[
\left| f(x)\right>=\int_\alpha^\beta \mathrm{C}(k) \left|  \mathrm{e}(k,x)\right> \mathrm{d}k = \int_a^b f(x') \left[\int_\alpha^\beta \mathrm{e}(k,x')^{\ast}\mathrm{e}(k,x)  \mathrm{d}k\right] \mathrm{d}x' =
 \int_a^b f(x') \delta(x-x') \mathrm{d}x' \,,
\]
y la relación: 
\[
\int_\alpha^\beta \mathrm{e}(k,x')^{\ast}\mathrm{e}(k,x)  \mathrm{d}k=\delta(x-x') \,,
\]
será la relación de cierre para el conjunto no numerable de funciones ortonormales. 

Una integral como la que nos permitió definir $\mathrm{C}(k)$, digamos, del tipo:
\[
F(k)=\int_a^b f(x') \mathrm{e}(k,x')^{\ast}  \mathrm{d}x' 
\quad \Rightarrow 
\left| f(x)\right>=\int_\alpha^\beta F(k) \mathrm{e}(k,x) \mathrm{d}k \,,
\]
es lo que se denomina una transformada de $f(x)$. Más adelante veremos que si la base, es por ejemplo $e^{ikx}/\sqrt{2\pi}$, la transformada será la de Fourier.

En el espacio vectorial de funciones de cuadrado integrable $\mathcal{L}^{2}$, definidas en $\mathds{R}^{3}$, tendremos que:
\[
\left|  {F}\right> =c^{i}\ \left|  \mathrm{e}_{i}\right>  \equiv 
\left< \mathrm{e}^{i}\right|  \left.  {F}\right>\ \left|  \mathrm{e}_{i}\right> =\sum_{i=0}^{\infty}\left( \int_{-\infty}^{\infty}\mathrm{d}^{3}r^{\prime} \xi_{i}^{\ast}\left( \mathbf{r}^{\prime}\right)  \ f\left(  \mathbf{r}^{\prime}\right)  \right) \left|  \mathrm{e}_{i}\right>\,,
\]
que se reescribe en términos de funciones como:
\[
f\left(  \mathbf{r}\right)  =\sum_{i=0}^{\infty}\left(  \int_{-\infty}^{\infty}\mathrm{d}^{3}r^{\prime}\xi_{i}^{\ast}\left(  \mathbf{r}^{\prime}\right)  \ f\left(  \mathbf{r}^{\prime}\right)  \right)  \xi_{i}\left(
\mathbf{r}\right) \,.
\]

Es claro que se pueden intercambiar los símbolos de $\int$ y $\sum$, por lo cual:
\[
f\left(  \mathbf{r}\right)  =\int_{-\infty}^{\infty}\mathrm{d}^{3}r^{\prime}\ f\left(  \mathbf{r}^{\prime}\right)  \underset{\mathcal{G}(\mathbf{r}^{\prime},\mathbf{r})}{\underbrace{\left[  \sum_{i=0}^{\infty}\xi_{i}^{\ast}\left(  \mathbf{r}^{\prime}\right)  \xi_{i}\left(  \mathbf{r}\right)\right]  }} \,,
\]
la función $\mathcal{G}(\mathbf{r}^{\prime},\mathbf{r})$, que depende de los argumentos $\mathbf{r}^{\prime}$ y $\mathbf{r}$, vive dentro de las integrales y convierte:
\[
f\left(  \mathbf{r}\right)  =\int_{-\infty}^{\infty}\mathrm{d}^{3}r^{\prime}\ f\left(  \mathbf{r}^{\prime}\right)  \ \mathcal{G}(\mathbf{r}^{\prime },\mathbf{r}) \,.
\]
Podemos ver entonces que:
\[
f\left(  \mathbf{r}\right)  =\int_{-\infty}^{\infty}\mathrm{d}^{3}r^{\prime
}\ f\left(  \mathbf{r}^{\prime}\right)  \ \mathbf{\delta}(\mathbf{r}^{\prime}-\mathbf{r}) \,.
\]

En resumen, la generalización de bases discretas a continuas se hace transformando el índice de la sumatoria en la variable de una integral:
\[
\left| {\Psi}\right> =\int\mathrm{d}\alpha\ c\left(\alpha\right)  \ \left|  {w}_{\alpha}\right> 
\,\, \Rightarrow \,\,
c\left(  \beta\right)  =\left< {w}_{\beta}\right.  \left|{\Psi}\right> =\int\mathrm{d}\alpha\ c\left(  \alpha\right)\ \left< {w}_{\beta}\right.  \left|  {w}_{\alpha}\right> =\int\mathrm{d}\alpha\ c\left(  \alpha\right)  \ \delta\left(\alpha-\beta\right) \,.
\]

Así, para los conceptos expresados hasta ahora se tiene el siguiente tabla resumen:

\begin{center}
\begin{tabular}
[c]{|c|c|c|}\hline
\textbf{Propiedad$\backslash$Base} & \textbf{Discreta} & \textbf{Continua}\\\hline
\textsf{Ortogonalidad} & $\left< {u}^{i}\right.  \left|
{u}_{j}\right> =\delta_{j}^{i}$ & $\ \left< {w}
_{\beta}\right.  \left|  {w}_{\alpha}\right> =\delta\left(
\alpha-\beta\right)  $\\\hline
\textsf{Cierre} & ${1=}\sum_{j=0}^{\infty}\ \left|  {u}
_{j}\right> \left< {u}^{j}\right|  $ & ${1=}
\int\mathrm{d}\alpha\ \left|  {w}_{\alpha}\right> \left<
{w}_{\alpha}\right|  $\\\hline
\textsf{Expansión } & $\left|  {F}\right> =\sum_{i=0}
^{\infty}c^{i}\ \left|  {u}_{i}\right> $ & $\left|  {\Psi
}\right> =\int\mathrm{d}\alpha\ c\left(  \alpha\right)  \ \left|
{w}_{\alpha}\right> $\\\hline
\textsf{Componentes} & $c^{i}=\left< {u}^{i}\right|  \left.
{F}\right> $ & $c\left(  \beta\right)  =\left<
{w}_{\beta}\right.  \left|  {\Psi}\right> $\\\hline
\textsf{Producto Interno} & $\left< {G}\right|  \left.
{F}\right> =\sum_{i=0}^{\infty}\ g^{i\ast}\ f_{i}$ &
$\left< {G}\right|  \left.  {F}\right>
=\int\mathrm{d}\alpha\ g^{\ast}\left(  \alpha\right)  \ f\left(
\alpha\right)  $\\\hline
\textsf{Norma} & $\left< {F}\right|  \left.  {F}
\right> =\sum_{i=0}^{\infty}\ \left|  f_{i}\right|  ^{2}$ &
$\left< {F}\right|  \left.  {F}\right>
=\int\mathrm{d}\alpha\ \left|  f\left(  \alpha\right)  \right|  ^{2}$\\\hline
\end{tabular}
\end{center}

\subsection{Bases de ondas planas y la transformada de Fourier}

En las teorías de oscilaciones es de gran importancia considerar el problema de la transformada de Fourier. Recientemente vimos que una función $f(x)$ puede representarse de la forma:
\[
\left| f(x)\right>=\int_\alpha^\beta \mathrm{C}(k) \mathrm{e}(k,x) \mathrm{d}k\,,
\]
si la función $f(x)$ está definida en $(-\infty, \infty)$ y si tomamos al conjunto de vectores base al conjunto de funciones continuas y ortonormales $\{\mathrm{e}(k,x)\}$ como $\{\mathrm{e}^{ikx}/\sqrt{2\pi}\}$, entonces la integral anterior la podemos escribir de la forma:
\begin{equation}
\left| f(x)\right>=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \mathrm{F}(k) \mathrm{e}^{ikx}\ \mathrm{d}k\,,
\label{fxdirecta}
\end{equation}

Si utilizamos la relación de ortogonalidad: 
$\langle \mathrm{e}(k,x) | \mathrm{e}(k,x)\rangle=
\int  \mathrm{e}(k,x) \mathrm{e}(k',x)^{\ast}\  \mathrm{d}x=\delta(k-k')$, resulta:
\[
\int_{-\infty}^\infty \mathrm{e}^{i(k-k')x}\ \mathrm{d}x=2\pi\delta(k-k')\,.
\]
Donde:
\[
\left|\mathrm{F}(k)\right>=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty f(x) \mathrm{e}^{-ikx}\ \mathrm{d}x\,.
\]

A las variables $x$ y $k$ se les denominan variables conjugadas de Fourier (no conjugadas como en variable compleja) y a $\mathrm{F}(k)$ la transformada de Fourier de $f(x)$ (y viceversa).

Notemos que si sustituimos $\mathrm{F}(k)$ en la otra integral, ecuación  (\ref{fxdirecta}), entonces:
\[
\frac{1}{{2\pi}}\int_{-\infty}^\infty \int_{-\infty}^\infty 
f(x) \mathrm{e}^{ik\left(x-x'\right)}\ \mathrm{d}k \ \mathrm{d}x' \,\, \Rightarrow  \,\,  
\left| f(x')\right>=\int_{-\infty}^\infty  f(x) \delta(x-x') \ \mathrm{d}x' \,.
\]

Por otra parte, podemos ver que la integral para $\mathrm{F}(k)$ se puede hacer en dos partes:
\[
\left|\mathrm{F}(k)\right>=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty f(x) \mathrm{e}^{-ikx}\ \mathrm{d}x 
\,\, \Rightarrow  \,\,
\frac{1}{\sqrt{2\pi}}\int_{-\infty}^0 f(x) \mathrm{e}^{-ikx}\ \mathrm{d}x + \frac{1}{\sqrt{2\pi}}\int_{0}^\infty f(x) \mathrm{e}^{-ikx}\ \mathrm{d}x \,.
\]

Si sustituimos  $x\rightarrow-x$, en la primera integral de la ecuación anterior resulta: 
\[
\left|\mathrm{F}(k)\right>=
\frac{1}{\sqrt{2\pi}}\int_{0}^\infty f(-x) \mathrm{e}^{ikx}\ \mathrm{d}x + \frac{1}{\sqrt{2\pi}}\int_{0}^\infty f(x) \mathrm{e}^{-ikx}\ \mathrm{d}x \,.
\]

Dos casos podemos considerar:
\begin{itemize}
\item Si la función es par, $f(x)=f(-x)$:
\[
\left|\mathrm{F}(k)\right>=
\frac{1}{\sqrt{2\pi}}\left[\int_{0}^\infty f(x) \mathrm{e}^{ikx}\ \mathrm{d}x +\int_{0}^\infty f(x) \mathrm{e}^{-ikx}\ \mathrm{d}x\right]=\sqrt{\frac{2}{\pi}}\int_{0}^\infty f(x) \cos(kx)\ \mathrm{d}x \,,
\]
entonces, de la transformada al hacer $\mathrm{F}(k)=\mathrm{F}(-k)$ se tiene:
\[
\left| f(x)\right>=\sqrt{\frac{2}{\pi}}\int_{0}^\infty \mathrm{F}(k) \cos(kx)\ \mathrm{d}k\,.
\]
Estas dos funciones se conocen como las transformadas coseno de Fourier. 

\item Si la función es impar, $f(x)=-f(-x)$:
\[
\left|\mathrm{F}(k)\right>=\frac{1}{\sqrt{2\pi}}\left[\int_{0}^\infty f(-x) \mathrm{e}^{ikx}\ \mathrm{d}x -\int_{0}^\infty f(-x) \mathrm{e}^{-ikx}\ \mathrm{d}x\right]=i\sqrt{\frac{2}{\pi}}\int_{0}^\infty f(x) \mbox{sen}(kx)\ \mathrm{d}x \,.
\]

Ahora al hacer $\mathrm{F}(k)=-\mathrm{F}(-k)$ se tiene: 
\[
\left| f(x)\right>=\sqrt{\frac{2}{\pi}}\int_{0}^\infty \mathrm{\tilde F}(k)  \mbox{sen}(kx)\ \mathrm{d}k\,, 
\]
donde ${\tilde F}(k)=i{F}(k)$. Tenemos ahora las transformadas seno de Fourier.
\end{itemize}

La generalización a $ \mathds{R}^n$ es obvia. Si $f=f({\bf r})$, entonces: 
\[
\left| f({\bf {r}})\right>=\left[\frac{1}{\sqrt{2\pi}}\right]^{\frac1n}\int_{-\infty}^\infty \cdots  \int_{-\infty}^\infty \mathrm{F}({\bf {k}}) \mathrm{e}^{i({\bf k}\cdot {\bf r})}\ \mathrm{d}{\bf {k}}\,,
\]
donde $\mathrm{d}{\bf {k}}=\mathrm{d}{ {k}}_1\mathrm{d}{ {k}}_2\mathrm{d}{ {k}}_3\dots \mathrm{d}{ {k}}_n$. Para la transformada:
\[
\left|\mathrm{F}({\bf k})\right>=\left[\frac{1}{\sqrt{2\pi}}\right]^{\frac1n}
\int_{-\infty}^\infty \cdots  \int_{-\infty}^\infty f({\bf r})
\mathrm{e}^{-i({\bf k}\cdot {\bf r})}\ \mathrm{d}{\bf {r}}\,,
\]
donde $\mathrm{d}{\bf {r}}=\mathrm{d}{ {x}}_1\mathrm{d}{ {x}}_2\mathrm{d}{ {x}}_3\dots \mathrm{d}{ {x}}_n$, representa el elemento de volumen en $ \mathds{R}^n$.

\subsubsection{Ondas planas}

Como un ejemplo de lo anterior, consideraremos la base de las ondas planas. 
Si a $k$ la llamaremos $s$ y a la variable $x$ el tiempo $t$,  vale decir:
\[
F(s)=\frac{1}{\sqrt{2\pi}}{\displaystyle\int_{-\infty}^{\infty}}
\mathrm{d}t\ {\large e}^{i st}\ f(t) \quad\rightleftarrows\quad f(t)=
\frac{1}{\sqrt{2\pi}}{\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}s\ {\large e}^{-i st}\ F(s) \,.
\]

De esta manera, a la función $F(s)$ se le denomina la distribución espectral de $f(t)$ y a $|F(s)|^2$ la densidad espectral, es decir, la intensidad de la onda en el intervalo $[s,s+\Delta s]$. Esto significa que la energía total es:
\[
E=\int_{-\infty}^\infty |F(s)|^2\mathrm{d}s\,.
\]


También podemos reescribir las transformadas en términos de la posición y el momento:
\[
\psi\left(  x\right)  =\frac{1}{\sqrt{2\pi\hbar}}{\displaystyle\int_{-\infty}^{\infty}}
\mathrm{d}p\ {\large e}^{i({px}/{\hbar})}\ \bar{\psi}\left(  p\right)\quad\rightleftarrows\quad\bar{\psi}\left(  p\right)  =\frac{1}{\sqrt{2\pi\hbar}}{\displaystyle\int_{-\infty}^{\infty}}
\mathrm{d}x\ {\large e}^{-i({px}/{\hbar})}\ \psi\left(  x\right) \,.
\]

Hemos tenido cuidado de incluir los factores de normalización adecuados para el caso de las descripciones en Mecánica Cuántica. Estas fórmulas pueden ser reinterpretadas en función de los conceptos anteriormente expuestos y podemos definir una base continua de la forma:
\[
\psi\left(  x\right)  =\frac{1}{\sqrt{2\pi\hbar}}
{\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}p\ \underset{v_{p}\left(  x\right)  }{\underbrace{\left(  \frac
{1}{\sqrt{2\pi\hbar}}{\large e}^{i({px}/{\hbar})}\right)  }}\ \bar{\psi}\left(p\right)  \quad\rightleftarrows\quad\bar{\psi}\left(  p\right)  =\frac{1}{\sqrt{2\pi\hbar}}{\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}x\ \underset{v_{p}^{*}\left(  x\right)  }{\underbrace{\left(\frac{1}{\sqrt{2\pi\hbar}}{\large e}^{-i({px}/{\hbar})}\right)  }}\ \psi\left(x\right)\,,
\]
por lo cual:
\[
\psi\left(  x\right)  ={\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}p\ v_{p}\left(  x\right)  \ \bar{\psi}\left(  p\right)\quad\rightleftarrows\quad\bar{\psi}\left(  p\right)  ={\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}x\ v_{p}^{\ast}\left(  x\right)  \ \psi\left(  x\right)\,.
\]

Diremos que la función $\psi\left(  x\right)$ está expresada en la base de ondas planas $v_{p}\left(  x\right)  =\frac{1}{\sqrt{2\pi\hbar}}e^{i({px}/{\hbar})}$.

Nótese lo siguiente:
\begin{itemize}
\item  El índice $p$ de $v_{p}\left(  x\right)  $ varía de forma continua entre $-\infty$ a $\infty$.

\item  Que $v_{p}\left(  x\right)  =\frac{1}{\sqrt{2\pi\hbar}}e^{i({px}/{\hbar})}\notin\mathcal{L}^{2}$, es decir, no pertenece al espacio vectorial de funciones de cuadrado integrable ya que su norma diverge: 
\[
\left< {v}_{p}\right|  \left.  {v}_{p}\right> ={\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}x\ \left|  v_{p}\left(  x\right)  \right|  ^{2}={\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}x\ \frac{1}{2\pi\hbar}\rightarrow\infty \,.
\]

\item  Que las proyecciones de $\psi\left(  x\right)$ sobre la base de ondas planas es: $\bar{\psi}\left(  p\right)  =\left< {v}_{p}\right|\left.  {\psi}\right> $.

\item  La relación de cierre para esta base se expresa como:
\[
{\Large 1}\mathbf{=}\int\mathrm{d}\alpha\ \left|  {v}_{\alpha}\right> \left< {v}_{\alpha}\right|  
\quad \rightleftarrows \quad{\displaystyle\int_{-\infty}^{\infty}}
\mathrm{d}p\ v_{p}^{\ast}\left(  x^{\prime}\right)  \ v_{p}\left(  x\right)  ={\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}p\ \frac{1}{2\pi\hbar}{\large e}^{i\left[p\left(x^{\prime}-x\right)
/\hbar\right]}=\mathbf{\delta}\left(  x^{\prime}-x\right)\,,
\]
mientras que de la definición de producto interno se obtiene:
\[
\left< {v}_{p^{\prime}}\right|  \left.  {v}_{p}\right> ={\displaystyle\int_{-\infty}^{\infty}}
\mathrm{d}x\ v_{p^{\prime}}^{\ast}\left(  x\right)  \ v_{p}\left(  x\right)  ={\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}x\ \frac{1}{2\pi\hbar}{\large e}^{i\left[x\left(  p^{\prime}-p\right)
/\hbar\right]}=\mathbf{\delta}\left(  p^{\prime}-p\right)\,.
\]
\end{itemize}

En este mismo orden de ideas, podemos construir otra base continua$\xi_{\mathbf{r}_{0}}\left(  \mathbf{r}\right)  $ a partir de la utilización de las propiedades de la delta de Dirac. Esto es:
\[
\psi\left(  \mathbf{r}\right)  =\int_{-\infty}^{\infty}\mathrm{d}^{3}
r_{0}\ \psi\left(  \mathbf{r}_{0}\right)  \ \underset{\xi_{\mathbf{r}_{0}
}\left(  \mathbf{r}\right)  }{\underbrace{\mathbf{\delta}(\mathbf{r}
_{0}-\mathbf{r})}} \quad\rightleftarrows\quad\psi\left(  \mathbf{r}
_{0}\right)  =\int_{-\infty}^{\infty}\mathrm{d}^{3}r\ \psi\left(
\mathbf{r}\right)  \ \mathbf{\delta}\left(  \mathbf{r}-\mathbf{r}_{0}\right) \,,
\]
por lo cual la reinterpretación es inmediata:
\[
\psi\left(  \mathbf{r}\right)  =\int_{-\infty}^{\infty}\mathrm{d}^{3}
r_{0}\ \psi\left(  \mathbf{r}_{0}\right)  \ \xi_{\mathbf{r}_{0}}\left(
\mathbf{r}\right)\,,  \quad\text{con: }\qquad\psi\left(  \mathbf{r}_{0}\right)
=\left< \xi_{\mathbf{r}_{0}}\right|  \left.  \mathbf{\psi}\right>
=\int_{-\infty}^{\infty}\mathrm{d}^{3}r\ \xi_{\mathbf{r}_{0}}^{\ast}\left(
\mathbf{r}\right)  \ \psi\left(  \mathbf{r}\right) \,,
\]
más aún, la ortogonalidad queda garantizada por la relación de cierre:
\[
\left< \xi_{\mathbf{r}_{0}}\right|  \left.  \xi_{\mathbf{r}_{0}
}\right> =\int_{-\infty}^{\infty}\mathrm{d}^{3}r_{0}\ \xi_{\mathbf{r}
_{0}}^{\ast}\left(  \mathbf{r}\right)  \ \xi_{\mathbf{r}_{0}}\left(
\mathbf{r}^{\prime}\right)  =\int_{-\infty}^{\infty}\mathrm{d}^{3}
r_{0}\ \mathbf{\delta}\left(  \mathbf{r}-\mathbf{r}_{0}\right)
\ \mathbf{\delta}\left(  \mathbf{r}^{\prime}-\mathbf{r}_{0}\right)
=\mathbf{\delta}\left(  \mathbf{r}^{\prime}-\mathbf{r}\right) \,,
\]
al igual que:
\[
\left< \xi_{\mathbf{r}_{0}}\right|  \left.  \xi_{\mathbf{r}_{0}^{\prime
}}\right> =\int_{-\infty}^{\infty}\mathrm{d}^{3}r\ \xi_{\mathbf{r}_{0}
}^{\ast}\left(  \mathbf{r}\right)  \ \xi_{\mathbf{r}_{0}^{\prime}}\left(
\mathbf{r}\right)  =\int_{-\infty}^{\infty}\mathrm{d}^{3}r\ \mathbf{\delta
}\left(  \mathbf{r}-\mathbf{r}_{0}\right)  \ \mathbf{\delta}\left(
\mathbf{r}-\mathbf{r}_{0}^{\prime}\right)  =\mathbf{\delta}\left(
\mathbf{r}_{0}^{\prime}-\mathbf{r}_{0}\right) \,.
\]

\subsection{Las representaciones $\left| {r}\right> $ y \textbf{\ }$\left|  {p}\right> $}

A partir de las bases de ondas planas $v_{p_{0}}\left(  x\right)$, y de distribuciones, $\xi_{\mathbf{r}_{0}}\left(  \mathbf{r}\right)$, construimos las llamadas representaciones $\left|  {r}\right> $ y $\left|
{p}\right> $ de la forma siguiente. 

Primero asociamos:
\[
\xi_{\mathbf{r}_{0}}\left(  \mathbf{r}\right)  \quad   \rightleftarrows
\quad\left|  {r}_{0}\right>  \,\,\, \wedge \,\,\,
v_{p_{0}}\left(  x\right)  \quad   \rightleftarrows\quad \left|
{p}_{0}\right> \,.
\]

De esta forma, dada las bases $\left\{  \xi_{\mathbf{r}_{0}}\left(\mathbf{r}\right)  \right\}  $ y $\left\{  v_{p_{0}}\left(  x\right) \right\} $ para el espacio vectorial $\textbf{\em V}$ definiremos dos
``representaciones'':  la representación de coordenadas, $\left|{r}_{0}\right> ,$ y la representación de momentos $\left|{p}_{0}\right> $ de $\textbf{\em V}$, respectivamente. De tal modo que:
\begin{align*}
\left< {r}_{0}\right|  \left.  {r}_{0}^{\prime}\right>  &  =\int_{-\infty}^{\infty}\mathrm{d}^{3}r\ \xi_{\mathbf{r}
_{0}}^{\ast}\left(  \mathbf{r}\right)  \ \xi_{\mathbf{r}_{0}^{\prime}}\left(\mathbf{r}\right)  =\mathbf{\delta}\left(  {r}_{0}^{\prime}-{r}_{0}\right)  \,\, \Rightarrow \,\,  {\Large 1} =\int\mathrm{d}^{3}r_{0}\ \left|  {r}_{0}\right> \left< {r}_{0}\right| \,, \\
\left< {p}_{0}\right|  \left.  {p}_{0}^{\prime}\right>  &  =
{\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}^{3}r\ v_{p_{0}^{\prime}}^{\ast}\left(  \mathbf{r}\right)
\ v_{p_{0}}\left(  \mathbf{r}\right)  ={\displaystyle\int_{-\infty}^{\infty}}\mathrm{d}^{3}r\ \frac{1}{2\pi\hbar}{\large e}^{-i (\mathbf{r_{0}\cdot p}_{0}/\hbar)}=\mathbf{\delta}\left(  \mathbf{p}_{0}^{\prime}-\mathbf{p}
_{0}\right)  \,\, \Rightarrow \,\, {\Large 1} &  =\int\mathrm{d}^{3}p_{0}\ \left|  {p}_{0}\right>
\left< {p}_{0}\right| \,.
\end{align*}

Podemos entonces expresar el producto interno para la representación de coordenadas como:
\[
\left< {\Phi}\right.  \left|  {\Psi}\right>=\left< {\Phi}\right|  \underset{{\Large 1}}{\underbrace{\left(
\int\mathrm{d}^{3}r_{0}\ \left|  {r}_{0}\right> \left<{r}_{0}\right|  \right)  }}\left|  {\Psi}\right>
=\int\mathrm{d}^{3}r_{0}\ \phi^{\ast}({\bf r}_{0})\psi({\bf r}_{0}) \,,
\]
y equivalentemente para la representación de momentos:
\[
\left< {\Phi}\right.  \left|  {\Psi}\right>=\left< {\Phi}\right|  \underset{{\Large 1}}{\underbrace{\left(
\int\mathrm{d}^{3}p_{0}\ \left|  {p}_{0}\right> \left<{p}_{0}\right|  \right)  }}\left|  {\Psi}\right>
=\int\mathrm{d}^{3}p_{0}\ \phi^{\ast}({\bf p}_{0})\psi({\bf p}_{0}) \,,
\]
por lo cual hemos encontrado que:
\begin{align*}
\left|  {\Psi}\right>  &  =\int\mathrm{d}^{3}r_{0}\ \left|{r}_{0}\right> \left< {r}_{0}\right|  \left.
{\Psi}\right> =\int\mathrm{d}^{3}p_{0}\ \left|  {p}_{0}\right> \left< {p}_{0}\right|  \left.  {\Psi}\right> \,, \\
\psi({\bf r}_{0}) &  =\left< {r}_{0}\right.  \left|{\Psi}\right> \qquad\text{y}\qquad\psi({\bf p}_{0})=\left<
{p}_{0}\right.  \left|  {\Psi}\right> \,,
\end{align*}
que es la representación de $\left| {\Psi}\right> $ en coordenadas, $\psi(r_{0})$, y en momentos, $\psi(p_{0})$. 

Adicionalmente cuando $\left|  {\Psi}\right> =\left|  {p}\right> $ tendremos que:
\begin{align*}
\left< {r}_{0}\right.  \left|  {p}_{0}\right>  &
=\left< {r}_{0}\right|  \underset{{\Large 1}}{\underbrace{\left(
\int\mathrm{d}^{3}r_{0}^{\prime}\ \left|  {r}_{0}^{\prime}\right>
\left< {r}_{0}^{\prime}\right|  \right)  }}\left|  {p}
_{0}\right> =\frac{1}{\left(  2\pi\hbar\right)^{3/2}} \int\mathrm{d}^{3}
r_{0}^{\prime}\ \delta\left(  {\bf r}_{0}^{\prime}-{\bf r}_{0}\right)
\ \mathrm{e}^{\frac{i}{\hbar}\left({\bf p}_{0}\cdot{\bf r}_{0}\right)}=\frac{1}{\left(  2\pi\hbar\right)^{3/2}}\ \mathrm{e}^{\frac{i}{\hbar}\left({\bf p}_{0}\cdot {\bf r}_{0}\right)} \,,
\end{align*}
con lo cual $\psi(p_{0})$ puede considerarse la transformada de Fourier de $\psi(r_{0})$, y denotaremos de ahora en adelante las bases $\left|{r}_{0}\right> \equiv\left|  {r}\right> $ y $\left|
{p}_{0}\right> \equiv\left|  {p}\right>$. 

Estos índices continuos, $\mathbf{r}_{0}$ y $\mathbf{p_{0}}$, representan tres índices continuos $\mathbf{r\rightleftharpoons}\left(  x,y,z\right)  $ y
$\mathbf{p\rightleftharpoons}\left(  p_{x},p_{y},p_{z}\right)$. La
proyección de un vector abstracto $\left| {\Psi}\right> $ en
la representación $\left|  {r}\right> $ será considerada
como su expresión en el espacio de coordenadas, igualmente su
proyección $\left< {p}\right.  \left|  {\Psi}\right>$ será su expresión en el espacio de los momentos. Eso nos permitirá hacer corresponder los elementos de espacios vectoriales abstractos  con elementos de un espacio vectorial de funciones. Por lo tanto todas las fórmulas de proyección quedan como:
\[
\left< {r}\right.  \left|  {\Psi}\right>=\psi(\mathbf{r})\quad\text{y}\quad\left< {p}\right.  \left|
{\Psi}\right> =\psi(\mathbf{p}) \,.
\]
mientras que las relaciones de cierre y ortonormalización son:
\begin{align*}
\left< {r}\right|  \left.  {r}^{\prime}\right>  &
={\delta}\left(  \mathbf{r}^{\prime}-\mathbf{r}\right)  \quad\text{y}\quad
{\Large 1}=\int\mathrm{d}^{3}r\ \left|  {r}\right>\left< {r}\right| \,, \\
\left< {p}\right|  \left.  {p}\right>  &
={\delta}\left(  \mathbf{p}^{\prime}-\mathbf{p}\right)  \quad\text{y}\quad
{\Large 1}=\int\mathrm{d}^{3}p\ \left|  {p}\right>\left< {p}\right| \,.
\end{align*}

Por su parte, la relación de cierre hará corresponder a la expresión del producto interno de dos vectores, tanto en la representación de las coordenadas como en la representación de momentos, en una de la forma:
\begin{eqnarray*}
\left< {\Phi}\right|  \left(  \int\mathrm{d}^{3}r\ \left|
{r}\right> \left< {r}\right|  \right)  \left|
{\Psi}\right> =\int\mathrm{d}^{3}r\ \phi^{\ast}(\mathbf{r}
)\ \psi(\mathbf{r}) \,\,\, \wedge \,\,\,
\left< {\Phi}\right|  \left(  \int\mathrm{d}^{3}p\ \left|
{p}\right> \left< {p}\right|  \right)  \left|
{\Psi}\right> =\int\mathrm{d}^{3}p\ \bar{\phi}^{\ast}
(\mathbf{p})\ \bar{\psi}(\mathbf{p}) \,,
\end{eqnarray*}
donde $\bar{\phi}^{\ast}(\mathbf{p})$ y $\bar{\psi}(\mathbf{p})$ son las transformadas de Fourier de $\phi^{\ast}(\mathbf{r})$ y $\psi(\mathbf{r})$, respectivamente. La afirmación anterior queda evidentemente demostrada del cambio entre las bases $\left|  {r}\right> $ y $\left|{p}\right>$. Esto es:
\[
\left< {r}\right.  \left|  {p}\right> =\left<{p}\right.  \left|  {r}\right> ^{\ast}=\frac{1}{\left(  2\pi
\hbar\right)^{3/2}}\ {\Large e}^{\frac{i}{\hbar}(\mathbf{p}\cdot\mathbf{r})} \,,
\]
por lo cual:
\[
\psi(\mathbf{r})=\left< {r}\right.  \left|  {\Psi}\right> =\left< {r}\right|  \left(  \int\mathrm{d}
^{3}p\ \left|  {p}\right> \left< {p}\right|\right)  \left|  {\Psi}\right> =\int\mathrm{d}^{3}p\ \left<
{r}\right.  \left|  {p}\right> \left< {p}\right|  \left.  {\Psi}\right> =\frac{1}{\left(  2\pi\hbar\right)^{3/2}} \int\mathrm{d}^{3}p\ {\Large e}^{\frac{i}{\hbar}({\bf p}\cdot\mathbf{r})}\ \bar{\psi}(\mathbf{p}) \,,
\]
e inversamente:
\[
\psi(\mathbf{p})=\left< {p}\right.  \left|  {\Psi}\right> =\left< {p}\right|  \left(  \int\mathrm{d}
^{3}r\ \left|  {r}\right> \left< {r}\right|\right)  \left|  {\Psi}\right> =\int\mathrm{d}^{3}r\ \left<
{p}\right.  \left|  {r}\right> \left< {r}\right|  \left.  {\Psi}\right> =
\frac{1}{\left(  2\pi\hbar\right)^{3/2}}\int\mathrm{d}^{3}r\ {\Large e}^{-\frac{i}{\hbar}(\mathbf{p}
\cdot\mathbf{r})}\psi(\mathbf{r}) \,.
\]
\newpage

\subsection{{\color{Fuchsia}Ejemplos}} 

\begin{enumerate}
\item Una distribución de carga eléctrica puede escribirse de manera sencilla utilizando la delta de Dirac a través de la siguiente integral:
\[
q=\int_V \rho({\bf r})\mathrm{d}V \,,
\]
de manera que para una carga puntual localizada en ${\bf r}={\bf r}_0$ se tiene que:
\[
q=q\underbrace{\int \delta({\bf r}-{\bf r}_0)\mathrm{d}V}_{=1}=\int_V \rho({\bf r})\mathrm{d}V 
\,\, \Rightarrow \,\,  \rho({\bf r})=q\delta({\bf r}-{\bf r}_0)\,.
\]

\item Demostremos que: 
\[
\delta(\alpha x)=\frac{\delta(x)}{\alpha} \,,\qquad \alpha > 0.
\]

Entonces, haremos el siguiente cambio de variable:  $y=\alpha x$. 

\[
\int_{-\infty}^{\infty} f(x)\delta(\alpha x)\mathrm{d}x=
\int_{-\infty}^{\infty} f\left(\frac{y}{\alpha}\right)\delta(y) \frac{1}{\alpha}
\mathrm{d}y=
\frac{1}{\alpha}\int_{-\infty}^{\infty} f\left(\frac{y}{\alpha}\right)\delta(y) 
\mathrm{d}y=\frac{f(0)}{\alpha}=
\int_{-\infty}^{\infty} f(x)\delta(x)\mathrm{d}x \,,
\]
por lo tanto: $\delta(\alpha x)=\frac{\delta(x)}{\alpha}$. 

La condición  $\alpha>0$ es porque se debe tener en cuenta que la función delta de Dirac es par, es decir,  $\delta(\alpha x)=\delta(-\alpha x)$. De manera que resulta más apropiado escribir:
\[
\delta(\alpha x)=\frac{\delta(x)}{|\alpha|}\,.
\]

\item Demostremos ahora  que:
\[
\delta(g(x))=\sum_\alpha \frac{\delta(x-\alpha)}{|g'(\alpha)|}\,, \quad 
g(\alpha)=0  \,\, \wedge \,\,  g'(\alpha)\neq 0\,.
\]

Podemos ver que para la función $g(x)$, en los intervalos donde se hace cero, se tiene:
\[
g(x)\approx g(\alpha)+(x-\alpha)g'(\alpha) \,, \quad 
-\varepsilon <  \alpha < \varepsilon \,.
\]
De manera que si sumamos para todos esos intervalos:
\[
\int_{-\infty}^{\infty} f(x)\delta(g(x))\mathrm{d}x=\sum_\alpha \int_{\alpha-\varepsilon}^{\alpha+\varepsilon} f(x)\delta\left( (x-\alpha)g'(\alpha)  \right)\mathrm{d}x = 
\int_{-\infty}^{\infty} f(x)\sum_\alpha \frac{\delta\left(x-\alpha\right)}{|g'(\alpha)|}\mathrm{d}x \,,
\]
donde hemos utilizado el resultado del ejemplo anterior. Por lo tanto:
\[
\delta(g(x))=\sum_\alpha \frac{\delta\left(x-\alpha\right)}{|g'(\alpha)|}\,.
\]

\item Podemos ver la transformada de Fourier de la derivada de una función. Si
\[
\left| f(x)\right>=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \mathrm{F}(k) \mathrm{e}^{ikx}\ \mathrm{d}k 
\,\,  \Rightarrow \,\, \frac{\mathrm{d}}{\mathrm{d} x^n}\left| f(x)\right>=
\frac{(ik)^n}{\sqrt{2\pi}}\int_{-\infty}^\infty \mathrm{F}(k) \mathrm{e}^{ikx}\ \mathrm{d}k \,.
\]
\newpage

\item Encontremos la transformada de Fourier de la siguiente función:
\label{ejemplo5s35}
%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{minipage}{7.4cm}
\[
f(t)=\left\{
\begin{array}{cc}
\mbox{sen}(\omega_0\ t) & -\frac{n\pi}{\omega_0}< t < \frac{n\pi}{\omega_0} \\
 & \\
0 & t< -\frac{n\pi}{\omega_0} \,\, \wedge \,\, t > \frac{n\pi}{\omega_0}
\end{array}
\right. 
\]

La función $f(t)$ representa un tren de ondas finito con $n$ ciclos en el intervalo dado, como se aprecia en la figura \ref{funciontren}. 

Por el hecho de ser $f(t)$ una función impar, podemos utilizar la transformada seno de Fourier:
\begin{eqnarray*}
\left|\mathrm{F}(\omega)\right>&=&
\frac{2}{\sqrt{2\pi}}\int_{0}^{\frac{n\pi}{\omega_0}} 
f(t) \mbox{sen}(\omega t)\ \mathrm{d}t \\
&=&
\frac{2}{\sqrt{2\pi}}\int_{0}^{\frac{n\pi}{\omega_0}} 
\mbox{sen}(\omega_0 t) \mbox{sen}(\omega t)\ \mathrm{d}t\,.
\end{eqnarray*}

\end{minipage} \hfill 
\begin{minipage}{8.0cm} 
\includegraphics[width=2.6in]{VOLUMEN_1/03_Funciones_Lineales/Figuras/Figura3_5}
\caption{ $f(t)$ con: $n=6$ y $\omega_0=\pi$.}
\label{funciontren}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%

Resolviendo la integral obtenemos la distribución espectral:
\[
\left|\mathrm{F}(\omega)\right>=
\frac {2 \left[
\omega_0\,\mbox{sen}\left( { \tau \,\omega} \right) \cos \left(n \pi \right)  - \omega\,\cos \left( { \tau \,\omega} \right) \mbox{sen}\left(n \pi \right)  \right] }{\sqrt{2\pi}\left({\omega}^{2}-{\omega_0}^{2}\right)} =
\frac {1}{\sqrt {2\pi }} \left[ \frac {\sin\left[ \tau \left( \omega_0-\omega \right)\right]}{2(\omega_0-\omega)} -\frac {\sin\left[ \tau \left( \omega_0+\omega \right)\right]}{2(\omega_0+\omega)}
 \right] \,,
\]
donde $\tau={\frac {n\pi}{\omega_0}}$.
%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{minipage}{9.0cm}

Se puede demostrar que los límites:
\[
\lim_{\omega_0 \rightarrow \infty}\left|\mathrm{F}(\omega)\right>=\lim_{\omega \rightarrow \infty}\left|\mathrm{F}(\omega)\right>=0 \,.
\] 

Se puede apreciar también que el primer término, de la expresión entre corchetes, es el de mayor relevancia debido a que en el denominador aparece la diferencia: $\omega_0-\omega$. 

En la Figura \ref{Figura3_6} se muestra  la función de distribución y en ella podemos notar que  a medida que $n$ crece la función $\left|\mathrm{F}(\omega)\right>$ es una delta de Dirac en $\omega=\omega_0=\pi$.

Por otro lado, es fácil ver que los ceros ocurren cuando:
\[
\frac{\omega_0-\omega}{\omega_0}=\pm \frac1n, \pm \frac2n, \pm \frac3n \dots
\]
\end{minipage} \hfill 
\begin{minipage}{6.0cm} 
\includegraphics[width=2.4in]{VOLUMEN_1/03_Funciones_Lineales/Figuras/Figura3_6}
\caption{ $\mathrm{F}(\omega)$ con: $n=6$ y $\omega_0=\pi$.}
\label{Figura3_6}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%

\end{enumerate}
\newpage


\subsection{{\color{red}Practicando con Maxima}}

Estudiaremos en este módulo como calcular transformadas integrales de Fourier con {\bf Maxima}. Disponemos de las siguientes funciones: {\bf fourint}(f,x) que calcula y devuelve la lista de los coeficientes integrales de Fourier de $f(x)$ definida en el intervalo $(-\infty, \infty)$. La función {\bf fourintcos} (f,x) que devuelve los coeficientes integrales de los cosenos $f(x)$ en $(0, \infty)$ y la función {\bf fourintsin}(f,x) que retorna los coeficientes integrales de los senos $f(x)$ en $(0, \infty)$

Vamos a aclarar a que se refieren cuando se habla de los coeficientes integrales.
 
Para {\bf Maxima} si $f(x)$ es una función definida en $(-\infty, \infty)$ la transformada de Fourier exponencial es:
\[
F(f,z)=\frac{1}{2\pi}\int_{-\infty}^{\infty} f(x)\ e^{izx} \mathrm{d}x
\,\, \Leftrightarrow \,\, 
f(x)=\int_{-\infty}^{\infty} f(x)\ e^{-izx} \mathrm{d}z\,.
\]

Como vimos anteriormente podemos tener los siguientes casos:
\begin{itemize}
\item La función es par: $f(-x)=f(x)$, entonces: $F(f,-z)=F(f, z)$ y podemos utilizar la representación en transformadas coseno de Fourier:
\[
F(f,z)=\frac12 F_{\mbox{\small cos}}(f,z)\,.
\]

\item La función es impar: $f(-x)=-f(x)$, entonces: $F(f,-z)=-F(f,z)$ y podemos utilizar la representación en transformadas seno de Fourier:
\[
F(f,z)=\frac{i}{2} F_{\mbox{\small sen}}(f,z)\,.
\]

\item La función no es par ni impar. Pero $f(x)$ siempre se puede escribir como la suma de una función par $f_{+}(x)$ y una impar $f_{-}(x)$:
\[
f(x)=f_{+}(x)+f_{-}(x)=\frac{1}{2}\left[f(x)+f(-x) \right]+\frac{1}{2}\left[f(x)-f(-x) \right] \,,
\]
de manera equivalente:
\[
F(f,z)=F(f_{+}+f_{-},z)=\frac{1}{2}F_{\mbox{\small cos}}(f_{+},z)+\frac{i}{2} F_{\mbox{\small sen}}(f_{-}, z) \,.
\]
\end{itemize}

Entonces, la librería {\bf fourie} lo que genera es la transformada de Fourier de la forma:
\[
F(f,z)=\frac{1}{2}a_z+\frac{i}{2}b_z \,.
\] 

Veamos algunos ejemplos:


\begin{enumerate}
\item Consideremos la siguiente función par: 
\[
f=x^2 e^{-|x|} \,, \quad \mbox{con  }  x \in (-\infty, \infty) \,.
\]

Podemos intentar resolver la integral que define la transformada de Fourier de manera ``manual'' o directa:
\[
\int_{-\infty}^{\infty} x^2 e^{-|x|} \ e^{-ikx} \mathrm{d}x \,.
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f:x^2*exp(-abs(x));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
x^2\,e^ {- \left| x\right|  }
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
integrate(exp(-%i*k*x)*f,x,minf,inf);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\int_{ -\infty }^{\infty }{x^2\,e^{-\left| x\right| -i\,k\,x}\;dx}
\end{math}
\newline

Lo que significa que el programa no supo resolverla. Intentemos ahora utilizar las funciones propias del programa. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(fourie)$
\end{verbatim}}
\end{minipage}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
fourint(f,x);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t4) }
a_{z}=\frac{2\,\left(\frac{2}{z^6+3\,z^4+3\,z^2+1}-\frac{6\,z^2}{z^
 6+3\,z^4+3\,z^2+1}\right)}{\pi}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t5) }
b_{z}=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ {\tt \%t4} , {\tt \%t5} \right]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ratsimp(%t4),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
a_{z}=-\frac{4\,\left(3\,z^2-1\right)}{\pi\,\left(z^2+1\right)^3}
\end{math}
\newline

Esta es la notación que el programa utiliza para decirnos que la transformada es
\[
F(k)=\frac12\left[-\frac{4\,\left(3\,k^2-1\right)}{\pi\,\left(k^2+1\right)^3}\right]=-\frac{2\,\left(3\,k^2-1\right)}{\pi\,\left(k^2+1\right)^3} \,.
\]

Notemos que el cálculo anterior se corresponde a la transformada coseno de Fourier:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
fourintcos(f,x);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t7) }
a_{z}=\frac{2\,\left(\frac{2}{z^6+3\,z^4+3\,z^2+1}-\frac{6\,z^2}{z^
 6+3\,z^4+3\,z^2+1}\right)}{\pi}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\left[ {\tt \%t{7}} \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ratsimp(%t7),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
a_{z}=-\frac{4\,\left(3\,z^2-1\right)}{\pi\,\left(z^2+1\right)^3}
\end{math}
\newline

Podemos intentar resolver directamente la integral para la transformada coseno de Fourier. Para tal fin escribimos toda la expresión a resolver.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Fz:ratsimp(1/%pi*integrate(cos(x*z)*f,x,0,inf)),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
-\frac{2\,\left(3\,z^2-1\right)}{\pi\,\left(z^2+1\right)^3}
\end{math}
\newline

Incluso podemos probar calculando la transformada inversa y verificar que los cálculos son correctos.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
integrate(exp(-%i*z*x)*Fz,z,minf,inf);
\end{verbatim}}
\end{minipage}
{\color{red}Is x positive, negative or zero?} p;

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
x^2\,e^ {- x }
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
integrate(exp(-%i*z*x)*Fz,z,minf,inf);
\end{verbatim}}
\end{minipage}
{\color{red}Is x positive, negative or zero?} n;

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
x^2\,e^ {x }
\end{math}


\item Consideremos la siguiente función impar:
\[
f=x e^{-|x|} \,.
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f:x*exp(-abs(x));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
x\,e^ {- \left| x\right|  }
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
fourint(f,x);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t13) }
a_{z}=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t14) }
b_{z}=\frac{4\,z}{\pi\,\left(z^4+2\,z^2+1\right)}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\left[ {\tt \%t13} , {\tt \%t14} \right]
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ratsimp(%t14),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
b_{z}=\frac{4\,z}{\pi\,\left(z^2+1\right)^2}
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
fourintsin(f,x);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t16) }
b_{z}=\frac{4\,z}{\pi\,\left(z^4+2\,z^2+1\right)}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
\left[ {\tt \%t16} \right]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ratsimp(%t16),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
b_{z}=\frac{4\,z}{\pi\,\left(z^2+1\right)^2}
\end{math}
\newline

Esto significa que la transformada seno de Fourier es:
\[
F(k)=\frac{i}{2}\left[\frac{4\,k}{\pi\,\left(k^2+1\right)^2}\right]=
\frac{2 i \,k}{\pi\,\left(k^2+1\right)^2} \,.
\]

Realicemos nuevamente el cálculo pero integrando  para comparar.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Fz:%i*ratsimp(1/%pi*integrate(sin(x*z)*f,x,0,inf)),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
\frac{2\,i\,z}{\pi\,\left(z^2+1\right)^2}
\end{math}
\newline

Vamos a calcular la transformada inversa:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
integrate(exp(-%i*z*x)*Fz,z,minf,inf);
\end{verbatim}}
\end{minipage}
{\color{red}Is x positive, negative or zero?} p;

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o19) }
x\,e^ {- x }
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i20) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
integrate(exp(-%i*z*x)*Fz,z,minf,inf);
\end{verbatim}}
\end{minipage}
{\color{red}Is x positive, negative or zero?} n;

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o20) }
x\,e^ {x }
\end{math}
\newline

\item Dada la siguiente función que no es par ni impar:
\[
f(x)=(x-1)e^{(-|x|)}\,.
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i21) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f:(x-1)*exp(-abs(x));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o21) }
\left(x-1\right)\,e^ {- \left| x\right|  }
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i22) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
fourint(f,x);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t22) }
a_{z}=-\frac{2}{\pi\,\left(z^2+1\right)}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t23) }
b_{z}=\frac{4\,z}{\pi\,\left(z^4+2\,z^2+1\right)}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o23) }
\left[ {\tt \%t{22}} , {\tt \%t{23}} \right]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i24) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
az:ratsimp(%t22),factor; bz:ratsimp(%t23),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o24) }
a_{z}=-\frac{2}{\pi\,\left(z^2+1\right)}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o25) }
b_{z}=\frac{4\,z}{\pi\,\left(z^2+1\right)^2}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i26) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Fz:1/2*rhs(az)+%i/2*rhs(bz),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o26) }
-\frac{z^2-2\,i\,z+1}{\pi\,\left(z^2+1\right)^2}
\end{math}
\newline

De nuevo,  la transformada inversa para verificar:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i27) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
integrate(exp(-%i*z*x)*Fz,z,minf,inf);
\end{verbatim}}
\end{minipage}
{\color{red}Is x positive, negative or zero?} p;

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o27) }
(x-1)\,e^ {- x }
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i28) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
integrate(exp(-%i*z*x)*Fz,z,minf,inf);
\end{verbatim}}
\end{minipage}
{\color{red}Is x positive, negative or zero?} n;

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o28) }
(x-1)\,e^ {x }
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i29) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}

\item Resolvamos ahora el ejemplo \ref{ejemplo5s35} donde la función era:
\[
f(t)=\mbox{sen}(\omega_0 t) \,.
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(fourie)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f:sin(w0*t);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\sin \left({\tt w0}\,t\right)
\end{math}
\newline

Como la función es impar, podemos intentar: 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
fourintsin(f,t);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t3) }
b_{z}=\frac{2\,\left(\lim_{t\rightarrow \infty }{-\frac{\sin \left(t\,
 z+{\tt w0}\,t\right)}{2\,z+2\,{\tt w0}}-\frac{\sin \left(t
 \,z-{\tt w0}\,t\right)}{2\,{\tt w0}-2\,z}}\right)}{\pi}
\end{math}

\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ {\tt \%t3} \right] 
\end{math}
\newline

¡Intento fallido! Podemos probar resolviendo la integral.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
tau:n*%pi/w0;
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Fw:sqrt(2/%pi)*ratsimp(integrate(sin(t*omega)*f,t,0,tau));
\end{verbatim}}
\end{minipage}
{\color{red}Is nw0 positive, negative or zero?} p;

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
-\frac{\sqrt{2}\,\left(\left({\tt w0}-\omega\right)\,\sin 
 \left(\frac{\pi\,n\,{\tt w0}+\pi\,n\,\omega}{{\it w0}}
 \right)+\left(-{\tt w0}-\omega\right)\,\sin \left(\frac{\pi\,n
 \,{\tt w0}-\pi\,n\,\omega}{{\tt w0}}\right)\right)}{\sqrt{
 \pi}\,\left(2\,{\tt w0}^2-2\,\omega^2\right)}
\end{math}

Asignemos los valores numéricos a los parámetros:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
n:6$ w0:%pi$
\end{verbatim}}
\end{minipage}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Fw:ev(Fw);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
-\frac{\sqrt{2}\,\left(\left(\pi-\omega\right)\,\sin \left(\frac{6
 \,\pi\,\omega+6\,\pi^2}{\pi}\right)+\left(-\omega-\pi\right)\,\sin 
 \left(\frac{6\,\pi^2-6\,\pi\,\omega}{\pi}\right)\right)}{\sqrt{\pi}
 \,\left(2\,\pi^2-2\,\omega^2\right)}
\end{math}
\newline

Grafiquemos ahora la transformada $F(w)$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
wxplot2d([Fw], [omega,0,3*%pi])$
\end{verbatim}}
\end{minipage}


%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\end{math}

\begin{figure}[h]\nonumber
\begin{center}
\includegraphics[height=2.6in,width=4.5in]{VOLUMEN_1/03_Funciones_Lineales/Figuras/Figura3_7.jpg}
\end{center}
\end{figure}


\end{enumerate}


\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}



\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Si: 
\[
\delta_n(x)=\left\{
\begin{array}{cl}
0 & x< -\frac{1}{2n} \\
n & -\frac{1}{2n}< x < \frac{1}{2n}\\
0 & x<\frac{1}{2n}
\end{array}
\right.
\]
demuestre que:
\[
f(0)= \lim_{n \rightarrow \infty}\int_{-\infty}^{\infty} f(x)\delta_{n}(x) \mathrm{d}x \,,
\]
para $f(x)$ continua en $x=0$.

\item Para: 
\[
\delta_n(x)=\frac{n}{\pi}\frac{1}{1+n^2x^2}\,.
\]
Demuestre lo siguiente:
\[
\int_{-\infty}^{\infty} \delta_{n}(x) \mathrm{d}x=1 \,.
\]

\item Si:
\[
\delta_n(x)=\frac{n}{\sqrt{\pi}}\mathrm{e}^{-n^2x^2}\,.
\]
Demuestre que:
\[
x\frac{\mathrm{d}\delta(x)}{\mathrm{d} x}= -\delta(x) \,.
\]


\item Demuestre la siguiente relación:
\[
\int_{-\infty}^{\infty} \delta'(x)f(x) \mathrm{d}x= -f(0) \,.
\]  

\item Dadas las siguientes funciones ortonormales:
\begin{eqnarray*}
\mbox{a)}\,\,\, \left|  \mathrm{e}(k,x)\right> &=& 
\left\{\frac{1}{\sqrt{\pi}}\mbox{sen}(kx)\,,\frac{1}{\sqrt{\pi}}\cos(kx)\right\} \,, \,\,\, \mbox{con: }\,\,\, 
0\leq k < \infty \,,  -\infty\leq x < \infty \,. \\
\mbox{b)}\,\,\, \left|  \mathrm{e}(k,x)\right> &=&
\left\{\frac{1}{\sqrt{2\pi}} \mathrm{e}^{ikx} \right\} \,, \,\,\, \mbox{con: }\,\,\, -\infty \leq k < \infty \,,  -\infty \leq x < \infty \,.
\end{eqnarray*}

\begin{enumerate}
\item Escriba las condiciones de ortogonalidad y cierre.
\item Demuestre:
\begin{enumerate}
\item $\int_{-\infty}^{\infty} \mbox{sen}(kx)\mbox{sen}(k'x) \mathrm{d}x=
\pi\delta(k-k')$.
\item $\int_{-\infty}^{\infty} \cos(kx)\cos(k'x) \mathrm{d}x=\pi\delta(k-k')$.
\item $\int_{-\infty}^{\infty} \mbox{sen}(kx)\cos(k'x) \mathrm{d}x=0$.
\end{enumerate}

\end{enumerate}

\item Demuestre que si:
\[
\delta_n(x)=\frac{n}{\pi(1+n^2x^2)} \,,
\]
entonces:
\[
\int_{-\infty}^{\infty} \delta_n \mathrm{d}x=1\,. 
\]

\item Dada la siguiente sucesión de funciones:
\[
\delta_n(x)=\frac{1}{2n \pi }\left(\frac{\mbox{sen}(nx/2)}{\mbox{sen}(x/2)}\right)^2 \,.
\]
Demuestre que $\delta_n(x)$ es una distribución delta de Dirac  al calcular
\[
\lim_{n \rightarrow \infty} \left[\frac{1}{2n \pi}\int_{-\infty}^{\infty}
f(x)\left(\frac{\mbox{sen}(nx/2)}{\mbox{sen}(x/2)}\right)^2 \mathrm{d}x\right] =f(0)\,.
\]

\item Encuentre las transformadas de Fourier de las siguientes funciones:

\[
a) \quad 
f(x)=\left\{
\begin{array}{cc}
\mathrm{e}^{-x}\,,& x > 0 \\
 & \\
0\,, & x<0
\end{array}
\right. \qquad \qquad 
b) \quad 
f(x)=\left\{
\begin{array}{cc}
h(1-a |x|)\,,& |x| < \frac1a \\
 & \\
0\,, & |x| > \frac1a
\end{array}
\right. 
\]

\item Dad  $F({\bf k})$ como la transformada de Fourier, en tres dimensiones, de $f({\bf r})$ y
$F_d({\bf k})$ la transformada de Fourier, tridimensional, de ${\boldsymbol \nabla} f({\bf r}) $. Demuestre que:
\[
F_d({\bf k})= -i{\bf k}({\bf k}) \,.
\] 

\item Resuelva los ejercicios anteriores con {\bf Maxima}. 


\end{enumerate}
