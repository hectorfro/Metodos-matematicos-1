\chapter{Espacios vectoriales lineales}
\label{CapEspaciosVectoriales}
\section*{La ruta de este capítulo}
Este es el capítulo central de esta obra, en el cual discutiremos los conceptos fundamentales sobre espacios vectoriales abstractos, sus propiedades y sus múltiples expresiones: vectores de $\mathds{R}^{n}$, matrices, polinomios, funciones continuas (entre otras). Además, introduciremos la notación  de \textit{bra} y \textit{ket} que facilita mucho la organización de los conceptos y, que nos acompañará por el resto de este libro. Haremos una constante referencia a los conceptos que fueron discutidos en el capítulo anterior en el marco de los vectores en $\mathds{R}^{3}$. Como en el capítulo 1, cada una de las secciones presenta ejemplos con la utilización de la herramienta de cómputo algebraico {\bf Maxima}, la cual consideramos como parte fundamental de estas notas y nos permite mostrar el alcance de los conceptos abstractos.  

Para empezar, en la próxima sección, iniciamos  con una discusión somera sobre grupos y sus propiedades.  Seguimos con la sección \ref{EspaciosVectoriales}, en la cual presentamos el concepto de espacio vectorial lineal para describir varios de sus escenarios que usualmente se presentan desconectados. Hacemos un esfuerzo por ilustrar, bajo un mismo enfoque, su aplicación desde $\mathds{R}^{n}$ hasta los espacios vectoriales de funciones continuas $\mathcal{C}_{[a,b]}$.  Luego, en la sección \ref{ProductoInterno},  abordamos los conceptos de distancia (métrica), norma y, finalmente el de producto interno.  Esta última definición nos permite equipar los espacios vectoriales, no solo con norma y distancia, sino también dotarlos de geometría, vale decir, definir ángulos entre vectores abstractos. Nos detenemos un momento para discutir el significado de ángulo entre vectores pertenecientes a espacios vectoriales reales y complejos.  En la sección \ref{VariedadesLineales} mostramos el concepto de variedades lineales, discutimos el criterio de independencia lineal y, a partir de éste definimos las bases para los espacios vectoriales. Apoyándonos en el criterio de ortogonalidad construimos bases ortogonales para varios espacios vectoriales, discutimos los subespacios vectoriales ortogonales y las proyecciones ortogonales. El capítulo lo finalizamos mostrando la utilidad de expresar funciones como combinación lineal de vectores ortogonales y, cómo este tipo de expansiones constituye la mejor aproximación a la función.  


\section{Grupos, campos y espacios vectoriales}
\label{losgrupos}
En los cursos básicos de matemáticas nos enseñaron el concepto de conjunto: una colección de elementos de una misma naturaleza, y aprendimos una gran variedad de operaciones aplicadas a los elementos que conforman los conjuntos y a los conjuntos mismos. Es probable que en esos tiempos nos quedara la sensación de la poca utilidad de todos esos conceptos de la teoría de conjuntos, pero como veremos en esta sección, la noción de conjuntos es  fundamental para desarrollar todas las ideas que conforman lo que se conoce como el {\it algebra abstracta}. En el estudio de las estructuras algebraicas se incluyen las teorías sobre: grupos, anillos, campos, espacios vectoriales, redes, algebras; que merecen ahora nuestra atención. 
Comenzaremos con la estructura de grupo, y nos daremos cuenta de que una buena cantidad de objetos matemáticos, en apariencia muy diferentes, que hemos utilizando en todos los cursos anteriores tienen incorporadas la estructura de grupo. 
La noción de grupo no llevará entonces al importante concepto de espacios vectoriales abstractos que discutiremos en la sección  \ref{EspaciosVectoriales} y a la definición de los espacios métricos, sección \ref{EspaciosMetricos}, fundamentales en el desarrollo de las teorías físicas.  

\subsection{Grupos}
\label{Grupos}
\index{Grupos}
\index{Abelianos!Grupos }
Considere el siguiente conjunto no vacio $\textbf{\em G}=\left\{  g_{1}, g_{2}, g_{3},\cdots,g_{n},\cdots\right\} $ y la operación interna $\square$ (la ley del grupo). Entonces los  elementos del conjunto forman un grupo respecto a la operación $\square$ si $\forall \, g_{i}\,  \in \, \textbf{\em G}$ se cumplen las siguientes condiciones:

\begin{enumerate}
\item  Cerrada respecto a la operación $\square$: $\left\{  g_{i} \in\textbf{\em G},g_{j}\in\textbf{\em G}\right\} \,\, \Rightarrow \,\, \exists \ g_{k}=g_{i}\ \square\ g_{j}\in\textbf{\em G}$.

\item  Asociativa respecto a la operación $\square$: $g_{k}\ \square
\ \left(  g_{i}\ \square\ g_{j}\right)  =\left(  g_{k}\ \square\ g_{i}\right)\ \square\ g_{j}$.

\item  Existencia de un elemento neutro: $\exists \ \ \hat{\texttt{g}}
\in\textbf{\em G}
\,\, \Rightarrow \,\, g_{i}\ \square\ \hat{\texttt{g}}=g_{i} =\hat{\texttt{g}}\ \square\ g_{i} $.

\item  Existencia de un elemento inverso: $g_{i} \in\textbf{\em G}\,\, \Rightarrow \,\,
 \exists \ g_{i}^{-1}\ \in\textbf{\em G} \,\, \Rightarrow \,\, g_{i}\ \square\ g_{i}^{-1}=g_{i}^{-1}\ \square\ g_{i}=\hat{\texttt{g}}$.
\end{enumerate} 

Si adicionalmente se cumple que:
\begin{enumerate}
\item[5] Conmutativa respecto a la operación $\square$: $g_{i}\ \square\ g_{j}\equiv g_{j}$ $\square\  g_{i}$;  el grupo se denomina {\it grupo abeliano}\footnote{NIELS
HENRIK ABEL, (1802-1829 Noruega) Pionero en el desarrollo de diferentes ramas de la matemática moderna, Abel mostró desde su infancia un notable talento para el estudio de las ciencias exactas. Tal predisposición se vería muy pronto confirmada por sus precoces investigaciones sobre cuestiones de álgebra y cálculo integral, en particular sobre la teoría de las integrales de funciones algebraicas (a las que se denominaría ``abelianas'' en honor de quien la formuló) que no habría de publicarse hasta 1841, doce años después de su fallecimiento. En 2002 el gobierno noruego lanzó el premio Abel que llenará el vacío que existe en la premiación Nobel del gobierno sueco, en el cual no existe premiación para la comunidad matemática.}. 
\end{enumerate}

Suele denotarse al grupo $\textbf{\em G}$ como $\left(\textbf{\em G}, \square  \right)$ para indicar el tipo de operación. 

\subsubsection{Algunos grupos:}
\index{Ejemplos de grupos}

\begin{itemize}
\item  Los enteros $\mathds{Z}=$ $\left\{  \cdots-3-2,-1,0,1,2,3,\cdots
\right\}  $ respecto a la suma.

\item  Los racionales respecto a la suma y a la multiplicación.

\item  Los números complejos $z=e^{i\theta}$ respecto a la multiplicación.

\item  Las rotaciones en 2 dimensiones (2D), y las rotaciones en 3D (grupo no-abeliano).

\item Las matrices de dimensión $n\times m$ respecto a la suma (grupo abeliano).
\end{itemize}

Dado un grupo de tres elementos: 
$\textbf{\em G}=\left\{  \mathbf{1}, a, b\right\}$, con $\hat{\texttt{g}}\equiv \mathbf{1}$ y la operación $\square$. Por construcción, si queremos que la operación de dos de los elementos provea un tercero distinto, entonces la ÚNICA ``tabla de multiplicación'' posible será:
\begin{center}
\begin{tabular}
[c]{|l|l|l|l|}\hline
$\square$ & $\mathbf{1}$ & $a$ & $b$\\\hline
$\mathbf{1}$ & $\mathbf{1}$ & $a$ & $b$\\\hline
$a$ & $a$ & $b$ & $\mathbf{1}$\\\hline
$b$ & $b$ & $\mathbf{1}$ & $a$\\\hline
\end{tabular}
\end{center}

Notemos que el grupo más simple será aquel conformado únicamente por el elemento neutro:
$\textbf{\em G}=\left\{  \mathbf{1}\right\}$ y, como veremos más adelante,  se puede definir subgrupos si un subconjunto de los  elementos de un grupo $g_{i} \in \textbf{\em G}$ también forman un grupo. 

Podemos resumir, sin demostración, las principales propiedades que presenta un grupo $\left(\textbf{\em G}, \square  \right)$. 

\begin{enumerate}
\item El elemento identidad o neutro es único.
\item Para todo $g\in \textbf{\em G}$ existe un inverso único $g^{-1}\in \textbf{\em G}$.
\item Para todo elemento $g\in \textbf{\em G}$ se cumple que:
$\left(g^{-1}\right)^{-1}=g$.
\item Para cualesquiera $g_1$ y $g_2$ $\in \textbf{\em G}$, se cumple que:
$\left(g_1\square g_2 \right)^{-1}=g^{-1}_1\square g^{-1}_2$.
\end{enumerate}

El número de los elementos de un grupo puede ser finito o infinito. En el primer caso se denominan \textit{grupos finitos} y el número de elementos que contenga, el cardinal del conjunto, se conoce como el orden del grupo. Un grupo finito que se construye a partir de una operación con un único miembro se denomina \textit{grupo cíclico}, y el caso más elemental es   $\textbf{\em G}=\left\{  \mathbf{1}, g, g^{2}, g^{3}, \cdots,g^{n-1}\right\}$. Obviamente hemos definido aquí: $g^{2} = g \square g$ y $g^{3} = g^{2} \square g= g \square g \square g $ y así consecutivamente hasta ejecutarse $n-1$ veces, entonces se retoma el elemento identidad,  esto es: $g^{n-1} \square \ g = g^{n}=  \mathbf{1}$.

\subsubsection{Subgrupos}
\label{subgrupos}
\index{Subgrupos}

Dado un grupo $\left(\textbf{\em G}, \square  \right)$ y $\textbf{\em H}$ un subconjunto de $\textbf{\em G}$, $\textbf{\em H} \subseteq \textbf{\em G}$. Si $\textbf{\em H}$ es un grupo bajo la operación $\square$, definida para $\textbf{\em G}$, entonces diremos que $\textbf{\em H}$ es un subgrupo. De manera equivalente: si $\textbf{\em H} \subseteq \textbf{\em G}$ entonces $\textbf{\em H}$  es un subgrupo si para cualesquiera  $h_1$ y $h_2$ $\in \textbf{\em H}$ se cumple:
\begin{itemize}
\item $h_1\square h_2 \in \textbf{\em H}$.
\item $h^{-1} \in \textbf{\em H} \,\, \forall\,\, h \in \textbf{\em H}$.
\end{itemize}
Es decir, $\mathbf{1}  \in \textbf{\em H}$ y $\textbf{\em H}$ es cerrado para la misma operación que define $\textbf{\em G}$ y para sus inversos. 

Existe una condición necesaria para que un subconjunto de un grupo finito $\textbf{\em G}$  sea un subgrupo, esta condición se conoce como el teorema de Lagrange, que dice lo siguiente: 
si $\left(\textbf{\em G}, \square  \right)$ es un grupo finito y $\textbf{\em H}$ un subgrupo de $\textbf{\em G}$, entonces el orden de $\textbf{\em H}$ divide al orden de $\textbf{\em G}$. 

Por otro lado, sea $\left(\textbf{\em G}, \square  \right)$ un grupo y $g \in \textbf{\em G}$, se puede demostrar que el conjunto $\textbf{\em H}=\{ g^n \}$, con $n$ perteneciente al conjunto de los números enteros,  
es un subgrupo de $\textbf{\em G}$. Con la propiedad adicional de que 
$\textbf{\em H}$ es el subgrupo,  que contiene a $g$, más pequeño de $\textbf{\em G}$. 

Como mencionamos con anterioridad, un grupo finito que se construye a partir de  operar con un único miembro se le denomina \textit{grupo cíclico}, en este caso de dice que $\textbf{\em H}$  es un subgrupo cíclico generado por $g$. A éste elemento se le llama el generador de $\textbf{\em H}$ y se acostumbra a denotarlo con $\textbf{\em H}=< g >$.

Para finalizar, definiremos el { \it orden de un elemento} de un grupo. Sea un elemento $g \in \textbf{\em G}$, el orden de $g$ es el menor entro positivo $n$ tal que: $g^n=\mathbf{1}$. Cuando éste entero positivo no existe se dice que $g$ tiene un orden infinito. 


\subsubsection{Grupos Isomorfos}
\index{Grupos isomorfos}
\index{Isomorfos!Grupos}

Muchas veces podemos comparar la estructura entre grupos buscando relaciones entre ellos, esto lo podemos hacer estudiando, por ejemplo, cómo ``se multiplican''.  La búsqueda de estas relaciones no llevan al concepto de {\it isomorfismo} de grupos. Si podemos decir que dos grupos son isomorfos es porque tienen aspectos en común dentro de sus estructuras algebraicas.

Consideremos los siguientes conjuntos y la operación\footnote{Vamos a considerar lo que se conoce como {\it Arimética Modular}. Sabemos que para la división de dos enteros $a$ y $b$ ($b\neq 0$) existe un único entero $c$ tal que $a=cb+r$ con $0\leq r \leq |b|$. Y como es conocido:  
$a$, $b$, $c$ y $r$ son: el dividendo, el divisor, el cociente y el resto, respectivamente. Cuando $r=0$ se dice que $a$ es divisible por $b$. En esta ``arimética" tan particular se establece una relación de congruencia sobre los enteros: para un entero positivo $n$, dos enteros $a$ y $b$ se llaman congruentes módulo $n$ (mod n)  si $a$ y $b$ tienen el mismos resto $r$ cuando se dividen entre $n$, y se denota $a\equiv b$ mod $n$.} multiplicación módulo X, es decir $(a \cdot b)_{modX}$: 

\begin{itemize}
  \item $\textbf{\em G}_{\mathrm{mod}8}=\left\{ 1, 3, 5, 7\right\}$ y la operación multiplicación módulo 8. De esta manera construimos la tabla de multiplicación del grupo:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
  $\times \mathrm{mod}8$   & 1 & 3  & 5 & 7 \\ \hline
 1  & 1 &  3 & 5 & 7 \\ \hline
 3  & 3 & 1  & 7 & 5 \\ \hline
 5  & 5 &  7 & 1 & 3 \\ \hline
 7  & 7 &  5 &  3 & 1 \\ \hline      
\hline
\end{tabular}
\end{center}
Recordemos que la regla usada, por ejemplo para 3 y 7, es  la siguiente: $3 \cdot 7 = 21$ y el residuo de dividir $21$ entre $8$ es $5$, por lo tanto $(3 \cdot 7)_{mod8} = (21)_{mod8} = 5$.  $(21\div 8= 2 \times 8 +5)$.

\item $\textbf{\em G}_{\mathrm{mod}5}=\left\{ 1, 2, 3, 4 \right\}$ y la operación multiplicación módulo 5. Tabla:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
 $\times \mathrm{mod}5$    & 1 & 2  & 3 & 4 \\ \hline
 1  & 1 &  2 & 3 & 4 \\ \hline
 2  & 2 & 4  & 1 & 3 \\ \hline
 3  & 3 &  1 & 4 & 2 \\ \hline
 4  & 4 &  3 &  2 & 1 \\ \hline      
\hline
\end{tabular}
$\quad \Leftrightarrow \quad$
\begin{tabular}{|c|c|c|c|c|}
\hline
 $\times \mathrm{mod}5$    & 1 & 2  & 4 & 3 \\ \hline
 1  & 1 & 2 & 4 & 3 \\ \hline
 2  & 2 & 4  & 3 & 1 \\ \hline
 4  & 4 & 3 & 1 & 2 \\ \hline
 3  & 3 & 1 &  2 & 4 \\ \hline      
\hline
\end{tabular}
\end{center}
\item $\textbf{\em G}_{\mathrm{mod}24}=\left\{ 1, 5, 7, 11 \right\}$ y la operación multiplicación módulo 24. Tabla de multiplicación: 
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
 $\times \mathrm{mod}24$    & 1 & 5  & 7 & 11 \\ \hline
 1  & 1 & 5 & 7 & 11 \\ \hline
 5  & 5 & 1  & 11 & 7 \\ \hline
 7  & 7 & 11 & 1 & 5 \\ \hline
 11  & 11 & 7 &  5 & 1 \\ \hline      
\hline
\end{tabular}
\end{center}
\item $\textbf{\em G}_{\times}=\left\{ 1, i, -1, -i \right\}$ y la operación multiplicación: 
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
 $\times$   & 1 & i   & -1 & -i \\ \hline
 1 		 & 1 & i   & -1 & -i \\ \hline
 i   		&  i  & -1  & -i & 1 \\ \hline
 -1  		& -1 & -i  & 1 & i \\ \hline
 -i  		& -i  & 1 &  i &  -1 \\ \hline      
\hline
\end{tabular}
\end{center}
\end{itemize}

Diremos que los grupos $\textbf{\em G}_{\mathrm{mod}8}$ y $\textbf{\em G}_{\mathrm{mod}24}$ son {\it isomorfos} porque tienen tablas equivalentes de multiplicación. 
Esto es, dado un grupo genérico $\textbf{\em G}=\left\{ 1, A, B, C \right\}$ su tabla de multiplicación será:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
$\times$     & 1 & A  & B & C \\ \hline
 1  & 1 & A & B & C \\ \hline
 A  & A & 1  & C & B \\ \hline
 B  & B & C & 1 & A \\ \hline
 C  & C & B &  A & 1 \\ \hline      
\hline
\end{tabular}
\end{center}
Note que $A^{-1}=A$, y que siempre la operación de dos elementos da uno distinto a los operados. 

De igual forma los grupos $\textbf{\em G}_{\times}$ y $\textbf{\em G}_{\mathrm{mod}5}$, estos son isomorfos con una tabla de multiplicación:
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
  $\times$   & 1 & A  & B & C \\ \hline
 1  & 1 & A & B & C \\ \hline
 A  & A & B  & C & 1 \\ \hline
 B  & B & C & 1 & A \\ \hline
 C  & C & 1 &  A & B \\ \hline      
\hline
\end{tabular}
\end{center}

\subsection{Campo}
\label{Campo}
\index{Campo}
Definiremos como un campo (o cuerpo)  el conjunto $\textbf{\em K}=\left\{  \alpha_{1}, \alpha_{2}, \alpha_{3},\cdots, \alpha_{n},\cdots\right\} $ sobre el cual están definidas dos operaciones: suma ($+$) y multiplicación  ($\cdot$) y que satisfacen las siguientes propiedades:

\begin{enumerate}
\item  Forman un grupo abeliano respecto a la suma ($+$) con el elemento neutro representado por el cero $0$.

\item  Forman un grupo abeliano respecto a la multiplicación ($\cdot$). Se excluye el cero $0$ y se denota el elemento neutro de la multiplicación como $1$. 

\item  Es distributiva respecto a la suma ($+$).  Dados $\alpha_{i}, \alpha_{j}$ y $\alpha_{k}$, 
se tiene que: $\alpha_{i}\cdot\left(  \alpha_{j}+\alpha_{k}\right)  =\alpha_{i}\cdot
\alpha_{j}+\alpha_{i}\cdot \alpha_{k}$.
\end{enumerate}

Ejemplos típicos de campos lo constituyen el conjunto de los números racionales $\mathds{Q}$, los números reales $\mathds{R}$ y los números complejos $\mathds{C}$.  Normalmente se refiere estos campos como \textit{campos escalares}. 

La noción de campo nos permite introducir el importante concepto de espacio vectorial. 

\subsection{Espacios vectoriales lineales}
\label{EspaciosVectoriales}
\index{Espacios vectoriales lineales}
\index{Vectores!Espacios vectoriales lineales}

Sea el conjunto de objetos $\textbf{\em V}=\left\{\left|v_{1}\right>, \left|v_{2}\right>, \left|{v}_{3}\right>, \cdots , \left| {v}_{i}\right>, \cdots \right\}$.  Se denominará $\textbf{\em V}$ un espacio vectorial lineal y sus elementos $\left| {v}_{i}\right> $ vectores, si existe una operación suma, $\boxplus,$ respecto a la cual los elementos $\left|  v_{i}\right> \in\textbf{\em V}$ forman un grupo abeliano y una operación multiplicación por un elemento de un campo, $\textbf{\em K}=\left\{  \alpha,\
\beta,\ \gamma\cdots\right\}$, tal que\footnote{También suele decirse: un espacio vectorial $\textbf{\em V}$ sobre $\textbf{\em K}$.}: 

\begin{enumerate}
\item  $\textbf{\em V}$ es cerrado bajo la operación $\boxplus$:
$\forall\ \left|  v_{i}\right> ,\left|  v_{j}\right> \in\textbf{\em V}\,\, \Rightarrow \,\,\left|  v_{k}\right>=\left|  v_{i}\right> \boxplus\left|  v_{j}\right> \in\textbf{\em V}$\,.

\item  La operación  $\boxplus$ es conmutativa:
$\forall\ \left|  v_{i}\right> ,\left|  v_{j}\right> \in\textbf{\em V}\,\, \Rightarrow \,\,
\left|  v_{i}\right> \boxplus \left|  v_{j}\right> =\left|  v_{j}\right> \boxplus\left|  v_{i}\right> $\,.

\item  La operación  $\boxplus$ es  asociativa:
$\forall\ \left|  v_{i}\right> ,\left|  v_{j}\right> ,\left|  v_{k}\right> \in\textbf{\em V}
\,\, \Rightarrow \,\,
\left(  \left|  v_{i}\right> \boxplus\left|  v_{j}\right> \right)  \boxplus\left|  v_{k}\right>
=\left|  v_{i}\right> \boxplus\left(  \left|  v_{j}\right> \boxplus\left|  v_{k}\right> \right)  $\,.

\item  Existe un único elemento neutro $\left|{0}\right>: \,\, \left|{0}\right>\boxplus\left|  v_{i}\right> =
\left|  v_{i}\right> \boxplus\left|  {0}\right> =\left|  v_{i}\right>\,\, \forall\ \left|  v_{i}\right > \in \textbf{\em V}$\,.

\item  Existe un elemento simétrico para cada elemento de $\textbf{\em V}$: 
$\forall\ \left|  v_{i} \right> \in\textbf{\em V}\,\,\exists \, \left|  -{v}_{i} \right> \,\, /  \,\, 
\left|{v}_{i}\right> \boxplus\left|-{v}_{i}\right > = \left|{0}\right> $\,.

\item $\textbf{\em V}$ es cerrado bajo el producto por un número:
$\forall \ \alpha \in  \textbf{\em K}$ y cualquier $\left|  v_{i}\right> 
\in \textbf{\em V} \,\, \Rightarrow \,\,  \alpha \left|  v_{i}\right> \in \textbf{\em V}$\,.

\item $\alpha\left(  \beta\left|  v_{i}\right> \right) = \left(  \alpha\beta\right)  \left|  v_{i}\right> $\,.

\item $\left(  \alpha+\beta\right)  \left|  v_{i}\right> = \alpha\left|  v_{i}\right> \boxplus \beta\left|  v_{i}\right> $\,.

\item $\alpha\left(  \left|  v_{i}\right> \boxplus\left| {v}_{j}\right> \right)  =\alpha\left|  v_{i}\right> \boxplus\alpha\left|  v_{j}\right> $\,.

\item $\mathbf{1}\left|  v_{i}\right> =\left|  v_{i}\right> \,\, \forall  \,\, \left|  v_{i}\right> \in \textbf{\em V} $ y $\mathbf{1} \in \textbf{\em K} $\,.
\end{enumerate}

Es importante resaltar lo siguiente: existen dos objetos neutros, 
el vector $\left| 0 \right> \in \textbf{\em V}$  y el elemento  $0 \in  \textbf{\em K}$ y también dos operaciones producto diferentes, el producto de dos números dentro de $\textbf{\em K}$ y el producto de un  $\alpha \in  \textbf{\em K}$ por un vector $\left| v \right> \in \textbf{\em V}$.

Notemos también que podemos definir subespacios $\textbf{\em S}$ vectoriales dentro de los espacios vectoriales. Ellos serán aquellos conjuntos de vectores que cumplan con los requisitos anteriores pero además cerrados dentro de los mismos conjuntos de vectores. Se puede ver entonces que la condición necesaria y suficiente para que $\textbf{\em S} \subseteq \textbf{\em V}$  sea un subespacio vectorial de $\textbf{\em V}$ es que 
para cualesquier $\left| u_i \right>$ y $\left| v_i \right>$ de
$\textbf{\em S}$ y para cualesquier $\alpha$ y $\beta$ de  $\textbf{\em K}$
se tiene que: $\alpha\left| u_i \right>+\beta\left| v_i \right> 
\, \in \, \textbf{\em S}$.



\subsection{Algunos espacios vectoriales}
\index{Espacios vectoriales lineales!Ejemplos de }
\index{Ejemplos de espacios vectoriales lineales}

\begin{enumerate}
\item  Los conjunto de los números reales $\textbf{\em V} = \mathds{R}$ y el conjunto de los números complejos $ \textbf{\em V}=\mathds{C}$ con el campo $\textbf{\em K} $ de reales o complejos y definidas las operaciones ordinarias de suma y multiplicación.

Cuando el campo $\textbf{\em K}$ es el conjunto de los números reales se dirá que es \textit{un espacio vectorial real de números reales} si  $\textbf{\em V} \equiv \mathds{R}$, pero si $\textbf{\em V} \equiv \mathds{C}$ se dirá \textit{un espacio vectorial real de números complejos}. 
Por su parte, si $\textbf{\em K} \equiv \mathds{C}$ diremos que es un espacio vectorial complejo. Siempre se asociará  el campo de escalares al espacio vectorial: se dirá que es un espacio vectorial sobre el campo de los escalares. Es decir, si el campo es real (complejo) se dirá que el espacio vectorial es real
(complejo).

\item  El conjunto $\textbf{\em V} \equiv \mathds{R}^{n} = \mathds{R}\times\mathds{R}\times \cdots \times \mathds{R}$, vale decir el producto cartesiano de $\mathds{R}$, cuyos elementos son $n-$uplas de números, con la operación suma ordinaria de vectores en $n-$dimensionales y la
multiplicación por números.
\begin{align*}
\left| x \right>  &  =\left(  x_{1},x_{2},x_{3}, \cdots x_{n}\right)  \quad \wedge \quad \left| y \right> =\left(y_{1},y_{2},y_{3}, \cdots, y_{n}\right) \,, \\
\left| x \right> \boxplus\left| y \right>  & \equiv \left(  x_{1}+y_{1},x_{2}+y_{2},x_{3}+y_{3},\cdots x_{n}+y_{n}\right)\,, \\
\alpha\left| x \right>  &  =\left(  \alpha x_{1},\alpha x_{2},\alpha x_{3}, \cdots \alpha x_{n}\right)\,.
\end{align*}
Este espacio vectorial es de dimensión finita. 

Igualmente, será un espacio vectorial $\textbf{\em V} \equiv \mathds{C}^{n}=\mathds{C}\times\mathds{C}\times \cdots \times \mathds{C}$, en donde los elementos $x_{i} \in \mathds{C}$. Si para este caso el campo, sobre el cual se define el espacio vectorial $\mathds{C}^{n}$ es real, tendremos un espacio vectorial real de números complejos. 

Es obvio que en el caso $\textbf{\em V}\equiv\mathds{R}$, para el cual $\left|{x}\right>_{1}=\left(  x_{1},0,0,\cdots,0\right)$ y $\left| y \right>_{1}=\left( y_{1},0,0,\cdots,0\right)$ o cualquier espacio de vectores formados por las componentes, i.e. $\left| {x}\right>_{i}=\left(0,0,0,\cdots,x_{i},\cdots0\right)$ y $\left| y \right> _{i}=\left(0,0,0,\cdots,y_{i},\cdots0\right)$ formarán subespacios vectoriales dentro de $\mathds{R}^{n}$.

En el caso específico de $\mathds{R}^{3}$, y en donde hemos  desarrollado todo un conjunto de conceptos matemáticos, es bueno comentar sobre la equivalencia que hemos tomado como obvia entre dos definiciones diferentes:
\begin{itemize}
\item  Los vectores ${\bf a}=(a^1,a^2, a^3)$ y ${\bf b}=(b^1,b^2, b^3)$  con sus respectivas 
operaciones para la suma  ${\bf a}+{\bf b}=(a^1+b^1,a^2+b^2, a^3+b^3)$ y multiplicación por un escalar $\lambda {\bf a}=(\lambda a^1,\lambda a^2, \lambda a^3)$, con  $\{a^i\}$, $\{b^i\}$ y $\lambda \, \in \, \mathds{R}$.
\item Los vectores geométricos, es decir, segmentos orientados en el espacio con un origen común y donde la suma se definió mediante la regla del paralelogramo y el producto por un escalar como el alargamiento o acortamiento de los segmentos o flechas con el posible cambio de dirección.
\end{itemize}

Aunque ambos  son vectores de $\mathds{R}^{3}$ es bueno tener claro que se trata de objetos diferentes que viven en espacios vectoriales diferentes. Se puede decir también que son dos representaciones diferentes para los vectores. Es buena la ocasión para señalar que existe una manera de pasar de una representación a otra, esto se hace por medio de una función que asigne a una tríada $(x^1,x^2,x^3)$ una y sólo una flecha (y viceversa), conservando por supuesto las operaciones de suma y multiplicación por números. A este tipo de funciones se les denomina un {\it isomorfismo}.

Para finalizar, también es posible ver $\mathds{R}^{3}$  simplemente como un conjunto de puntos donde se pueden definir superficies embebidas, como por ejemplo una esfera, $\mathds{S}^{2}$, centrada en el origen y de radio $R$. En un punto $q$ sobre la esfera es posible generar un plano tangente a la esfera y en ese punto construir un espacio vectorial con todos los vectores cuyo origen está en $q$, y por lo tanto, son tangentes a la esfera.  En el lenguaje de la geometría diferencial y las variedades a este conjunto de vectores se le denota con: $T_q\mathds{S}^{2}$. Lo anteriormente dicho se puede generalizar para $\mathds{R}^{n}$.

\item  El espacio $\textbf{\em E}^{ \infty}$ constituido por vectores $\left|{x}\right> =\left(  x_{1},x_{2},x_{3},\cdots x_{n},\cdots\right)$ contables pero con infinitas componentes.
\begin{align*}
\left| x \right>  &  =\left(  x_{1},x_{2},x_{3},\cdots ,x_{n},\cdots\right)  \quad\wedge\quad
\left| y \right> =\left(y_{1},y_{2},y_{3},\cdots,y_{n},\cdots\right) \,, \\
\left| x \right> \boxplus\left| y \right>  &
\equiv\left(  x_{1}+y_{1},x_{2}+y_{2},x_{3}+y_{3},\cdots,x_{n}+y_{n}
,\cdots\right) \\
\alpha\left| x \right>  &  =\left(  \alpha x_{1},\alpha x_{2},\alpha x_{3},\cdots,\alpha x_{n},\cdots\right)\,,
\end{align*}
con la siguiente restricción:
\[
\lim_{n\rightarrow\infty}\sum_{i=1}^{n}x_{i}=L\,,\quad\text{con }L\text{ finito}\,.
\]

\item  El conjunto de las matrices $n\times n$, reales o complejas, con el campo $\textbf{\em K}$ real o complejo.
\begin{align*}
\left| x \right>  &  =M_{ab}\quad \wedge \quad\left| {y}\right> =N_{ab} \,,\\
\left| x \right> \boxplus\left| y \right>  & \equiv M_{ab}+N_{ab}=\left(  M+N\right)  _{ab} \,, \\
\alpha\left| x \right>  &  =\alpha M_{ab}=\left(  \alpha M \right)_{ab} \,.
\end{align*}
Es también obvio que se podrán formar subespacios vectoriales cuyos elementos sean matrices de dimensión menor a $n\times n$.

\item El conjunto de los vectores geométricos, o vectores cartesianos, en 2 y 3 dimensiones, con las propiedades habituales de suma vectorial y multiplicación por un escalar.  

\item  El conjunto de todos los polinomios con coeficientes reales:
$\mathcal{P}=\left\{  a_{0},a_{1}x,a_{2}x^{2}, \cdots, a_{n}x^{n}, \cdots \right\} $, con $\boxplus$ la suma ordinaria entre polinomios y la multiplicación ordinaria de polinomios con números.

\item  Espacios Funcionales (de los cuales los polinomios son un caso particular). En estos espacios los vectores serán funciones, la suma será la suma ordinaria entre funciones y la multiplicación por un escalar también será la multiplicación ordinaria de una función por un elemento de un campo:
\begin{align*}
\left|  {f}\right>  &  =f(x)  \quad \wedge \quad\left|  {g}\right> =g(x) \,, \\
\left|  {f}\right> \boxplus\left|  {g}\right>  & \equiv f(x)  +g(x)  \equiv\left(  f+g\right) (x) \,, \\
\alpha\left|  {f}\right>  &  =\left(  \alpha f\right) (x)  \equiv \alpha f(x)\,.
\end{align*}

\item  El conjunto de todas las funciones continuas e infinitamente diferenciables, definidas en el intervalo $\left[ a,b\right]: \ \mathcal{C}_{\left[ a,b\right]  }^{\infty}$\,.

\item  El conjunto de todas las funciones complejas de variable real, $\psi(x)$, definidas en $\left[a,b\right]$,  de cuadrado integrable (es decir para las cuales $\int_{a}^b$
\textrm{d}$x$ $\left|  \psi\left(x\right)  \right|^{2}$ sea finita).
Este espacio se denomina comúnmente $\mathcal{L}^{2}$ y puede ser definido dentro de un rango $\left[a,b\right]$, finito o infinito, y para más de una variable.

\item El conjunto de todas las soluciones de un sistema de ecuaciones lineales y homogéneas, por ejemplo:
\begin{eqnarray*}
a+3b	+c &=& 0 \\
4a+2b+2c	&= &0
\end{eqnarray*}
Este sistema tiene como solución al conjunto: $\{a, b=a/2, c=-5a/2\}$. La suma de estos elementos y la multiplicación por un número  conforman un espacio vectorial en el sentido de que son soluciones del sistema de ecuaciones.

\end{enumerate}

\subsubsection{Subespacios}

Supongamos que tenemos un conjunto $\textbf{\em S}$ de elementos de un espacio vectorial lineal $\textbf{\em V}$ que satisface las siguientes propiedades:
\begin{enumerate}
\item El vector neutro de $\textbf{\em V}$ está en $\textbf{\em S}$.
\item Si $\left|{s}_1\right> , \left|{s}_2\right> \in \textbf{\em S}$, entonces 
$\left|{s}_1\right>  \boxplus \left|{s}_2\right> \in \textbf{\em S}$.
\item Si $\left|{s}\right>  \in \textbf{\em S}$ y $\alpha$ es un elemento del campo $\textbf{\em K}$, entonces $\alpha \left|{s}\right>  \in \textbf{\em S}$.
\end{enumerate}

De esta manera, las operaciones que hacen de $\textbf{\em V}$ un espacio vectorial también están definidas para $\textbf{\em S}$. Se puede demostrar que $\textbf{\em S}$ es también un espacio vectorial lineal, es decir, satisface el conjunto de axiomas \ref{EspaciosVectoriales}. Al conjunto $\textbf{\em S} \subset\textbf{\em V}$ que satisface las propiedades $1$ y $2$ más los  axiomas \ref{EspaciosVectoriales} se le denomina un subespacio vectorial lineal (o simplemente subespacio) del espacio vectorial lineal $\textbf{\em V}$.

Notemos que el conjunto conformado con el vector neutro como único elemento: $\textbf{\em S}=\{ \left|0\right> \} \in \textbf{\em V}$ es el subespacio vectorial más pequeño de $\textbf{\em V}$. Por otro lado, el espacio completo $\textbf{\em V}$ es el subespacio más grande posible de $\textbf{\em V}$. Se acostumbra llamar a estos dos subespacios  los subespacios triviales de $\textbf{\em V}$. 

Notemos que el espacio vectorial $\mathds{R}^{2}$ no es un subespacio de $\mathds{R}^{3}$ porque $\mathds{R}^{2}$ ni siquiera es un subconjunto de $\mathds{R}^{3}$. De la misma manera, un plano en $\mathds{R}^{3}$ que no contenga el origen no es un subespacio de $\mathds{R}^{3}$, porque el plano no contiene el vector cero de $\mathds{R}^{3}$.

\subsection{La importancia de la notación}
\label{NotacionDirac}
\index{Notación!Dirac}
\index{Dirac!Notación}

En los ejemplos antes mencionados hemos utilizado para representar un vector abstracto la notación de $\left|{v}\right> $ y con éstos construimos un espacio vectorial abstracto 
$\textbf{\em V}=\left\{  \left|{v}_{1}\right>, \left|  v_{2}\right>, \left| v_{3}\right>, \cdots ,\left| v_{n}\right> \right\}$. Un espacio vectorial abstracto será un conjunto de elementos genéricos que satisfacen ciertos axiomas. Dependiendo del conjunto de axiomas tendremos distintos tipos de espacios abstractos, la teoría desarrolla las consecuencias lógicas que resultan de esos axiomas. En matemática el concepto de espacios abstractos es reciente (1928) y, aparentemente, se le debe a Maurice Fréchet\footnote{MAURICE FRÉCHET (1878 Maligny, Yonne, Bourgogne-1973 París, Francia). Versátil matemático francés, con importantes contribuciones en espacios métricos, topología y creador del concepto de espacios abstractos.}.  

Los elementos de esos espacios se dejan sin especificar a propósito. Ese vector abstracto puede representar, vectores en $\mathds{R}^{n}$, matrices $n\times n$ o funciones continuas. La notación $\left|{v}\right> $, que se denomina un \textit{ket} y al cual le corresponde un \textit{bra} $\left< u\right|  $ proviene del vocablo inglés \textit{braket} que significa corchete y será evidente más adelante cuando construyamos escalares \textit{braket} $\left<u\right. \left|{v}\right>$. Esta útil notación la ideó Paul Dirac\footnote{PAUL ADRIEN MAURICE DIRAC (1902 Bristol, Inglaterra 1984-Tallahassee, EE.UU). Además de contribuir de manera determinante en la comprensión de la Mecánica Cuántica, es uno de los creadores de la Mecánica Cuántica Relativista la cual ayudó a comprender el papel que juega el espín en las partículas subatómicas. Por sus importantes trabajos compartió con Erwin Schrödinger el Premio Nobel de Física en 1933.}, uno de los físicos más influyentes en el desarrollo de la Física del siglo XX. En Mecánica Cuántica un estado cuántico particular suele representarse por una función de onda $\psi(x)$, que depende de la variable posición $x$ o de una función alternativa  que puede depender de la variable momentum $p$. En la notación de Dirac, el símbolo $\left|{\psi}\right>$ denota el estado cuántico sin  referirse a la función en particular y también sirve para distinguir a los vectores de los escalares (números complejos) que vienen a ser los elementos fundamentales en el espacio de Hilbert de la Mecánica Cuántica.

\subsection{{\color{Fuchsia}Ejemplos}} 
\label{EjemploGrupo}
\index{Grupo de Permutaciones}
\index{Permutaciones!Grupos de}
\begin{enumerate}
\item 
Consideremos el conjunto de las permutaciones de 3 objetos, cuyos elementos pueden ser representados como se muestra a continuación:
\[
\mathbb{P}_{0} =\left[ \begin{array}{ccc}
   1   &  2 & 3 \\
   1   &  2 & 3
\end{array}
\right]; \;
\mathbb{P}_{1} =\left[ \begin{array}{ccc}
   1   &  2 & 3 \\
   2   &  1 & 3
\end{array}
\right]; \;
\mathbb{P}_{2} =\left[ \begin{array}{ccc}
   1   &  2 & 3 \\
   3   &  2 & 1
\end{array}
\right]; \;
\]
\[
\mathbb{P}_{3} =\left[ \begin{array}{ccc}
   1   &  2 & 3 \\
   1   &  3 & 2
\end{array}
\right];\;
\mathbb{P}_{4} =\left[ \begin{array}{ccc}
   1   &  2 & 3 \\
   2   &  3 & 1
\end{array}
\right];\;
\mathbb{P}_{5} =\left[ \begin{array}{ccc}
   1   &  2 & 3 \\
   3   &  1 & 2
\end{array}
\right];
\]
y la operación de composición de permutaciones:
\[
\mathbb{P}_{1} \bigodot \mathbb{P}_{3} = 
\left[ \begin{array}{ccc}
   1   &  2 & 3 \\
   2   &  1 & 3
\end{array}
\right] \bigodot
\left[ \begin{array}{ccc}
   1   &  2 & 3 \\
   1   &  3 & 2
\end{array}
\right] = 
\left[ \begin{array}{ccc}
   1   &  2 & 3 \\
   2   &  3 & 1
\end{array}
\right] = \mathbb{P}_{4}, 
\] 
es decir, luego de intercambiar las primera y segunda posición, intercambio la segunda y la tercera. 
La tabla de multiplicación del grupo quedará:

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
$\bigodot$         & $\mathbb{P}_{0}$& $\mathbb{P}_{1}$& $\mathbb{P}_{2}$& $\mathbb{P}_{3}$& $\mathbb{P}_{4}$& $\mathbb{P}_{5}$\\ \hline
$\mathbb{P}_{0}$& $\mathbb{P}_{0}$& $\mathbb{P}_{1}$& $\mathbb{P}_{2}$& $\mathbb{P}_{3}$& $\mathbb{P}_{4}$& $\mathbb{P}_{5}$\\ \hline
$\mathbb{P}_{1}$ &$\mathbb{P}_{1}$ & $\mathbb{P}_{0}$& $\mathbb{P}_{5}$& $\mathbb{P}_{4}$& $\mathbb{P}_{3}$ & $\mathbb{P}_{2}$ \\ \hline
$\mathbb{P}_{2}$ &$\mathbb{P}_{2}$ & $\mathbb{P}_{4}$& $\mathbb{P}_{0}$& $\mathbb{P}_{5}$  & $\mathbb{P}_{1}$ & $\mathbb{P}_{3}$ \\ \hline
$\mathbb{P}_{3}$ &$\mathbb{P}_{3}$ & $\mathbb{P}_{5}$&  $\mathbb{P}_{4}$ & $\mathbb{P}_{0}$ & $\mathbb{P}_{2}$ & $\mathbb{P}_{1}$ \\ \hline  
$\mathbb{P}_{4}$ &$\mathbb{P}_{4}$ & $\mathbb{P}_{2}$& $\mathbb{P}_{3}$ & $\mathbb{P}_{1}$  & $\mathbb{P}_{5}$ & $\mathbb{P}_{0}$ \\ \hline
$\mathbb{P}_{5}$ &$\mathbb{P}_{5}$ & $\mathbb{P}_{3}$&  $\mathbb{P}_{1}$ & $\mathbb{P}_{2}$ & $\mathbb{P}_{0}$ & $\mathbb{P}_{4}$ \\ \hline      
\hline
\end{tabular}
\end{center}

\item El conjunto de la rotaciones del espacio ordinario cuando rotamos un ángulo $\phi$ alrededor del eje $z$, ecuación (\ref{rotacionejez}), forman un grupo. 

Denominaremos a este grupo por  $\mathbf{R}_{z \phi}=\{ \mathcal{G}_{\phi_i} \}$, con $0 \leq \phi \leq 2\pi$. Donde con $\mathcal{G}_\phi$ esteremos indicando un giro alrededor del eje $z$ y la operación que nos define las rotaciones la definiremos de la siguiente forma:
\[
\mathcal{G}_{\phi_1} \square \, \mathcal{G}_{\phi_2}  = \mathcal{G}_{{\phi_1}+{\phi_2}} \,.
\]

Entonces podemos ver que:
\begin{enumerate}
\item  $\square$ es una operación cerrada ya que dos rotaciones resulta en otro rotación: 
\[ 
\mathcal{G}_{\phi_1} \ \square \  \mathcal{G}_{\phi_2}  = \mathcal{G}_{{\phi_1}+{\phi_2}} = \mathcal{G}_{\phi_3}\in\mathbf{R}_{z \phi}\,.
\]

\item $\square$ es asociativa: 
\[ 
\left(\mathcal{G}_{\phi_1} \ \square \  \mathcal{G}_{\phi_{2}} \right) \ \square \ \mathcal{G}_{\phi_{3}}=\mathcal{G}_{\phi_1} \ \square \  \left( \mathcal{G}_{\phi_{2}}  \ \square \ \mathcal{G}_{\phi_{3}} \right)\,.
\]

\item  Existe el elemento neutro, $\mathcal{G}_{\phi_0}$, que no hace ninguna rotación:
\[ 
\mathcal{G}_{\phi_1} \ \square \  \mathcal{G}_{\phi_0}  = \mathcal{G}_{\phi_0} \ \square \  \mathcal{G}_{\phi_1}=\mathcal{G}_{\phi_1}\,.
\]

\item  Existe el elemento inverso, $\mathcal{G}_{\phi_{-}}$, ya que podemos rotar en un sentido y en sentido inverso:
\[ 
\mathcal{G}_{\phi_1} \ \square \  \mathcal{G}_{\phi_{-1}}  = \mathcal{G}_{\phi_{-1}} \ \square \  \mathcal{G}_{\phi_1}=\mathcal{G}_{\phi_0}\,.
\]

\item  Es conmutativa ya que las rotaciones  no se afectan por el orden en que son producidas:
\[ 
\mathcal{G}_{\phi_1} \ \square \  \mathcal{G}_{\phi_2}  = \mathcal{G}_{\phi_2} \ \square \  G_{\phi_1}\,.
\]
\end{enumerate} 

\item Consideremos el grupo $\textbf{\em G}=\mathds{Z} \times \mathds{Z}$, es decir, el conjunto de duplas $(x_i,y_i)$ de números enteros. Y la operación: 
\[
(x_1,y_1) \square (x_2,y_2)=(x_1+x_2, y_1+y_2) \,.
\]
Aquí la suma $+$ es la suma convencional de números enteros.

Probemos que tenemos un grupo.
\begin{enumerate}
\item La operación $\square$ es cerrada para la suma:
\[
(x_1,y_1) \square (x_2,y_2)=(x_1+x_2, y_1+y_2) = (x_3,y_3) \in \textbf{\em G}\,.
\]

\item La operación $\square$ es asociativa: 
\begin{eqnarray*}
\left((x_1,y_1) \square (x_2,y_2) \right) \square (x_3,y_3)&=&
(x_1+x_2, y_1+y_2)  \square (x_3,y_3)=
\left((x_1+x_2)+x_3, (y_1+y_2)+y_3\right) \\
&=&
 \left(x_1+(x_2+x_3), y_1+(y_2+y_3)\right) =
 (x_1,y_1) \square \left( (x_2,y_2)\square (x_3,y_3)\right) \,.
\end{eqnarray*}

\item  Existe el elemento neutro, $(0,0)$:
\[ 
(x_1,y_1)\square (0,0)  =(0,0) \ \square (x_1,y_1)=(x_1,y_1)\,.
\]

\item  Existe el elemento inverso, $(-x_1,-y_1)$:
\[ 
(x_1,y_1) \square (-x_1,-y_1) = (-x_1,-y_1) \square (x_1,y_1)=(0,0)\,.
\]

\item  La operación $\square$ es conmutativa: 
\[ 
(x_1,y_1)\square (x_2,y_2)  = (x_2,y_2) \square (x_1,y_1)\,.
\]

\end{enumerate}

\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}} 
\index{Conjuntos con Maxima}
\index{Maxima!Conjuntos}

En este módulo utilizaremos algunas de las herramientas disponibles para incorporar el algebra discreta. Comenzaremos con la introducción a los conjuntos. Podemos operar con conjuntos pero primero debemos definirlos. Existen varias posibilidades, como mostramos a continuación:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A:{1,2,3,4,5,6,7,8,9};
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\left \{1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9  \right \}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
B:set(1,3,5,7,9);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\left \{1 , 3 , 5 , 7 , 9 \right \}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
C:set(2,4,6,8);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left \{2 , 4 , 6 , 8  \right \}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
D:makeset(i/j, [i,j], [[1,3], [2,3], [3,3], [4,3]]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\left \{\frac{1}{3} , \frac{2}{3} , 1 , \frac{4}{3} \right \}
\end{math}
\newline

Notemos que es igual definir los conjuntos con llaves, con la función {\bf set} o {\bf makeset}. Podemos preguntar si determinado elemento perteneces, o no, a un conjunto.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
elementp(7,A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\mbox{true}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
elementp(7,C);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\mbox{false}
\end{math}
\newline 

Operaciones elementales con conjuntos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
UBC:union(B,C);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\left \{1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9  \right \}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
is (A=UBC);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\mbox{true}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Cv:intersection(B,C);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\left \{ \right \}
\end{math}
\newline

Para {\bf Maxima} el conjunto vacío es $\{ \}$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
setdifference(A,C);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\left \{1 , 3 , 5 , 7 , 9 \right \}
\end{math}
\newline

Esto es, el conjunto con los elementos del conjunto $A$ que no pertenecen al conjunto $C$.

Si queremos el conjunto de todos los subconjuntos del conjunto $\{a,b,c \}$ le podemos pedir al programa que nos lo muestre:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
powerset({a,b,c});
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\left \{\left \{ \right \} , \left \{a \right \} , 
\left \{a , b \right \} , \left \{a , b , c \right \} , 
\left \{a , c \right \}, \left \{b \right \} , \left \{b , c \right \} , \left \{c\right \} \right \}
\end{math}
\newline

El producto cartesiano de los conjuntos $A$ y $B$ es el conjunto conformado por los pares $(a,b)$:
\[
A \ \mbox{x}\  B = \{(a, b) / a \in A \,, b \in B\}.
\] 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
cartesian_product(B,C);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\left \{\left[ 1 , 2 \right]  , \left[ 1 , 4 \right]  , \left[ 1 , 
 6 \right]  , \left[ 1 , 8 \right]  , \left[ 3 , 2 \right]  , \left[ 
 3 , 4 \right]  , \left[ 3 , 6 \right]  , \left[ 3 , 8 \right]  , 
 \left[ 5 , 2 \right]  , \left[ 5 , 4 \right]  , \left[ 5 , 6
  \right]  , \left[ 5 , 8 \right]  , \left[ 7 , 2 \right]  , \left[ 7
  , 4 \right]  , \left[ 7 , 6 \right]  , \left[ 7 , 8 \right]  , \right. 
 \end{math}
 \begin{math} 
 \left.
\qquad \qquad \left[ 9 , 2 \right]  , \left[ 9 , 4 \right]  , \left[ 9 , 6
  \right]  , \left[ 9 , 8 \right]  \right \}
\end{math}
\newline

Le podemos pedir al programa la suma de los pares del producto cartesiano anterior:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
makeset(a+b, [a,b], cartesian_product(B,C));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
\left \{3 , 5 , 7 , 9 , 11 , 13 , 15 , 17 \right \}
\end{math}
\newline

{\bf Maxima} trata a los conjuntos y a las listas como objetos de distinta naturaleza, lo que permite trabajar con conjuntos cuyos elementos puedan ser también conjuntos o listas, es decir, subconjuntos.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
lista:makelist(i,i,1,30);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\left[1, 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15
  , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 
 29 , 30 \right] 
\end{math}
\newline

La lista anterior la convertiremos en un conjunto, para este fin debemos utilizar el comando {\bf setify}. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
E:setify(lista);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
\left \{1, 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15
  , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 
 29 , 30 \right \}
\end{math}
\newline

De este último conjunto podemos construir el subconjunto conformado por los números primos: 
 
%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Primos:subset(E,primep);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
\left \{2 , 3 , 5 , 7 , 11 , 13 , 17 , 19 , 23 , 29 \right \}
\end{math}
\newline

Con {\bf cardinality} podemos saber cuantos elementos contiene un conjunto:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
cardinality(%);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
10
\end{math}
\newline

La función de {\bf Maxima} que nos permite calcular las tablas que utilizamos en la sección \ref{losgrupos} es la función {\bf mod}. Veamos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Gm8:{1,3,5,7};
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
\left \{1 , 3 , 5 , 7 \right \}
\end{math}
\newline

Todos los productos posibles entre los elementos de este conjunto son:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
makeset(a.b, [a,b],cartesian_product(Gm8,Gm8));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o19) }
\left \{1 , 3 , 5 , 7 , 9 , 15 , 21 , 25 , 35 , 49 \right \}
\end{math}
\newline

Los modulo $8$ para algunos de los números anteriores son:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i20) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
mod(21,8);mod(35,8);mod(49,8);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o20) }
5
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o21) }
3
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o22) }
1
\end{math}
\newline

Para generar el grupo $\textbf{\em G}_{\mathrm{mod}5}$ escribimos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i23) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
setify(makelist(mod(i,5),i,1,4));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o23) }
\left \{1 , 2 , 3 , 4 \right \}
\end{math}
\newline

En la sección \ref{subgrupos}, definimos el orden de un elemento $g \in \textbf{\em G}$. Consideremos el siguiente conjunto de números enteros: 
\[
\mathds{Z}_{15}=\{0,1,2,3,4,5,6,7,8,9,10,11,12,13,14 \}\,.
\]
Se podría demostrar que el orden de $g \in \textbf{\em G}$ es igual al número de elementos de $< g >$ y lo dejaremos como ejercicio. 

Vamos a calcular el orden de todos los elementos de $\mathds{Z}_{15}$, sabiendo que el orden de cada uno de esos elementos divide a $15$, que es el cardinal de $\mathds{Z}_{15}$.

Probemos primero con el número $6 \in \mathds{Z}_{15}$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i24) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
setify(makelist(mod(6*i,15),i,0,14));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o24) }
\left \{0 , 3 , 6 , 9 , 12 \right \}
\end{math}
\newline

El orden de $6 \in \mathds{Z}_{15}$ es:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i25) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
length(%);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o25) }
5
\end{math}
\newline

En la siguiente instrucción lo haremos para todos los elementos de $Z_{15}$.  

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i26) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
makelist([j,length(setify(makelist(mod(j*i,15),i,0,14)))],j,0,14);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o26) }
\left[ \left[ 0 , 1 \right]  , \left[ 1 , 15 \right]  ,  \left[ 2 , 15 \right]  , \left[ 3 , 5 \right] , \left[ 4 , 15 \right]  ,  \left[ 5 , 3 \right]  , \left[ 6 , 5 \right]  , \left[ 7 , 15  \right]  , \left[ 8 , 15 \right]  , \left[ 9 , 5 \right]  , 
\left[ 10 , 3 \right]  , \left[ 11 , 15 \right]  , \left[ 12 , 5 \right] , \right. \\
\end{math}
\begin{math}
\left.
\qquad \qquad \quad  \left[ 13 , 15 \right]  , \left[ 14 , 15 \right]  \right] 
\end{math}
\newline

La salida no es más que una lista $[x,y]$  con cada elemento $x \in \mathds{Z}_{15}$ y su orden $y$. Por lo tanto, el conjunto de órdenes es: $\{ 1, 3, 5, 15 \}$, todos divisores de $15$, como estipula el teorema de Lagrange.

\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}

\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Diga cuales de los siguientes grupos son abelianos: $\mathds{Z} , +$;  $\mathds{Z}_n, + \; \forall \,, n \in \mathds{N} $; $\mathds{N}, +$; $\mathds{Z}, \cdot$ .

\item  Sea $\textbf{\em S}$ el conjunto de todos los números reales excluyendo $-1$ y defina la operación $\square$ tal que: 
\[
a\ \square\ b=a+b+ab \,.
\]  
Donde $+$ es la suma estándar entre números reales.  Entonces:
\begin{enumerate}
\item  Muestre que $\left[  \textbf{\em S},\square\right]  $ forman grupo.

\item  Encuentre la solución en $\textbf{\em S}$ para la ecuación $2\ \square \ x \ \square\ 3=7$.
\end{enumerate}

\item  Considere un triángulo equilátero que se muestra en la figura \ref{TriangRotRef}. Se pueden identificar operaciones de rotación alrededor de un eje perpendicular a la figura que pasa por su baricentro $\star$ y, reflexiones respecto a planos, $\mathcal{X}_{A}$, $\mathcal{X}_{B}$ y $\mathcal{X}_{C}$-- que dejan invariante la figura del triángulo. Adicionalmente, se puede definir la operación concatenación de rotaciones y reflexiones que dejan igualmente invariante al triángulo, tal y como mostramos en la mencionada figura \ref{TriangRotRef}. Note que lo ilustrado en la figura, puede esquematizarse como:
\[
(A \ \alpha, B \ \beta, C \ \gamma) \qquad \overrightarrow{\mathcal{R}_{\frac{2 \pi}{3}}} \qquad 
(A \ \gamma, B \ \alpha, C \ \beta ) \qquad \overrightarrow{\mathcal{X}_{A}} \qquad 
(A \ \gamma, B \ \beta, C \ \alpha) \, .
\]
\begin{figure}[ht]
\begin{center}
\includegraphics[width=5in]{VOLUMEN_1/02_Espacios_Lineales/Figuras/TriangRotRef.png}
\caption{Transformaciones  que dejan invariante un triángulo equilátero. Concatenación de una rotación, $\mathcal{R}_{\frac{2 \pi}{3}}$ con una reflexión, $\mathcal{X}_{A}$,  respecto a un plano que pasa por $A$}
\label{TriangRotRef}
\end{center}
\end{figure}
\begin{enumerate}
  \item Construya la tabla de multiplicación para $\textbf{\em G}_{\triangle}$, vale decir $\textbf{\em G}_{\triangle}=\left\{ \mathcal{I}, \left\{\mathcal{R}_{i}\right\}, \left\{\bar{\mathcal{R}}_{j}\right\}, \left\{\mathcal{X}_{k}\right\} \right\}$ y la operación es concatenación tal y como mostramos en la figura \ref{TriangRotRef}. Donde $\mathcal{I}$ es la operación identidad, $\left\{\mathcal{R}_{i}\right\}$ es un conjunto de rotaciones en sentido horario, mientras que $\left\{\bar{\mathcal{R}}_{j}\right\}$ es un conjunto de rotaciones en el sentido antihorario, y $\left\{\mathcal{X}_{k}\right\}$ el conjunto de las reflexiones que dejan invariante el triángulo.
  \item Muestre que el conjunto de estas operaciones forman el grupo: $\textbf{\em G}_{\triangle}$.  
  
\item Identifique cada una de las $\mathcal{R}_{i}$ y $\bar{\mathcal{R}}_{j},$ y muestre además, que forman un subgrupo cíclico de orden 3. De igual modo identifique las reflexiones y muestre que, cada una de las reflexiones y la identidad, $\left\{\mathcal{I}, \mathcal{X}_{i}\right\}$, forman también un subgrupo cíclico, pero de orden 2.
  \item Considere las siguientes matrices:
\[
\mathbb{I}= \left(\begin{array}{cc}1 & 0 \\0 & 1\end{array}\right)\,, \quad 
\mathbb{A} = \left(\begin{array}{cc} -\frac{1}{2} & \frac{\sqrt{3}}{2} \\ \\ -\frac{\sqrt{3}}{2} & -\frac{1}{2}\end{array}\right)\,, \quad
\mathbb{B} = \left(\begin{array}{cc} -\frac{1}{2} & -\frac{\sqrt{3}}{2} \\ \\ \frac{\sqrt{3}}{2} & -\frac{1}{2}\end{array}\right) \,,
\]
\[
\mathbb{C}= \left(
\begin{array}{cc}
-1 & 0 \\
0 & 1\end{array}\right)\,, \quad 
\mathbb{D} = \left(
\begin{array}{cc} 
\frac{1}{2} & -\frac{\sqrt{3}}{2} \\ \\ 
-\frac{\sqrt{3}}{2} & -\frac{1}{2}\end{array}\right) \,, \quad
\mathbb{E} = \left(
\begin{array}{cc} 
\frac{1}{2} & \frac{\sqrt{3}}{2} \\ \\ 
\frac{\sqrt{3}}{2} & -\frac{1}{2}\end{array}\right) \,.
\]
Muestre que forman grupo bajo la multiplicación de matrices y que ese grupo es isomorfo a $\textbf{\em G}_{\triangle}$. 

\item Considere el conjunto de permutaciones de 3 objetos y la operación composición de permutaciones que discutimos como ejemplo en la sección \ref{EjemploGrupo}. ?` Es ese grupo isomorfo a $\textbf{\em G}_{\triangle}$? Justifique su respuesta.   
\end{enumerate}

\item Considere las siguientes funciones:
\[
f_{1}(x)= x\,, \quad f_{2}(x)= \frac{1}{x}\,, \quad f_{3}(x)=  \frac{1}{1-x}\,, \quad 
f_{4}(x)=  \frac{x-1}{x}\,, \quad f_{5}(x)=  1-x\,, \quad f_{6}(x)=  \frac{x}{x-1}\,.
\]
Muestre que forman grupo bajo la operación: $f_{i}(x) \odot f_{j}(x)= f_{i}(f_{j}(x))$, y que ese grupo es isomorfo a $\textbf{\em G}_{\triangle}$, del ejercicio anterior.  


\item  Definamos una operación binaria $\blacksquare$ como: 
\[ 
x\ \blacksquare\ y=x+y+\alpha xy \,,
\] 
con $x,y,\alpha\in\mathds{R}$ y además $\alpha\neq0$.
\begin{enumerate}
\item  Demuestre que $\blacksquare$ es asociativa.
\item  Muestre que $\blacksquare$ genera un grupo en $\left\{  \mathds{R}-
\left(  \frac{-1}{\alpha}\right)  \right\}$. Es decir, $\forall\,\, x,y\in\mathds{R}\,\,\wedge \,\,  x\neq\frac{-1}{\alpha}, y\neq\frac{-1}{\alpha}$, entonces: $x\ \blacksquare\ y$ forma un grupo.
\end{enumerate}

\item  Muestre que el siguiente conjunto de transformaciones en el plano $xy$ forman un grupo y construya su tabla de multiplicación.
\begin{enumerate}
\item $I=\left\{ x\rightarrow x  \,\, \wedge \,\, y\rightarrow y \right\}$.

\item $I=\left\{ x\rightarrow-x \,\, \wedge \,\, y\rightarrow-y \right\} $.

\item $I_{x}=\left\{ x\rightarrow-x \,\, \wedge \,\, y\rightarrow y \right\} $.

\item $I_{y}=\left\{ x\rightarrow x \,\, \wedge \,\, y\rightarrow-y \right\} $.
\end{enumerate}

\item Considere un conjunto $\textbf{\em S}$ conformado únicamente por números reales positivos. Consideremos las siguientes reglas sobre $\textbf{\em S}$: Por ``suma" de dos números entenderemos su producto  en el sentido usual, y el ``producto" de un elemento $r\in\textbf{\em S}$  y un número real $\lambda$ entenderemos $r$ elevado a la potencia  de $\lambda$, en el sentido usual ¿$\textbf{\em S}$ es un espacio vectorial?

\item Considere el conjunto de vectores en el plano conformado por  vectores localizados en el origen y cuyos puntos finales permanecen siempre en el primer cuadrante ¿Este conjunto es un espacio vectorial?

\item Muestre que también serán espacios vectoriales:
\begin{enumerate}
\item  El  conjunto de todas las funciones  $f=f(x)$ definidas en $x=1$ con $f\left(  1\right) =0$.  Si $f\left( 1\right)=c$ ¿Tendremos igual un espacio vectorial? ¿Por qué?
\item  Los vectores $\left( x,y,z\right)  \in \textbf{\em V}^{3}$ tal que sus componentes satisfacen el siguiente sistema de ecuaciones lineales:
\begin{align*}
a_{11}x+a_{12}y+a_{13}z  &  =0\\
a_{21}x+a_{22}y+a_{23}z  &  =0\\
a_{31}x+a_{32}y+a_{33}z  &  =0 \,.
\end{align*}
\end{enumerate}


%
%%%%%%%%%
\item Sea $\mathcal{P}_{n}$ el conjunto de todos los polinomios de grado $n,$ en $x,$ con coeficientes reales:
\[
\left| {p}_{n}\right> \rightleftharpoons p(x)=a_{0}+a_{1}x +a_{2}x^{2} +\dots +a_{n-1}x^{n-1}=\sum_{i=0}^{n-1}a_{i}x^{i}\,.
\]

\begin{enumerate}
\item  Demostrar que $\mathcal{P}_{n}$ es un espacio vectorial respecto a la suma de polinomios y a la multiplicación de polinomios por un número (número real).

\item  Si los coeficientes $a_{i}$ son enteros ¿$\mathcal{P}_{n}$ será un espacio vectorial? ¿Por qué?

\item ¿Cuál de los siguientes subconjuntos de $\mathcal{P}_{n}$ es un subespacio vectorial?

\begin{enumerate}
\item  El polinomio cero y todos los polinomios de grado $n-1$.

\item  El polinomio cero y todos los polinomios de grado par.

\item  Todos los polinomios que tienen a $x$ como un factor (grado $n>1$).

\item  Todos los polinomios que tienen a $x-1$ como un factor.
\end{enumerate}
\end{enumerate}

%
%%%%%%%%%
\item Un subespacio $\mathcal{P}$ es generado por: 
$\left| {x}_1 \right> =x^{3}+2x+1\,, \,\, \left| {x}_2 \right> =x^{2}-2\,, \,\, \left| {x}_3\right> =x^{3}+x $
¿Cuál(es) de los siguientes polinomios pertenece al subespacio  
$\mathcal{P}$?
\begin{enumerate}
\item $x^{2}-2x+1$.
\item $x^{4}+1$.
\item $-\frac12x^{3}+\frac52x^{2}-x-1$.
\item $x-5$.
\end{enumerate}

\item Resuelva los ejercicios anteriores utilizando {\bf Maxima}.

\end{enumerate}


\section{Espacios métricos, normados y con producto interno}

En está sección vamos a introducir una función de distancia, de manera que si tenemos un par de puntos o elementos de un espacio vectorial podemos hablar de que existe una cierta distancia entre ellos. Se dice que la función distancia induce una topología sobre el espacio vectorial. 

Comenzaremos definiendo el concepto de métrica sobre espacios vectoriales y con esta estructura llegar a la noción de norma. 

\subsection{Métricas y espacios métricos}
\label{EspaciosMetricos}
\index{Espacios vectoriales lineales!Métricos}
\index{Métricos!Espacios vectoriales lineales}
\index{Métrica}
\index{Distancia!Espacios vectoriales lineales}
\index{Espacios vectoriales lineales!Distancia}
La dotación de propiedades en los espacios vectoriales lineales lo constituye la idea de métrica o distancia entre sus elementos. El concepto de métrica surge de la generalización de la idea de distancia entre dos puntos de la recta real.

Un espacio vectorial será métrico si podemos definir una función:  
\[
d:\textbf{\em V}\times\textbf{\em V}\rightarrow\mathds{R} \,\, /  \,\, \forall  \left| x \right> ,\left| y
\right> ,\left|  {z}\right> \in\textbf{\em V} \,,
\] 
tal que se cumple lo siguiente:
\begin{enumerate}
\item $d\left(  \left| x \right> ,\left|  {y} \right> \right)  \geq 0\,, \quad \mathrm{si}  \quad 
d\left(  \left| x \right> ,\left| y \right> \right) = 0 \,\, \Rightarrow \,\, \left| x \right> \equiv \left|  {y} \right>$.

\item $d\left(  \left| x \right> ,\left| y \right> \right)  \equiv d\left(  \left| y \right>, \left| x \right> \right)$.

\item $d\left(  \left| x \right> ,\left|  y\right> \right)  \leq d\left(  \left| x \right> ,\left| z\right> \right)  +
d\left(  \left| y \right> ,\left|  {z}\right> \right) $  (La desigualdad triangular).
\end{enumerate}

Así, diremos que $\left(\textbf{\em V}, \textbf{\em K}, \boxplus , d\right)  $ es un espacio vectorial lineal, métrico.

Podemos enumerar algunos ejemplos de espacios métricos:

\begin{enumerate}

\item Espacios reales $\mathds{R}^{n}$. Aquí indicaremos  los diferentes puntos del espacio por las coordenadas:  $(x_1, x_2, \dots, x_n)$, $(y_1, y_2, \dots, y_n)$, ...

\begin{enumerate}
\item  Para $\mathds{R}$, es decir la recta real, la definición de métrica es:
$d\left(  \left| x \right> ,\left| y \right> \right)  \equiv \left|  x-y\right|$\,.

\item  Para $\mathds{R}^{2}$, es decir el plano, una definición de métrica
es: $d\left(  \left| x \right> ,\left|  {y} \right> \right)  \equiv\sqrt{\left(  x_{1}-y_{1}\right)^{2}+\left( x_{2}-y_{2}\right)^{2}}$. 

También podemos construir otra definición de métrica como: 
$d \left(  \left| x \right> ,\left|
{y}\right> \right)  \equiv\left|  x_{1}-y_{1}\right|  +\left|x_{2}-y_{2}\right|$. La primera de estas métricas se conoce como métrica euclídea y la segunda como métrica Manhattan o métrica de taxistas. Es claro como el mismo espacio vectorial genera varios espacios métricos, dependiendo de la definición de métrica. Para estos casos particulares, las métricas ``miden'' el desplazamiento entre dos puntos de forma distinta: en aviones (métrica euclídea) o vehículos terrestre en ciudades. 

\item  En general para espacios  reales $\mathds{R}^{n}$ una posible
definición de métrica será:
 \[ 
 d\left(  \left| x \right>,\left| y \right> \right)  \equiv
 \sqrt{\left(  x_{1} -y_{1}\right)^{2}+\left(  x_{2}-y_{2}\right)^{2}+\left(  x_{3} -y_{3}\right)^{2}+\cdots+\left(  x_{n}-y_{n}\right)^{2}}\,.
\]
Esta definición de métrica, o distancia, no es más que una generalización del teorema de Pitágoras y se denomina ``distancia euclidiana''. 
\end{enumerate}

\item  Espacios unitarios $n-$dimensionales, o espacios  complejos,
$\mathds{C}^{n}$. La definición de distancia puede construirse
como:  
\[
d\left(  \left| x \right> ,\left|  {y}\right> \right)  \equiv 
\sqrt{\left|  x_{1}-y_{1}\right|^{2}+\left|x_{2}-y_{2}\right|^{2}+\left|  x_{3}-y_{3}\right|^{2}+\cdots+\left| x_{n}-y_{n}\right|^{2}}\,,
\]  
y es claro que se recobra la idea de distancia en el plano complejo: $d\left(  \left| x \right> ,\left| y\right> \right)  \equiv\left|  x-y\right|$.

\item  Para los espacios de funciones $\mathcal{C}_{\left[  a,b\right]}^{\infty}$ una posible definición de distancia sería: 
\[
d\left(\left|  {f}\right> ,\left|  {g}\right> \right) \equiv
\max_{t\in\left[  a,b\right]  }\left|  f(t)  -g\left(t\right)  \right| \,.
\]  
\item La métrica trivial o discreta
\[
d \left( \left|x\right> ,\left|y\right> \right) =
\left\{
\begin{tabular}{cc}
1 & si  $\ x \neq y$ \\
0 & si  $\ x= y$
\end{tabular}
\right.
\]

\end{enumerate}

\subsection{Normas y espacios normados}
\label{EspaciosNormados}
\index{Norma!Espacios vectoriales lineales}
\index{Espacios vectoriales lineales!Norma}
\index{Espacios vectoriales lineales!Normados}
\index{Normados!Espacios vectoriales lineales}
La idea de distancia (o métrica) es el equipamiento más elemental que uno le puede exigir a un espacio vectorial. Mucho más interesante aún son aquellos espacios vectoriales que están equipados con la idea de norma y, a partir de allí, se define la idea de distancia. La norma tiene que ver con el ``tamaño'' del vector y la métrica tiene que ver con la distancia entre vectores. Cuando definimos la métrica a partir de la norma, vinculamos las propiedades algebraicas del espacio con sus propiedades geométricas.

La norma, $\mathcal{N}\left(  \left|  v_{i}\right> \right) \equiv \left\|  \left|  v_{i}\right> \right\|$, de un espacio vectorial $\textbf{\em V}=\left\{  \left|{v}_{1}\right>, \left| v_{2}\right>, \left|  v_{3}\right>, \cdots , \left| v_{n}\right> \right\}$ será una función:
\[
\mathcal{N}: \textbf{\em V}\rightarrow\mathds{R} \,\, /  \,\, \forall \ \left| v_{i}\right> \in\textbf{\em V} \,,
\] 
que cumple con:
\begin{enumerate}
\item  $\left\| \left|  v_{i}\right> \right\|  \geq0 \,, \quad \mathrm{si} \quad \left\|\left|v_{i}\right> \right\|=0
\,\, \Rightarrow \,\, \left|  v_{i}\right> \equiv\left|  {0}\right> $. 

\item $\left\|  \alpha\left|  v_{i}\right> \right\|  = \left|\alpha\right| \, \left\|  \left|  v_{i}\right> \right\|$. 

\item $\left\|  \left|{v}_{i}\right> +\left|{v}_{j} \right> \right\|  \leq\left\|  \left|{v}_{i} \right> \right\| +\left\|  \left|{v}_{j} \right> \right\|$ (Desigualdad triangular).
\end{enumerate}

\index{Norma!Distancia}
\index{Distancia!Norma}
\index{Banach!Stefan Banach}
\index{Banach!Espacios Vectoriales Normados}
La definición de norma induce una métrica de la forma:
\[
d\left(\left| v_{i} \right>,\left|{v}_{j} \right> \right)\equiv \left\|  \left|{v}_{i}\right> -\left| v_{j}\right> \right\| \,.
\label{banach}
\]

Se denota en este caso un espacio vectorial normado\footnote{El concepto de espacio normado fue formulado en 1922 de manera independiente por: S. Banach, H. Hahn y N. Wiener.} como 
$\left(  \textbf{\em V}, \textbf{\em K}, \boxplus; \left\|
\cdot\right\|  \right) $, espacio que también es conocido como un espacio de Banach\footnote{{STEFAN BANACH} (1892 Kracovia, Polonia-1945 Lvov,Ucrania) Matemático polaco, uno de los fundadores del análisis funcional moderno, con sus mayores contribuciones a la teoría de espacios topológicos. Hizo también importantes aportes a la teoría de la medida, integración y teoría de conjuntos y series ortogonales.}. Esto significa que todo espacio normado es a su vez un espacio métrico, pero es importante señalar que no todo espacio métrico es normado.

De la definición de distancia (\ref{banach}) resulta que la métrica así definida es invariante bajo traslaciones de vectores. Esto es, si; 
$\left|  {\tilde{x}}\right> =\left| x \right> +\left|  {a}\right> \,\, \wedge \,\,  
\left|  {\tilde{y}}\right> =\left| y \right> +\left|  {a}\right> $, entonces,  $ d\left(\left| x \right> ,\left| y\right> \right)  \equiv d\left(  \left|  {\tilde{x}}\right>,\left|  {\tilde{y}}\right> \right)$. Y además es homogénea: $d\left(\lambda\left| x \right> , \lambda \left| y\right> \right) = |\lambda| d\left(\left| x \right> ,\left| y\right> \right)$.


Como ejemplos de espacios normados podemos mostrar los siguientes:

\begin{enumerate}
\item  Los espacios reales, $\mathds{R}^{n}$ y los espacios complejos $\mathds{C}^{n}$.  Para estos espacios de Banach, la norma se define como:
\[
\left\|  \left| x \right> \right\|  =\sqrt{\left|x_{1}\right|^{2}+\left|  x_{2}\right|^{2}+\left|  x_{3}\right|
^{2}+\cdots+\left|  x_{n}\right|^{2}}=\left(  \sum_{i=1}^{n}\left| x_{i}\right|^{2}\right)^{\frac{1}{2}} \,.
\]

Para un espacio en $\mathds{R}^{3}$ se cumple que $\left\|\left| x \right> \right\|  = \sqrt{x_{1}^{2}+x_{2}^{2}+x_{3}^{2}}$, por lo tanto, la idea de norma generaliza la noción de ``tamaño'' del vector $\left| x \right>$. Es claro que la definición de distancia se construye a partir de la norma de la forma:
\[
d\left(  \left| x \right> ,\left| y \right>\right)  \equiv\left\|  \left| x \right> -\left|
{y}\right> \right\|  =\sqrt{\left|  x_{1}-y_{1}\right|^{2}+\left|  x_{2}-y_{2}\right|^{2}+\left|  x_{3}-y_{3}\right|^{2}
+\cdots+\left|  x_{n}-y_{n}\right|^{2}}\,.
\]

Este espacio se llama espacio normado euclidiano de dimensión $n$.

\item  Para el espacio lineal de matrices $n\times n$, reales o complejas, con el campo $\textbf{\em K}$ real o complejo, una definición de norma es:
\[
\left\|  M\right\|  =\sum_{a=1}^{m}\sum_{b=1}^{n}\left|  M_{ab}\right| \,,
\]
y la correspondiente definición de distancia:
\[
d\left(  \left| x \right> ,\left| y \right>\right)  \equiv\left\|  M-N\right\|  =\sum_{a=1}^{m}\sum_{b=1}^{n}\left|M_{ab}-N_{ab}\right| \,.
\]

\item  Para los espacios funcionales $\mathcal{C}_{\left[  a,b\right]
}^{\infty}$ una posible definición de norma sería:
\[
\left\|  \left|  {f}\right> \right\|  =\max_{t\in\left[a,b\right]  }\left|  f(t)  \right| \,,
\]
otra posible definición puede ser:
\[
\left\|  \left|  {f}\right> \right\|  =\sqrt{  \int_{a}^b\ \left|  f(t)  \right|^{2} \mathrm{d}t }\,.
\]
\end{enumerate}

\subsection{Espacios euclidianos}
\label{EspaciosHilbert}
\index{Interno!Producto}
\index{Producto interno!Espacios vectoriales lineales}
\index{Espacios vectoriales lineales!Producto interno}
\index{Hilbert!Espacios vectoriales lineales}
\index{Hilbert!David Hilbert}
\index{Espacios vectoriales lineales!Hilbert}
El siguiente paso en la construcción de espacios vectoriales más ricos es equiparlo con la definición de producto interno y a partir de esta definición construir el concepto de norma y con éste el de distancia. La idea de producto interno generaliza el concepto de producto escalar de vectores en
$\mathds{R}^{3}$ e incorpora a los espacios vectoriales abstractos el concepto de ortogonalidad y descomposición ortogonal. Históricamente, la teoría de espacios vectoriales con producto interno es anterior a la teoría de espacios métricos y espacios de Banach y se le debe a D.
Hilbert\footnote{{DAVID HILBERT }(1862 Kaliningrad, Rusia-1943 Göttingen, Alemania) Matemático alemán defensor de la axiomática como enfoque primordial de los problemas científicos. Hizo importantes contribuciones en distintas áreas de la matemática, como: Invariantes, Campos de Números Algebraicos, Análisis Funcional, Ecuaciones Integrales, Física-Matemática y Cálculo en
Variaciones.}.  Adicionalmente, la semejanza entre la geometría euclidiana y la geométrica de
$\mathds{R}^{n}$ ha hecho que espacios en los cuales se puedan definir, distancia, ángulos, a partir de una definición de producto interno, se denominen también espacios euclidianos.

\subsubsection{Producto interno}
\label{ProductoInterno}
\index{Interno!Producto}
\index{Producto interno}
En un espacio vectorial  $\textbf{\em V}=\left\{  \left| v_{1}\right>, \left| v_{2}\right>, \left| v_{3}\right>, \cdots , \left|  v_{n}\right> \right\}$, la definición del producto interno de dos vectores la denotaremos como $\left<{v}_{i}\right|\left.{v}_{j}\right>$ y es una aplicación:  
\[
\mathcal{I}\left( \left| v_{i}\right>,\left| v_{j}\right>\right): 
\textbf{\em V} \times \textbf{\em V} \rightarrow \textbf{\em K}\,,  \,\, \forall \,\,\left| v_{i}\right> ,\left| v_{j}\right> \in\textbf{\em V}\,.
\]
Es decir, asocia a ese par de vectores con un elemento del campo $\textbf{\em K}$. 

Las propiedades que definen el producto interno son:
\begin{enumerate}
\item $\left<{v}_{i}\right|  \left. v_{i}\right> \equiv
\left\|  \left| v_{i}\right> \right\|^2  \,\, {\in} \,\, \textbf{\em K} \,\,\wedge\,\, \left<{v}_{i}\right|  \left.{v}_{i}\right> \geq0{\quad}\forall\ \left|  v_{i} \right> \in\textbf{\em V}\,,\quad \mathrm{si}
\quad \left<{v}_{i}\right|  \left. v_{i}\right> =0\,\, \Rightarrow \,\,\left| v_{i}\right> \equiv\left| {0}\right> $.

\item $\left<{v}_{i}\right|  \left.{v}_{j}\right> =\left<{v}_{j}\right|  \left. v_{i}\right> ^{\ast}{\quad}\forall\ \left| v_{i}\right> ,\left| v_{j}\right> \in\textbf{\em V}$.

\item $\left<{v}_{i}\right|  \left. \alpha{v}_{j}+\beta v_{k} \right> = 
\alpha \left<{v}_{i}\right|  \left.{v}_{j}\right> + \beta \left<{v}_{i}\right|  \left. v_{k}\right> 
\,\,\forall\,\, \left| v_{i}\right> ,\left|{v}_{j}\right> ,\left| v_{k}\right> \in\textbf{\em V} \,\, \wedge \,\, \alpha, \beta \, \in \, \textbf{\em K}$.

\item $\left<{\alpha{v}_{i}+\beta{v}_{j}}\right|  \left. v_{k}\right> =
\alpha^{\ast}\left<{v}_{i}\right|  \left.{v}_{k}\right> + \beta^{\ast} \left<{v}_{j}\right|  \left. v_{k}\right> 
\,\,\forall \,\, \left| v_{i}\right> ,\left|{v}_{j}\right>,\left| v_{k}\right> \in\textbf{\em V} \,\, \wedge \,\,  \alpha, \beta \, \in \, \textbf{\em K}$.

\item $\left<{v}_{i}\right|  \left.  {0}\right> =\left<{0}\right|  \left.{v}_{i}\right> =0$.
\end{enumerate}

\index{Producto interno!Norma}
\index{Norma!Producto interno}
\index{Producto interno!Distancia}
\index{Distancia!Producto interno}

Nota: la segunda y cuarta propiedad resultan del hecho de que si el campo  es complejo $\textbf{\em K}= \mathds{C}$, entonces:
\[
\left<\alpha{v}_{i}\right|  \left. \alpha{v}_{i}\right>=
\alpha^2\left<{v}_{i}\right|  \left.{v}_{i}\right>= 
i^2\left<{v}_{i}\right|  \left.{v}_{i}\right>= 
-\left<{v}_{i}\right|  \left.{v}_{i}\right>\,,
\]
lo cual contradice el hecho de que la norma tiene que ser positiva. Por eso la necesidad de tomar el complejo conjugado. Se dice entonces, que el producto escalar es {\it antilineal} respecto al primer factor y {\it lineal} respecto al segundo. 


A partir de la definición de producto interno se construyen los conceptos de norma y distancia:
\[
\left\|  \left| v_{i}\right> \right\|  =\sqrt{\left<{v}_{i}\right|  \left.  v_{i}\right> }\quad \mbox{y} \quad
d\left(  \left| v_{i}\right> ,\left| v_{j}\right> \right)  \equiv\left\|  \left| v_{i}\right> -\left| v_{j}\right> \right\|  =\sqrt{\left<{{v}_{i}-{v}_{j}}\right| \left.  {{v}_{i}-{v}_{j}}\right> }\,.
\]

\subsubsection{La desigualdad de Cauchy-Schwarz: los ángulos entre vectores reales y complejos}
\label{DesigualdadCauchy-Schwarz}
\index{Cauchy-Schwarz!Desigualdad}
\index{Desigualdad!Cauchy-Schwarz}
Todo producto interno $\left<{v}_{i}\right|  \left.{v}_{j} \right> $ definido en un espacio vectorial normado $\textbf{\em V}=\left\{  \left|  v_{1}\right>, \left| v_{2}\right>, \left|  v_{3}\right>, \cdots ,\left|  v_{n}\right> \right\} $ cumple con la desigualdad de Cauchy-Schwarz:
\[
\left|  \left<{v}_{i}\right|  \left. v_{j}\right> \right|^{2}\leq\left<{v}_{i}\right|  \left.  v_{i}\right> \left<{v}_{j}\right|  \left.{v}_{j}\right>\quad \Longleftrightarrow \quad 
\left|  \left<{v}_{i}\right|  \left. v_{j}\right> \right|  \leq\left\|  \left| v_{i}\right> \right\|  \ \left\|  \left|{v}_{j}\right> \right\|\,.
\]
Es claro que si $\left|{v}_{i}\right> =\left|  {0}\right> \,\,\wedge \,\, \left| v_{j}\right> =\left|  {0}\right>$ se cumple la igualdad y es trivial la afirmación. 

Para demostrar la desigualdad, tomemos dos vectores $\left|{v}_{i}\right> \,\, \wedge \,\, \left| v_{j}\right>$ cualesquiera, entonces podemos construir un tercero: $\left|{v}_{k}\right>=\alpha\left| v_{i}\right> +\beta\left|{v}_{j}\right>$  ($\alpha$ y  $\beta$ tendrán valores particulares), por lo tanto:
\[
\left<{v}_{k}\right|  \left. v_{k}\right>  \equiv \left<\alpha{{v}_{i}+\beta v_{j}}\right|  \left.  \alpha{v}_{i}+{\beta v_{j}}\right> \geq 0 \,,
\]
esto significa que:
\begin{align*}
\left<\alpha{{v}_{i}+\beta v_{j}}\right|  \left.  \alpha{v}_{i} +{\beta v_{j}}\right>  &  =\left<\alpha{v}_{i}\right|
\left.  \alpha{v}_{i}\right> +\left<\alpha{v}_{i}\right| \left.  {\beta v_{j}}\right> +\left<{\beta v_{j}}\right|
\left.  \alpha{v}_{i}\right> +\left<{\beta v_{j}}\right| \left.  {\beta v_{j}}\right> \geq0\\
&  =\left|  \alpha\right|^{2}\left< v_{i}\right|  \left.  v_{i}\right> +\alpha^{\ast} \beta\left<{v}_{i}\right|  \left.  v_{j}\right> +{\beta}^{\ast}\alpha\left<{v}_{j}\right|  \left. v_{i}\right> +\left|  {\beta}\right|^{2}\left< v_{j}\right|  \left.  v_{j}\right> \geq 0 \,.
\end{align*}

Si $\alpha=\left<{v}_{j}\right|  \left. v_{j}\right>$,  se tiene que:
\begin{eqnarray*}
\left<{v}_{j}\right|  \left. v_{j}\right> \left< v_{i}\right|  \left.  v_{i}\right> &+&{\beta}\left< v_{i}\right|  \left.  v_{j}\right> +{\beta}^{\ast}\left<{v}_{j}\right|  \left.  v_{i}\right> +\left| {\beta}\right|^{2}\geq0 \\ 
\left<{v}_{j}\right|  \left. v_{j}\right> \left< v_{i}\right|  \left.  v_{i}\right> &\geq &-{\beta}\left<{v}_{i}\right|  \left.  v_{j}\right> - {\beta}^{\ast}\left<{v}_{j}\right|  \left.  v_{i}\right> - \left|{\beta}\right|^{2} \,,
\end{eqnarray*}
seguidamente seleccionamos: $\beta=-\left<{v}_{j}\right|  \left. v_{i}\right>$, y por lo tanto: ${\beta}^{\ast}=-\left<{v}_{i}\right|  \left. v_{j}\right>$,  consecuentemente:
\begin{eqnarray*}
\left<{v}_{j}\right|  \left. v_{j}\right> \left< v_{i}\right|  \left.  v_{i}\right> &\geq &
\left<{v}_{j}\right|  \left. v_{i}\right>\left<{v}_{i}\right|  \left.  v_{j}\right> +
\left< v_{i}\right|  \left. v_{j}\right>\left<{v}_{j}\right|  \left.  v_{i}\right> -
\left<{v}_{j}\right|  \left. v_{i}\right>\left<{v}_{i}\right|  \left.
{v}_{j}\right> \\
\left<{v}_{j}\right|  \left. v_{j}\right> \left< v_{i}\right|  \left. v_{i}\right> &\geq &
\left< v_{i}\right|  \left. v_{j}\right> \left<{v}_{j} \right|  \left. v_{i}\right> =\left|  \left<{v}_{i} \right|  \left. v_{j}\right> \right|^{2} \,. \quad \quad \blacktriangleleft
\end{eqnarray*}

De la desigualdad de Cauchy-Schwarz y la definición de norma se desprende que:
\[
\frac{\left|  \left<{v}_{i}\right|  \left. v_{j}\right>\right|^{2}}{\left\|  \left| v_{i}\right> \right\|^{2}\left\|
\left|  v_{j}\right> \right\|^{2}}\leq1 \,\, \Rightarrow\,\, -1\ {\leq}\frac{\left|  \left<{v}_{i}\right|  \left.{v}_{j}\right> \right|  }{\left\|  \left| v_{i}\right>\right\|  \left\|  \left|  v_{j}\right> \right\|  }\leq1 \,,
\]
por lo tanto podemos definir el ``ángulo'' entre los vectores abstractos
$\left| v_{i}\right> \,\,\wedge\,\,\left| v_{j}\right>$ como:
\[
\cos(\Theta_{\mathds{G}})=\frac{\left|  \left<{v}_{i}\right|  \left.{v}_{j}\right> \right|  }{\left\|  \left|  v_{i}\right> \right\|  \left\|  \left| v_{j}\right> \right\|  } \,,
\]
donde hemos denotado como $\Theta_{\mathds{G}}$ el ángulo genérico que forman los vectores reales o complejos. 
 
Si estamos considerando espacios vectoriales reales  -en los cuales el campo corresponde a los números reales- entonces el ángulo definido entre vectores abstractos reales corresponde al que intuitivamente siempre hemos considerado para los vectores cartesianos y que discutimos en la sección \ref{ProductoEscalar1},  
\[
\cos(\Theta_{\mathds{R}})=\frac{\left|  \left<{v}_{i}\right|  \left.{v}_{j}\right> \right|  }{\left\|  \left|  v_{i}\right> \right\|  \left\|  \left| v_{j}\right> \right\|  }\,,  \quad \mathrm{con } \quad 0 \leq \Theta_{\mathds{R}} \leq \pi \,,
\]
donde se toma $\Theta_{\mathds{R}} = 0$ para vectores colineales y $\Theta_{\mathds{R}} = \pi$ para vectores opuestos (antilineales). Si bien es cierto que esta definición coincide con la de los vectores cartesianos, hay que resaltar que la estamos extendiendo para cualquier vector abstracto, vale decir: funciones reales, matrices, y todos los objetos matemáticos que cumplan con las reglas para los espacios vectoriales expuestas en \ref{EspaciosVectoriales}. 

Para el caso de espacios vectoriales complejos la situación es más sutil y significa definir un ángulo entre dos vectores abstractos y complejos, sin embargo podemos abordar el problema suponiendo:
\begin{enumerate}
  \item un espacio complejo $n-$dimensional de $n-$uplas de números complejos $\left| z \right> \leftrightarrow (z_{1}, z_{2}, \cdots z_{n}) $ asociado (isomorfo) a un espacio vectorial real de $2n$ dimensiones, con vectores representados por $2n-$uplas de números reales $\left|w\right> \leftrightarrow (\mathrm{Re}(z_{1}), \mathrm{Re}(z_{2}), \cdots \mathrm{Re}(z_{n}), \mathrm{Im}(z_{1}),\mathrm{Im}(z_{2}), \cdots \mathrm{Im}(z_{n}) )$, donde hemos representado $\mathrm{Re}(z_{j})$ y $\mathrm{Im}(z_{j})$ como las partes reales e imaginarias de $z_{j}$, respectivamente o,  
  \item directamente a partir de una definición de producto interno entre vectores complejos implementado por: $\left<{w}_{i}\right|  \left.{v}_{j}\right> = \sum_{j=1}^{n} {w}^{*}_{j} v_{j}$.
\end{enumerate}
Ambas aproximaciones no son del todo independientes pero igualmente justificadas\footnote{Scharnhorst, K. (2001). Angles in complex vector spaces. \textit{Acta Applicandae Mathematica}, \textbf{69}(1), 95-103.}. 

Consideremos el segundo caso, esto es: directamente a partir de una definición de producto interno entre vectores complejos. Para este caso consideramos un ángulo complejo, y $\cos(\Theta_{\mathds{C}})$ una función de variable compleja, que puede ser expresada en su forma polar como:
\[
\cos(\Theta_{\mathds{C}})=\frac{\left|  \left<{v}_{i}\right|  \left.{v}_{j}\right> \right|  }{\left\|  \left|  v_{i}\right> \right\|  \left\|  \left| v_{j}\right> \right\|  }  \,\, \Rightarrow \,\, 
\cos(\Theta_{\mathds{C}})= \rho \, \mathrm{e}^{ \phi}\,,  \quad \mathrm{con } \quad 
\rho = |\cos(\Theta_{\mathds{C}})| < 1\,.
\]

Entonces podemos asociar $\rho = \cos(\Theta_{H})$ y definir $\Theta_{H}$, en el rango $0 \leq \Theta_{H} \leq \pi/2$, como el \textit{ángulo hermítico} entre los vectores complejos $\left| v_{i} \right>$ y $\left| v_{j} \right>$. Mientras que $\phi$, definido en $-\pi \leq \phi \leq \pi$, corresponde al pseudo ángulo de Kasner, que representa la orientación o rotación del ángulo hermítico y no tiene mayor significado al cuantificar el ángulo entre esos dos vectores. Esta diferencia de significados puede intuirse cuando  multiplicamos $\left| v_{i} \right>$ y $\left| v_{j} \right>$, por una constante compleja: $\left| \tilde{v}_{i} \right> \rightarrow \alpha_{i} \left| v_{i} \right>$ y comprobamos que el ángulo $\Theta_{H}$ permanece inalterado y no así el ángulo de Kasner\footnote{Puede consultarse Reju, V. G., Koh, S. N., y Soon, Y. (2009). An algorithm for mixing matrix estimation in instantaneous blind source separation. \textit{Signal Processing}, \textbf{89}(9), 1762-1773.}. 

\subsubsection{Teoremas del coseno y de Pitágoras}
\label{TeoremaCoseno}
\index{Teorema del!Coseno}
\index{Coseno!Teorema del}
\index{Teorema de!Pitágoras}
\index{Pitágoras!Teorema de}
A partir de la definición de norma se obtiene:
\[
\begin{array}{rl}
\left\|  \left|v_{i}\right> -\left|v_{j}\right>\right\| ^{2}= & \left<{{v}_{i} -{v}_{j}}\right|  \left.  {{v}_{i} -{v}_{j}}\right> =\left<{v}_{i}\right|  \left. v_{i} \right> -\left<{v}_{i}\right|  \left.v_{j}\right> -\left<{v}_{i}\right|  \left.v_{j}\right> ^{\ast} +\left<{v}_{j}\right|  \left. v_{j}\right>   \\
       =& \left<{v}_{i}\right|  \left.v_{i}\right> +\left<{v}_{j}\right|  \left. v_{j}\right>   -2 \, \mathrm{Re}\left(\left<{v}_{i}\right|  \left. v_{j}\right> \right) \,,
\end{array}
\]
con lo cual hemos generalizado el teorema del coseno para un espacio vectorial abstracto: 
\[
\left\|  \left|v_{i}\right> -\left|v_{j}\right>\right\|^{2}=
\left\|  \left|v_{i}\right> \right\|^{2} +\left\|  \left|v_{j}\right> \right\|^{2} -2\left\|  \left|{v}_{i}\right> \right\|  \left\|  \left| v_{j}\right> \right\|  \cos(\Theta_{\mathds{G}}) \, .
\]

De tal forma que para espacios vectoriales reales tendremos: 
\[
\left\|  \left| v_{i}\right> -\left| v_{j}\right>\right\|^{2}=
\left\|  \left| v_{i}\right> \right\|^{2} +\left\|  \left| v_{j}\right> \right\|^{2} -2\left\|  \left|{v}_{i}\right> \right\|  \left\|  \left| v_{j}\right> \right\|  \cos(\Theta)\,, \quad \mathrm{con } \quad 0 \leq \Theta \leq \pi \,,
\]
y para espacios vectoriales complejos: 
\[
\left\|  \left| v_{i}\right> -\left| v_{j}\right>\right\|^{2}=
\left\|  \left| v_{i}\right> \right\|^{2}+\left\|  \left| v_{j}\right> \right\|^{2} -2\left\|  \left|{v}_{i}\right> \right\|  \left\|  \left| v_{j}\right> \right\|  \cos(\Theta_{H}) \cos(\phi)\,, \quad \mathrm{con} \quad 0 \leq \Theta_{H} \leq \pi/2 \,.
\]

Para el caso que los vectores $\left| v_{i}\right> \,\,\wedge\,\, \left|{v}_{j}\right>$ sean ortogonales, esto es $\left<{v}_{i}\right|  \left. v_{j}\right> =0$, tendremos el teorema de Pitágoras generalizado: 
\[
\left\|  \left| v_{i}\right> -\left| v_{j}\right>\right\|^{2} \equiv \left\|  \left| v_{i}\right> +\left| v_{j}\right>\right\|^{2}=\left\|  \left| v_{i}\right> \right\|^{2}+\left\|  \left| v_{j}\right> \right\|^{2}\,.
\]

Veamos algunos ejemplos de espacios vectoriales con producto interno.

\begin{enumerate}
\item  Espacios euclidianos reales, $\mathds{R}^{n}$ y espacios euclidianos complejos
$\mathds{C}^{n}$.

Los vectores en estos espacios euclidianos  pueden ser representados por 
$\left| x \right> =\left(x_{1},x_{2},\cdots x_{n}\right) \,\,\wedge\,\, \left| y \right> =\left(  y_{1},y_{2},\cdots,y_{n}\right)  $ y \textbf{el producto interno} queda definido por:
\[
\left<{x}\right|  \left.  {y}\right> =x_{1}^*
y_{1}+x_{2}^*y_{2}+x_{3}^*y_{3},\cdots x_{n}^*y_{n}=\sum_{i=1}^{n}x_{i}^*y_{i} \,,
\]
es claro que esta definición de producto interno coincide, para $\mathds{R}^{2}$
(y $\mathds{R}^{3}$) con la idea de producto escalar convencional que consideramos en las secciones \ref{ProductoEscalar1} y \ref{ProductoEscalar2}, vale decir:
\[
\left.
\begin{array}
[c]{c}
{\bf{a}}=a_{x}{\mathbf{i}} +a_{y}{\mathbf{j}}\\
\\
{\bf{b}}=b_{x}{\mathbf{i}} +b_{y}{\mathbf{j}}
\end{array}
\right\} \,\, \Rightarrow \,\, {\bf{a}} \cdot {\bf{b}}=a_{x}b_{x}+a_{y}b_{y} \,.
\]

Ahora bien, el lector puede comprobar que para vectores en $\mathds{R}^{2}$ también se puede proveer una definición de producto interno:
\[
{\bf{a}}\circledast{\bf{b}}=2a_{x}b_{x}+a_{x}b_{y}+a_{y}b_{x}+a_{y}b_{y} \,,
\]
igualmente válida, con lo cual es claro que en un mismo espacio vectorial pueden coexistir diferentes productos internos. 

Por su parte, la \textbf{norma} es:
\[
\left\|  \left| x \right> \right\|  =\sqrt{\left< {x}\right|  \left.  {x}\right> }=\sqrt{x_{1}^{2}+x_{2}^{2}+x_{3}^{2},\cdots+x_{n}^{2}}=\sqrt{\sum_{i=1}^{n}x_{i}^{2}} \,.
\]

La \textbf{distancia} también recupera la idea intuitiva de distancia euclidiana:
\begin{align*}
d\left(  \left| x \right> ,\left| y \right>\right)   &  \equiv\left\|  \left| x \right> -\left| {y}\right> \right\|  =\sqrt{\left<{x-y}\right| \left.  {x-y}\right> }\\
         & \\
d\left(  \left| x \right> ,\left| y \right> \right)   &  = \sqrt{\left(  x_{1}-y_{1}\right)^{2}+\left(  x_{2} -y_{2}\right)^{2}+\left(  x_{3}-y_{3}\right)^{2}+\cdots+\left( x_{n}-y_{n}\right)^{2}} \,.
\end{align*}
El teorema del coseno queda como:
\[
\sum_{i=1}^{n}\left(  x_{i}+y_{i}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2} +\sum_{i=1}^{n}y_{i}^{2}+2 \sqrt{\sum_{i=1}^{n}x_{i}^{2}} \  \sqrt{\sum_{i=1}^{n}y_{i}^{2}} \  \cos(\Theta) \,,
\]
mientras que el teorema de Pitágoras es:
\[
\sum_{i=1}^{n}\left(  x_{i}+y_{i}\right)^{2}=\sum_{i=1}^{n}x_{i}^{2} +\sum_{i=1}^{n}y_{i}^{2}\,,
\]
es obvio que para $\mathds{R}^{2}$ tanto el teorema del coseno como el teorema de Pitágoras retoman su forma tradicional.

Finalmente la desigualdad de Cauchy-Schwarz se expresa de la siguiente manera:
\[
\left|  \left<{x}\right|  \left.  {y}\right>\right|  \leq \left\|  \left| x \right> \right\|  \ \left\|\left| y \right> \right\| 
\,\, \Rightarrow \,\,
\left|  \sum_{i=1}^{n}x_{i}y_{i}\right|^{2}\leq  \sum_{i=1}^{n}x_{i}^{2} \ \sum_{i=1}^{n}y_{i}^{2} \,.
\]

\item  Para los espacios de funciones continuas $\mathcal{C}_{\left[a,b\right]  }^{\infty}$ una posible definición de \textbf{producto interno} sería:
\[
\left<{f}\right|  \left.  {g}\right>  = \int_a^b\mathrm{d}x\ f^{\ast}\left(x\right)  \ g\left(x\right) \,,
\]
de la cual se deriva la expresión para la \textbf{norma}:
\[
\left\|  \left|  {f}\right> \right\|^{2}=\left<{f}\right|  \left.  {f}\right> =\int_a^b \mathrm{d}x\ \left|  f(x)  \right|^{2}\,.
\]

La \textbf{distancia} entre funciones quedará definida como:
\begin{align*}
d\left(  \left|  {f}\right> ,\left|  {g}\right> \right)   &  \equiv\left\|  \left|  {f}\right> -\left|{g}\right> \right\|  \equiv
\sqrt{\left<{f-g}\right|  \left.  {f-g}\right> }=
\sqrt{\left<{f}\right|  \left.  {f}\right>  -\left<{f}\right|\left.  {g}\right> -\left<{f}\right|  \left.{g}\right> ^{\ast}+\left<{g}\right|  \left.{g}\right> }\\
& \\
d\left(  \left|  {f}\right> ,\left|  {g}\right>\right)   &  =\sqrt{\int_a^b \mathrm{d}x\ \left| f(x)  -g(x)  \right|^{2}} \\
& \\
&  =\sqrt{\int_a^b \mathrm{d}x\ \left|f(x)  \right|^{2}  -2\operatorname{Re}\left(  \int_a^b \mathrm{d}x\ f^{\ast}(x)  \ g(x)\right)  +\int_a^b \mathrm{d}x\ \left|  g\left(x\right)  \right|^{2}}\,.
\end{align*}

El teorema del coseno puede ser escritos como:
\begin{align*}
\int_a^b \mathrm{d}x\ \left|  f(x) +g(x)  \right|^{2}  &  =
\int_a^b \mathrm{d}x\ \left|  f(x)  \right|^{2}+\int_a^b\mathrm{d}x\ \left|  g(x)  \right|^{2}\\
&  
+2\left(  \int_a^b \mathrm{d}x\ \left|  f\left(x\right)  \right|^{2}\right)  ^{\frac{1}{2}}\left(  \int_a^b\mathrm{d}x\ \left|  g(x)  \right|^{2}\right)
^{\frac{1}{2}}\cos(\Theta) \,,
\end{align*}
donde:
\[
\cos(\Theta)=\frac{\int_a^b \mathrm{d}x\ f^{\ast}\left( x\right)  \ g(x)  }{\left(  \int_a^b \mathrm{d}x\ \left|  f(x)  \right|^{2}\right)  ^{\frac{1}{2}
}\left(  \int_a^b \mathrm{d}x\ \left|  g\left( x\right)  \right|^{2}\right)  ^{\frac{1}{2}}} \,. 
\]

Y como era de esperarse el teorema de Pitágoras queda:
\[
\int_a^b \mathrm{d}x\ \left|  f(x) +g(x)  \right|^{2}=
\int_a^b \mathrm{d}x\ \left|  f(x)  \right|^{2}+\int_a^b \mathrm{d}x\ \left|  g(x)  \right|^{2} \,,
\]
para funciones $f(x)  $ y $g(x)  $ ortogonales, mientras que para este caso, la desigualdad de Cauchy-Schwarz se expresa:
\[
\left|  \int_a^b \mathrm{d}x\ f^{\ast}(x) \ g(x)  \right|^{2}\leq   \int_a^b  \mathrm{d}x\ \left|  f(x)  \right|^{2} \ \int_a^b \mathrm{d}x\ \left|  g(x)\right|^{2}\,.
\]
\end{enumerate}

\subsection{{\color{Fuchsia}Ejemplos}} 

\begin{enumerate}
\item Como vimos en la sección anterior, en el campo de los números complejos el valor absoluto de $z=z+iy$ es $|z|=\sqrt{x^2+y^2}$. La métrica que podemos asociar a este espacio vectorial viene dada por:
\[
d\left(  \left| z_1 \right> ,\left| z_2 \right>\right) \equiv \left\|  \left| z_1 \right> -\left|z_2\right> \right\|  =
\sqrt{\left<{z_1-z_2}\right|\left.  {z_1-z_2}\right> }= \sqrt{(x_1-x_2)^2+(y_1-y_2)^2} \,,
\]
con: $\left| z_1 \right>=x_1+iy_1$ y $\left| z_2\right>=x_2+iy_2$. 


\item  Consideramos el espacio vectorial de polinomios  de grado $g\leq n$ definidos en el intervalo $\left[ 0,1\right] $ o en el intervalo $\left[ -1,1\right]$ según el caso. Suponiendo las siguientes definiciones de producto interno en $ \mathcal{P}^{n}$:
\[
\left<{q}_{n}\right. \left|{p}_{n}\right> = \int_{-1}^{1}p(x)q(x)\mathrm{d}x \quad \text{y} \quad 
\left<{q}_{n}\right. \left|{p}_{n}\right> = \int_{0}^{1}p(x)q(x)\mathrm{d}x \,.
\]
Vamos a encontrar la distancia y el ángulo entre los vectores: $\left|{x}_1 \right> = x(x-1)$ y $\left|{x}_2\right> =x$.

En general, la definición de distancia es:
\[
d\left( \left|{x}_1\right>,\left|{x}_2\right> \right)=\sqrt{\left< {x}_2-{x}_1 \right. \left|{x}_2-{x}_1\right>}\,,
\]
por lo tanto para $\left<{q}_{n}\right. \left|{p}_{n}\right>  = \int_{-1}^{1}p(x)q(x)\mathrm{d}x$ la distancia será:
\[
\sqrt{\left< {x}_2-{x}_1\right. \left|{x}_2-{x}_1\right> } =
\sqrt{\int_{-1}^{1}\left[ x(x-1)-x\right]^{2}\mathrm{d}x} = \frac{1}{15}\sqrt{690} \,,
\]
y para $ \left<{q}_{n}\right. \left|{p}_{n}\right>  = \int_{0}^{1}p(x)q(x)\mathrm{d}x$, será:
\[
\sqrt{\left< {x}_2-{x}_1\right. \left|{x}_2-{x}_1\right> }=
\sqrt{\int_{0}^{1} \left( x(x-1)-x\right)^{2}\mathrm{d}x} = \frac{2}{15}\sqrt{30} \,.
\]

Con respecto a los ángulos:
\[
\theta=\arccos \left( \frac{\left< {x}_1\right. \left|{x}_2 \right> }{\sqrt{ \left< {x}_1\right. \left|{x}_1 \right> } \sqrt{\left< {x}_2\right. \left|x_{2}\right> }}\right)\,.
\]

Para $\left<{q}_{n}\right. \left|{p}_{n} \right>  = \int_{-1}^{1}p(x)q(x)\mathrm{d}x$ tenemos:

\begin{align*}
\theta & =\arccos\left( \frac{\left< {x}_1\right. \left| {x}_2 \right> }{ \sqrt{\left< {x}_1\right. \left| {x}_1 \right> }\sqrt{\left< {x}_2\right. \left| {x}_2 \right> }}\right) =
\arccos \left(\frac{\int_{-1}^{1}\left( x(x-1)\right) x \ \mathrm{d}x}{\sqrt{ \int_{-1}^{1}\left( x(x-1)\right)^{2}\mathrm{d}x}\sqrt{\int_{-1}^{1} x^{2}\mathrm{d}x}}\right) \\
& \\
& =\arccos\left( -\frac{1}{12}\sqrt{15}\sqrt{6}\right) =2.4825 \text{ rad} \,.
\end{align*}
Para $\left<{q}_{n}\right. \left|{p}_{n}\right> = \int_{0}^{1}p(x)q(x)\mathrm{d}x$
\begin{align*}
\theta & =\arccos\left( \frac{\left< {x}_1\right. \left| {x}_2\right> }{\sqrt{\left< {x}_1\right. \left| {x}_1 \right> }\sqrt{\left< {x}_2\right. \left| {x}_2 \right> }}\right) =
\arccos\left(\frac{\int_{0}^{1}\left( x(x-1)\right) \left( x\right) \mathrm{d}x}{\sqrt{\int_{0}^{1}\left( x(x-1)\right) ^{2}\mathrm{d}x}\sqrt{\int_{0}^{1} x^{2} \mathrm{d}x}}\right) \\
& \\
& =\arccos\left( -\frac{1}{12}\sqrt{15}\sqrt{2}\right) =2.4825 \text{ rad}  
\qquad \text{¡El mismo ángulo!}
\end{align*}

\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}} 
\index{Espacios vectoriales con Maxima}
\index{MaximaEspaciosVectoriales}

\subsubsection{Espacios y subespacios vectoriales}

Sea el espacio vectorial $\textbf{\em V}=\textbf{\em K}^n$, definido en 
$\textbf{\em K}=\mathds{R}$ y donde $n$ es un entero positivo. Consideremos el caso $n=4$.
El producto de un elemento de $\textbf{\em K}^4$, digamos $\left| x \right>=(x_1, x_2, x_3, x_4)$ por un escalar $\alpha \in \textbf{\em K}$ resulta en otro elemento de $\textbf{\em K}^4$. 

Primero introducimos los elementos como listas:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
X:[x1,x2,x3,x4];Y:[y1,y2,y3,y4];Z:[z1,z2,z3,z4];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\left[ { x_1} , { x_2} , { x_3} , { x_4} \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\left[ { y_1} , { y_2} , { y_3} , { y_4} \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ { z_1} , { z_2} , { z_3} , { z_4} \right] 
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
alpha*X=Y;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\left[ \alpha\,{ x_1} , \alpha\,{ x_2} , \alpha\,{ x_3} ,  \alpha\,{ x_4} \right] 
=\left[ { y_1} , { y_2} , { y_3} , { y_4} \right] 
\end{math}
\newline

El resultado es un elemento del espacio vectorial $\textbf{\em K}^4$.

La suma de $\left| x \right>=(x_1, x_2, x_3, x_4)$ y $\left| y \right>=(y_1, y_2, y_3, y_4)$ será:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
X+Y=Z;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ { y_1}+{ x_1} , { y_2}+{ x_2} , { y_3}+ { x_3} , { y_4}+{ x_4} \right] =\left[ { z_1} ,  { z_2} , { z_3} , { z_4} \right] 
\end{math}
\newline

con $(z_1 ,  z_2 ,  z_3,  z_4) \in \textbf{\em K}^4$. 

Podemos ver rápidamente que el conjunto  de vectores que tienen la forma  $(x_1,  x_2,  x_3,  0)$ conforman un subespacio de  $\textbf{\em K}^4$, ya que:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
map(":",[x4,y4,z4],[0,0,0]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\left[ 0 , 0 , 0 \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
X:[x1,x2,x3,x4];Y:[y1,y2,y3,y4];Z:[z1,z2,z3,z4];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\left[ { x_1} , { x_2} , { x_3} , 0 \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\left[ { y_1} , { y_2} , { y_3} , 0 \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\left[ { z_1} , { z_2} , { z_3} , 0 \right] 
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
alpha*X+beta*Y=Z;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\left[ \beta\,{ y_1}+\alpha\,{ x_1} , \beta\,{ y_2}+\alpha
 \,{ x_2} , \beta\,{ y_3}+\alpha\,{ x_3} , 0 \right] =
 \left[ { z_1} , { z_2} , { z_3} , 0 \right] 
\end{math}
\newline

Para recobrar las variables $x_4,y_4,z_4$ escribimos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(x4,y4,z4)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
X:[x1,x2,x3,x4];Y:[y1,y2,y3,y4];Z:[z1,z2,z3,z4];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\left[ { x_1} , { x_2} , { x_3} , { x_4} \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
\left[ { y_1} , { y_2} , { y_3} , { y_4} \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\left[ { z_1} , { z_2} , { z_3} , { z_4} \right] 
\end{math}
\newline

Para calcular el producto interno entre vectores es necesario utilizar la librería {\bf eigen}.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load("eigen")$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
innerproduct(X,Y);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
{x_4}\,{y_4}+{ x_3}\,{y_3}+{x_2}\,{y_2}+ {x_1}\,{y_1}
\end{math}
\newline

Consideremos ahora  $\textbf{\em V}=\textbf{\em K}^n$, definido en $\textbf{\em K}=\mathds{C}$, con $n=3$. Por lo tanto, los vectores serán ahora de la siguiente forma: $z=(x_1+iy_1, x_2+iy_2, x_3+iy_3)$. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Z1:[x1+%i*y1,x2+%i*y2,x3+%i*y3]; Z2:[u1+%i*v1,u2+%i*v2,u3+%i*v3];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
\left[ i\,{y_1}+{ x_1} , i\,{ y_2}+{ x_2} , i\,{ y_3}+{ x_3} \right]
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
\left[ i\,{ v_1}+{ u_1} , i\,{ v_2}+{ u_2} , i\, { v_3}+{ u_3} \right] 
\end{math}
\newline

Y los escalares de la forma $\alpha=a+ib$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
alpha:a+%i*b;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o19) }
i\,b+a
\end{math}
\newline

El producto por el escalar $\alpha$ es:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i20) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Z3:alpha*Z1;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o20) }
\left[ \left(i\,b+a\right)\,\left(i\,{ y_1}+{ x_1}\right) , 
 \left(i\,b+a\right)\,\left(i\,{ y_2}+{ x_2}\right) , \left(i\, b+a\right)\,\left(i\,{ y_3}+{ x_3}\right) \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i21) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
map(rectform,Z3);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o21) }
\left[ i\,\left(a\,{y_1}+b\,{x_1}\right)-b\,{y_1}+a\,
 {x_1} , i\,\left(a\,{y_2}+b\,{x_2}\right)-b\,{y_2}+a
 \,{x_2} , i\,\left(a\,{y_3}+b\,{x_3}\right)-b\,{y_3}+a\,{ x_3} \right]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i22) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
map(realpart,Z3),factor; map(imagpart,Z3),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o22) }
\left[ -\left(b\,{y_1}-a\,{x_1}\right) , -\left(b\,{y_2}-a\,{ x_2}\right) , -\left(b\,{y_3}-a\,{x_3} \right) \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o23) }
\left[ a\,{y_1}+b\,{x_1} , a\,{y_2}+b\,{x_2} , a\, {y_3}+b\,{x_3} \right] 
\end{math}
\newline

Calculemos ahora el producto interno: 
\[
Z_1 \cdot Z_2=(x_1+iy_1)^*(u_1+iv_1)+(x_2+iy_2)^*(u_2+iv_2)+(x_3+iy_3)^*(u_3+iv_3)\,.
\]


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i24) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Z4:innerproduct(Z1,Z2);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o24) }
\left({ v_3}-i\,{ u_3}\right)\,{ y_3}+\left({ v_2}-i\, {u_2}\right)\,{y_2}+\left({ v_1}-i\,{ u_1}\right)\, {y_1}+\left(i\,{ v_3}+{ u_3}\right)\,{x_3}+
\left(i\, {v_2}+{ u_2}\right)\,{ x_2}+\left(i\,{ v_1}+{ u_1} \right)\,{ x_1}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i25) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Re:map(realpart,Z4)$ Im:map(imagpart,Z4)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i26) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Re+%i*Im;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o26) }
i\,\left(-{ u_3}\,{ y_3}-{ u_2}\,{ y_2}-{ u_1}\, { y_1}+{ v_3}\,{ x_3}+{ v_2}\,{ x_2}+{ v_1}\, { x_1}\right)+{ v_3}\,{ y_3}+{ v_2}\,{ y_2}+{ v_1}
 \,{ y_1}+{ u_3}\,{ x_3}+{ u_2}\,{ x_2}+{ u_1}\,{ x_1}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i27) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}

\subsubsection{Producto de polinomios}

Consideremos el siguiente producto escalar entre elementos de un espacio vectorial de polinomios:
\[
\left<p_i\right. \left|p_j\right> = \int_{a}^{b}p_i(x)p_j(x)\mathrm{d}x \,,
\]

Vamos a encontrar la distancia y el ángulo entre los vectores $\left|{x}_1 \right> = x(x-1)$ y $\left|{x}_2\right> =x$ en dos intervalos diferentes: $\left[ 0,1\right] $ y $\left[ -1,1 \right]$

Debemos introducir los objetos a multiplicar:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
P1:x*(x-1); P2:x;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\left(x-1\right)\,x
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
x
\end{math}
\newline

Ahora calculamos las distancias entre los vectores para ambos intervalos. Haremos gala de algunas posibilidades que ofrece el programa para escribir las expresiones. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sqrt('integrate(((P1-P2)^2),x,-1,1))=sqrt(integrate(((P1-P2)^2),x,-1,1));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\sqrt{\int_{-1}^{1}{\left(\left(x-1\right)\,x-x\right)^2\;dx}}=
 \frac{\sqrt{46}}{\sqrt{15}}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
sqrt('integrate(((P1-P2)^2),x,0,1))=sqrt(integrate(((P1-P2)^2),x,0,1));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\sqrt{\int_{0}^{1}{\left(\left(x-1\right)\,x-x\right)^2\;dx}}=
 \frac{2^{\frac{3}{2}}}{\sqrt{15}}
\end{math}
\newline

El ángulo entre los polinomios definidos en el intervalo $\left[ -1,1 \right]$ es:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
'integrate((P1*P2),x,-1,1)/(sqrt('integrate((P1*P1),x,-1,1))*
sqrt('integrate((P2*P2),x,-1,1)));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\frac{\int_{-1}^{1}{\left(x-1\right)\,x^2\;dx}}{\sqrt{\int_{-1}^{1
 }{x^2\;dx}}\,\sqrt{\int_{-1}^{1}{\left(x-1\right)^2\,x^2\;dx}}}
 \end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ev(%,integrate);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
-\frac{\sqrt{15}}{2^{\frac{3}{2}}\,\sqrt{3}}
 \end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
acos(%),numer;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
2.482534617763384
 \end{math}
\newline

Y ahora, el ángulo entre los polinomios definidos en el intervalo $\left[ 0,1 \right]$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
'integrate((P1*P2),x,0,1)/(sqrt('integrate((P1*P1),x,0,1))*
sqrt('integrate((P2*P2),x,0,1)));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\frac{\int_{0}^{1}{\left(x-1\right)\,x^2\;dx}}{\sqrt{\int_{0}^{1}{x
 ^2\;dx}}\,\sqrt{\int_{0}^{1}{\left(x-1\right)^2\,x^2\;dx}}}
 \end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ev(%,integrate);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
-\frac{\sqrt{30}}{4\,\sqrt{3}}
 \end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
acos(%),numer;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
2.482534617763384
 \end{math}


\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}


\subsection{{\color{OliveGreen}Ejercicios}}

\begin{enumerate}
%%%%%%%%% Ejercicio 1
\item Consideremos el espacio vectorial conformado por los vectores geométricos en $\mathds{R}^{3}$ ¿Serán espacios euclidianos para las siguientes definiciones de producto interno?
\begin{enumerate}
\item El producto de las longitudes de los vectores. 
\item El producto de las longitudes por el cubo del coseno del ángulo entre ellos. 
\item El producto como dos veces el producto escalar usual entre vectores.
\end{enumerate}

%%%%%%%%% Ejercicio 2
\item  Considerando estas definiciones de producto interior en $\mathcal{P}_{n}$:
\[
a)\quad \left<{q}_{n}\right. \left|{p}_{n}\right> =\int_{-1}^{1}p(x)q(x)\mathrm{d}x\,, \qquad
b) \quad  \left<{q}_{n}\right. \left|{p}_{n}\right> =\int_{0}^{1}p(x)q(x)\mathrm{d}x \,.
\]
\begin{enumerate}
  \item Encuentre los ángulos en el ``triángulo'' formado por los vectores: $\left| x_1 \right>=1, \left| x_2 \right>=t, \left| x_3 \right>=1-t$.
  \item Encuentre la distancia y el ángulo entre los siguientes pares de vectores en $\mathcal{P}_{3}$:
\begin{enumerate}
\item $\left| {x}_1\right> = 1; \quad \left|{x}_2\right> = x$.
\item $\left| {x}_1 \right>  = 2x;\quad \left|{x}_2\right> = x^{2}$.
\end{enumerate}
\end{enumerate}


%%%%%%%%% Ejercicio 3
\item Sea $\textbf{\em E}^\prime$ un subespacio euclidiano de dimensión $k$, $\textbf{\em E}^\prime  \subset \textbf{\em E}$,  y sea  
$\left| v \right>$ un vector que no necesariamente es un elemento 
$\textbf{\em E}^\prime$. Podemos plantearnos el problema de representar $\left| v \right>$ de la forma:
$\left| v \right>= \left| g \right>+\left| h \right> $;  donde $\left| g \right> \in \textbf{\em E}^\prime$ y $\left| h \right>$ es ortogonal a $\textbf{\em E}^\prime$. La existencia de la expansión anterior nos muestra que el espacio total $\textbf{\em E}$, de dimensión $n$, es la suma directa de los subespacios $\textbf{\em E}^\prime$ y su complemento ortogonal $\textbf{\em E}^\perp$ de dimensión $n-k$. 

Encuentre el vector $\left| v \right>$, como la suma del vector $\left| g \right>$, expandido por los vectores $\left| g_i \right>$, y el vector perpendicular $\left| h\right>$ cuando:
\begin{enumerate}
\item $\left| h \right>=(5,2,-2,2)\,,\, \left| g_1 \right>=(2,1,1,-\alpha)\,,\,
\left| g_2 \right>=(1,\beta,3,0)$.
\item $\left| h \right>=(-3,5,9,3)\,,\, \left| g_1 \right>=(1,1,1,\gamma)\,,\,
\left| g_2 \right>=(2\eta,-1,1,1)\,,\, \left| g_3 \right>=(2,-7\delta,-1,-1)$.
\end{enumerate}


%
%%%%%%%%%
\item  Sean $\left| p_{n} \right> = p(x) = \sum_{i=0}^{n-1}a_{i}x^{i}\,; \quad \left| {q}_{n} \right> =q(x)=\sum_{i=0}^{n-1}
b_{i}x^{i} \in \mathcal{P}_{n}$. Considérese la siguiente definición:
\[
\left< {q}_{n} \right. \left|{p}_{n}\right> \rightleftharpoons a_{0}b_{0}
+a_{1}b_{1}+a_{2}b_{2}+...+a_{n-1}b_{n-1}=\sum_{i=0}^{n-1}a_{i}b_{i}
\]

\begin{enumerate}
\item  Muestre que ésta es una buena definición de producto interno.

\item  Con esta definición de producto interior ?`Se puede considerar $\mathcal{P}_{n}$ un subespacio de $\mathcal{C}_{[a,b]}$? ?`Por qué?
\end{enumerate}



%
%%%%%%%%% Ejercicio 4
\index{Cuaternión}
\item Los vectores en $\mathds{R}^{3}$ en coordenadas cartesianas los definimos como
${\bf a}= a^i \left|{e}_{i}\right \rangle = a_{x}{\bf {i}}+a_{y}{\bf {j}}+a_{z}{\bf {k}}$ y definimos una ``tabla de multiplicación'' entre ellos
de la forma $\left<{e}^{i}\right.  \left| {e}_{j}\right> =
\delta_{j}^{i}$ con $i,j=1,2,3$, esto es:
\[
\begin{tabular}
[c]{|c|c|c|c|}
\hline
$\left<{e}^{i}\right.  \left|  {e}_{j}\right> $ &${\bf {i}}$ & ${\bf {j}}$ & ${\bf {k}}$\\\hline\hline
${\bf {i}}$ & $1$ & $0$ & $0$\\\hline
${\bf {j}}$ & $0$ & $1$ & $0$\\\hline
${\bf {k}}$ & $0$ & $0$ & $1$\\\hline
\end{tabular}
\]
Un cuaternión cartesiano puede escribirse de manera análoga a los
vectores cartesianos, vale decir:
\[
\left|  {a}\right> =a^{\alpha}\left|  {q}_{\alpha}\right> =a^{0}+a^{i}\left|  {q}_{i}\right> =a_{0}
+a_{x}{\bf {i}}+a_{y}{\bf {j}}+a_{z}{\bf {k}} \,,
\]
con $\alpha=0,1,2,3$ y donde las $a^{i}$ (con $i=1,2,3$) son números reales que representan las componentes vectoriales en coordenadas cartesianas de los cuaterniones,  mientras que la $a^{0},$ también un número real se le llama componente escalar\footnote{Recuerde que estamos utilizando la convención de Einstein en la cual: $c^{\alpha}\left|  {q}_{\alpha}\right> \equiv c^{0}+\sum_{j=1}^{3}c^{j}\left|  {q}_{j}\right> $. Es decir, hemos supuesto que: $\left|  {q}_{0}\right> \equiv1$, la unidad en los números reales. Adicionalmente, nótese que los índices griegos $\alpha,\beta,\cdots$ toman los valores $0,1,2,3$, mientras que los latinos que acompañan a los vectores cartesianos toman los siguiente valores $j,k,l=1,2,3$.}. 

Los cuaterniones fueron inventados por el matemático irlandés William Rowan Hamilton  a mediados del siglo XIX, y por decirlo de alguna manera, son híbridos o generalizaciones a un plano hipercomplejo. Un vector cartesiano es un cuaternión con la componente escalar nula. Hoy encontramos aplicaciones del álgebra de cuaterniones en Física\footnote{Hace algunas décadas se dio una discusión sobre la importancia de utilizar esta representación en Física Cuántica. Pueden consultar:
\begin{itemize}
  \item Berezin, A. V., Kurochkin, Y. A., \& Tolkachev, E. A. (1989). Quaternions in relativistic physics. Nauka i Tekhnika, Minsk.
  \item Girard, P. R. (1984). The quaternion group and modern physics. European Journal of Physics, 5(1), 25.
  \item Horwitz, L. P., \& Biedenharn, L. C. (1984). Quaternion quantum mechanics: second quantization and gauge fields. Annals of Physics, 157(2), 432-488.
\end{itemize}}
 y más recientemente ha tenido alguna utilización computación gráfica que discutiremos en el próximo problema en el contexto del álgebra geométrica y las algebras de Grassman.

Basándonos en este esquema podemos definir la ``tabla de multiplicación'' para los cuaterniones cartesianos como:
\[
\begin{tabular}
[c]{|c||r|r|r|r|}\hline
$\left|  {q}_{i}\right> \odot\left|  {q}_{j}\right> $ & ${1}$ & $\left|  {q}_{1}\right> $ &$\left|  {q}_{2}\right> $ & $\left|  {q}_{3}\right>$\\\hline\hline
${1}$ & ${1}$ & $\left|  {q}_{1}\right> $ &$\left|  {q}_{2}\right> $ & $\left|  {q}_{3}\right>
$\\\hline
$\left|  {q}_{1}\right> $ & $\left|  {q}_{1}\right> $ & $-{1}$ & $\left|  {q}_{3}\right> $ &$-\left|  {q}_{2}\right> $\\\hline
$\left|  {q}_{2}\right> $ & $\left|  {q}_{2}\right> $ & $-\left|  {q}_{3}\right> $ & $-{1}$& $\left|  {q}_{1}\right> $\\\hline
$\left|  {q}_{3}\right> $ & $\left|  {q}_{3}\right> $ & $\left|  {q}_{2}\right> $ & $-\left|{q}_{1}\right> $ & $-{1}$\\\hline
\end{tabular}
\]

Nótese que por el hecho de que: 
\[\left|  {q}_{j}\right>\odot \left|  {q}_{j}\right> =-1\Rightarrow\left|  {q}_{1}\right> \odot\left|  {q}_{1}\right> =\left|{q}_{2}\right> \odot\left|  {q}_{2}\right> = \left|{q}_{3}\right> \odot\left|  {q}_{3}\right> =-1\,,
\]
se puede pensar que un cuaternión es la generalización de los números complejos a más de una dimensión (un número hipercomplejo), donde la parte imaginaria tendría tres dimensiones y no una como es costumbre. 

Esto es:
\[
\left|  {a}\right> = 
a^{\alpha}\left|  {q}_{\alpha}\right> =a^{0}\underset{{1}}{\underbrace{\left|  {q}_{0}\right> }}+a^{j}\left|  {q}_{j}\right> =
a^{0}+\underset{\text{``parte compleja''}}{\underbrace{a^{1}\left|  {q}_{1}\right> +a^{2}\left|  {q}_{2}\right> +a^{3}\left| {q}_{3}\right> }} \,.
\]

Siendo consistente con esa visión de generalización de un número complejo, definiremos el conjugado de un cuaternión como: 
\[
\left| {b}\right> ^{\maltese}=b^{0}\left|  {q}_{0}\right> -b^{j}\left|  {q}_{j}\right> \,,
\]
con $\ j=1,2,3$. 

Es decir, en analogía con los números complejos el conjugado de un cuaternión cambia el signo de su ``parte compleja vectorial''. 

Igualmente, definiremos la suma entre cuaterniones de la siguiente manera:
\[
\left.
\begin{array}[c]{c}
\left|  {a}\right> =a^{\alpha}\left|  {q}_{\alpha }\right> \\
\\
\left|  {b}\right> =b^{\alpha}\left|  {q}_{\alpha}\right>
\end{array}
\right\} \,\, \Rightarrow \,\, \left|  {c}\right> =
c^{\alpha}\left|{q}_{\alpha}\right> = 
\left|  {a}\right> +\left|{b}\right> =
\left(  a^{\alpha}+b^{\alpha}\right)  \left| {q}_{\alpha}\right> \,\, \Rightarrow \,\,  c^{\alpha}=
\left(a^{\alpha}+b^{\alpha}\right) \,.
\]

Esto quiere decir que los vectores se suman componente a componente. Mientras que la multiplicación por un escalar queda definida por $\alpha\left| {c}\right> =\alpha c^{\alpha}\left|  {q}_{\alpha }\right>$, es decir se multiplica el escalar por cada componente.

Con la información anterior, responda las siguientes preguntas:

\begin{enumerate}
\item  Compruebe si los cuaterniones, $\left|  {a}\right>$,  forman un espacio vectorial respecto a esa operación  suma y esa multiplicación por escalares, análoga a la de los vectores en $\mathds{R}^{3}$ en coordenada cartesianas.

\item  Dados dos cuaterniones cualesquiera $\left|  {b}\right> \equiv\left( b^{0},{\bf b}\right) $ y $\left|  {r}\right> \equiv\left( r^{0}, {\bf r}\right) $, y su tabla de multiplicación, muestre que el producto entre esos cuaterniones $\left| {d}\right> =\left|  {b}\right> \odot \left| {r}\right> $ podrá representarse como:
\[
\left|  {d}\right> =\left|  {b}\right> \odot \left| {r}\right> \longleftrightarrow \left(  d^{0}, {\bf d}\right)=
\left(  b^{0}r^{0} -{\bf b} \cdot {\bf r},\ r^{0} {\bf b} +b^{0}{\bf r} + {\bf b} \times {\bf r}\right) \,,
\]
donde $\cdot$ y $\times$ corresponden con los productos escalares y vectoriales tridimensionales de siempre.

\item Ahora con índices: dados $\left|  {b}\right> = b^{\alpha}\left|  {q}_{\alpha}\right> $ y $\left| {r}\right> =r^{\alpha}\left|  {q}_{\alpha}\right>$,  compruebe si el producto $\left|  {d}\right> =\left| {b}\right> \odot\left|  {r}\right> $ puede ser siempre escrito de la forma:
\[
\left|  {d}\right> = \left|  {b}\right> \odot \left|{r}\right> = a\left|  {q}_{0}\right> + {S}^{(\alpha j)}\delta_{\alpha}^{0}\left|  {q}_{j}\right> +A^{\left[  jk\right] i}b_{j}r_{k}\left|  {q}_{i}\right>\,.
\]  
donde $a$ representa un número, $S^{\left(  \alpha j\right)  }\delta_{\alpha}^{0}$ (recuerde que los índices latinos toman los  valores $j,k,l=1,2,3$, mientras $\alpha = 0,1,2,3$),  donde $S^{\left(  ij\right)  }$ indica $S^{ji}=S^{ij}$, que la cantidad $S^{ij}$ es simétrica, y por lo tanto $\left(  S^{\alpha j}\delta_{\alpha}^{0}+S^{j\alpha}\delta_{\alpha}^{0}\right)  \left|  {q}_{j}\right>$.  

Mientras $A^{\left[  jk\right]  i}$ representa un conjunto de objetos antisimétricos en $j$ y $k$:\footnote{Para familiarizarse con las expresiones vectoriales con la notación de índices puede consultar la sección \ref{AlgebraVectorialIndices}.}
\[
A^{\left[  jk\right]  i} \rightarrow A^{jki}=-A^{kji} \rightarrow
\left(  A^{jki}b_{j}r_{k}-A^{kji}b_{j}r_{k}\right) \left|  {q}_{i}\right> \,.
\]


\item Identifique las cantidades: $a$, $S^{\left(ij\right)}$ y $A^{\left[  jk\right]  i}$,  
 en términos  de las componentes de los cuaterniones.
 
 \textquestiondown El producto de cuaterniones $\left|  {d}\right> =\left|  {a}\right>\odot\left|  {r}\right>$ será un vector, pseudovector o ninguna de las anteriores? Explique por qué.

\item  Muestre que los cuaterniones pueden ser representados por matrices complejas $2 \times 2$ del tipo:
\[
\left|  {b}\right> \longleftrightarrow
\left(
\begin{array}
[c]{cc}
z & w\\
-{w}^* & {z}^*
\end{array}
\right)\,,
\]
donde $z,w$ son números complejos.

\item  Muestre que una representación posible para la base de cuaterniones es, la matriz unitaria $4x4$:
\[
\left|  {q}_{1}\right> =\left(
\begin{array}
[c]{cccc}
0 & 1 & 0 & 0\\
-1 & 0 & 0 & 0\\
0 & 0 & 0 & 1\\
0 & 0 & -1 & 0
\end{array}
\right)\,,\quad \left|  {q}_{2}\right> =\left(
\begin{array}
[c]{cccc}
0 & 0 & 0 & -1\\
0 & 0 & -1 & 0\\
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0
\end{array}
\right)\,,\quad \left|  {q}_{3}\right> =\left(
\begin{array}
[c]{cccc}
0 & 0 & -1 & 0\\
0 & 0 & 0 & 1\\
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0
\end{array}
\right) \,.
\]

\item  Compruebe si la siguiente es una buena definición de producto interno:
\[
\widetilde{\left<{a}\right.  \left|  {b}\right> }=\left|
{a}\right>^{\maltese}\odot\left|  {b}\right> \,.
\]

\item  Modifique un poco la definición anterior de tal forma que:
\[
\left<{a}\right.  \left|  {b}\right>  =\frac{1}{2}\left[
\widetilde{\left<{a}\right.  \left|  {b}\right> } -\left|{q}_{1}\right> \odot \widetilde{\left<{a}\right.  \left|  {b}\right> } \odot\left|  {q}_{1}\right> \right] \,,
\]
y compruebe si esta definición compleja del producto interno cumple con todas las propiedades. Nótese que un cuaternión de la forma $\left|{f}\right> =f^{0}+f^{1}\left| {q}_{1}\right>$ es un número complejo convencional.

\item  Compruebe si la siguiente es una buena definición de norma para los cuaterniones:
\[
n(\left|  {b}\right> )=\left\|  \left|  {a}\right>\right\|  =\sqrt{\left<{a}\right.  \left|  {a}\right> }=\sqrt{\left|  {a}\right> ^{\maltese}\odot\left|{a}\right> } \,.
\]

\item  Compruebe si un cuaternión definido por:
\[
\overline{\left|  {a}\right> }=\frac{\left|  {a}\right>^{\maltese}}{\left\|  \left|  {a}\right> \right\|^{2}}\,,
\]
puede ser considerado como el inverso o elemento simétrico de $\left|{a}\right>$, respecto a la multiplicación $\odot$.

\item  Compruebe si los cuaterniones $\left|  {a}\right> $ forman un grupo respecto a una operación multiplicación $\odot$.

\item  Los vectores en $\mathds{R}^{3}$ en coordenadas cartesianas, $\left|{v}\right>$, pueden ser representados como cuaterniones, donde la parte escalar es nula $v^{0}=0\rightarrow \left| {v}\right>=v^{j}\left|  {q}_{j}\right>$. Compruebe si el siguiente producto conserva la norma:
\[
\left|  {v}^{\prime}\right> =\overline{\left|  {a} \right> }\odot\left|  {v}\right> \odot\left|{a}\right> \,. 
\]
Estos es: 
$ 
\left\|  \left| {v}^{\prime}\right> \right\|^{2}=\left(  v^{1^\prime}\right)^{2}+\left(  v^{2^\prime}\right) ^{2}+\left(v^{3^\prime}\right) ^{2}\equiv\left(  v^{1}\right)^{2}+\left(  v^{2}\right)
^{2}+\left(  v^{3}\right)^{2}=\left\|  \left|  {v}\right>\right\|  ^{2}\,.
$ 
\end{enumerate}
%\newpage

\item En el mismo espíritu de los cuaterniones considerados previamente estudiemos el siguiente problema.

%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{minipage}{7.5cm}

Consideremos otra vez el espacio $\mathds{R}^{3}$ expandido por la base ortonormal estándar $\left\{  \bf{i}, \bf{j}, \bf{k} \right\}$.

Supongamos en este espacio el producto de dos vectores $\bf{a}$ y $\bf{b}$ (o $\left|a\right>$ y $\left|b\right>$ en la notación de vectores abstractos de Dirac) definido a la manera del \textit{álgebra geométrica}.  

Esto es:
\[
\left|a\right> \bigodot \left|b\right> \equiv \bf{a}\bf{b} = \bf{a}\cdot\bf{b} + \bf{a}\wedge\bf{b} \,,
\]
con  $\bf{a}\cdot\bf{b}$ el producto escalar estándar de $\mathds{R}^{3}$, representando la parte conmutativa de $\bf{a}\bf{b}$ y $\bf{a} \wedge \bf{b}$ su parte anticonmutativa. Esta última parte se relaciona con el producto vectorial estándar de la representación de Gibbs como: ${\bf a}\wedge{\bf b} = i \bf{a}\times\bf{b}$, con $ i = {\bf i} \wedge {\bf j} \cdot {\bf k}$ un pseudoescalar.


\end{minipage} \hfill 
\begin{minipage}{8.0cm} 
\includegraphics[height=2.3in,width=3.3in]{VOLUMEN_1/02_Espacios_Lineales/Figuras/FiguraCasitaPropuestos.jpg}
\caption{Se ilustran los vectores ortonormales estándares $\left\{ {\bf i}, {\bf j} \right\}$ y como siempre ${\bf k} = {\bf i} \times {\bf j} $.}
\label{FiguraAlgebraGeometrica}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%
Este formalismo ha tenido cierto impacto en computación gráfica\footnote{Esta representación para objetos físicos ha tenido cierto éxito en Física clásica y cuántica; se puede consultar:
\begin{itemize}
  \item Hestenes, D., \& Sobczyk, G. (2012). Clifford algebra to geometric calculus: a unified language for mathematics and physics (Vol. 5). Springer Science \& Business Media.
  \item Hestenes, D. (1971). Vectors, spinors, and complex numbers in classical and quantum physics. Am. J. Phys, 39(9), 1013-1027.
  \item Hestenes, D. (2003). Spacetime physics with geometric algebra. American Journal of Physics, 71(7), 691-714.  
  \item Dressel, J., Bliokh, K. Y., \& Nori, F. (2015). Spacetime algebra as a powerful tool for electromagnetism. Physics Reports, 589, 1-71.
\end{itemize}}-\footnote{Y equivalentemente, por isomorfismo $i=\sqrt{-1}$.}-\footnote{Pueden consultar 
\begin{itemize}
  \item  Goldman, R. (2002). On the algebraic and geometric foundations of computer graphics. ACM Transactions on Graphics (TOG), 21(1), 52-86.
  \item Hildenbrand (2011). From Grassmann's vision to geometric algebra computing. Springer Basel.
  \item Hildenbrand, D., Fontijne, D., Perwass, C., \& Dorst, L. (2004). Geometric algebra and its application to computer graphics. In Tutorial notes of the EUROGRAPHICS conference.
  \item Vince, J. (2008). Geometric algebra for computer graphics. Springer Science \& Business Media
\end{itemize} y las referencias allí citadas.} y lo vamos a utilizar para modelar transformaciones de objetos en el espacio. 

Considere el caso 2D representado en la figura \ref{FiguraAlgebraGeometrica},  en el formalismo de algebra geométrica el objeto triángulo lo asociamos con vector abstracto $\left| \bigtriangleup \right>$, mientras que el objeto cuadrado lo representaremos como $\left| \Box \right>$, y el tercer objeto por $\left| \Uparrow \right>$. 

Entonces:

\begin{enumerate}
  \item Exprese los vectores: $\left| \bigtriangleup \right>, \left| \Box \right>, \left| \Uparrow \right>$ en términos de la base ortonormal estándar $\left\{  {\bf i}, {\bf j}, {\bf k} \right\}$ (o $\left|{ \rm i}\right>, \left|{ \rm j}\right>, \left|{ \rm k}\right> $) en el formalismo de álgebra geométrica.
  
  \item Exprese $\left| \bigtriangleup \right> \bigodot \left| \Uparrow \right>$ en término de la base geométrica. Vale decir, si $\left|O_{g}\right> $ es un objeto geométrico este podrá  representarse en términos de una base: $\left| \epsilon \right>, \left| {\sigma} \right>, \left| B \right>, \left| i \right> $, donde $\left| \epsilon \right>$ es un escalar; $\left| {\sigma} \right>$ es un vector, $\left| B \right>$ un bivector y, finalmente $\left| i \right>$ un pseudoescalar.
  
  \item  Encuentre la norma de cada uno de ellos y la distancia entre  $\left| \bigtriangleup \right>$ y $\left| \Uparrow \right>$.
  
\item Considere ahora la operación: $\mathbb{A}\left| \bigtriangleup \right> = \left| \tilde{\bigtriangleup} \right>$.

\begin{enumerate}
  \item Si $\mathbb{A}_{\left| {\rm j} \right>}$ es el operador reflexión respecto $\left| {\rm j} \right>$, exprese en términos geométricos $\left| \tilde{\bigtriangleup} \right> = \mathbb{A}_{\left| {\rm j} \right>}\left| \bigtriangleup \right>$ y $\mathbb{A}_{\left| {\rm i} \right>}\left(\left| \bigtriangleup \right> \bigodot \left| \Uparrow \right>\right) $.
  
  \item Si $\mathbb{A}_{\theta, \left| B \right>}$ es el operador de rotación alrededor de un bivector $\left| B \right>$ un ángulo $\theta$. Encuentre $\mathbb{A}_{\pi/4, \left| B \right>}\left(\left| \bigtriangleup \right> \bigodot \left| \Uparrow \right>\right) $ con $\left| B \right> = \left|{\rm j}\right> + \left|{\rm k}\right>$.
\item ¿Cómo interpreta Ud. la ecuación de autovalores $\mathbb{A}\left| \bigtriangleup \right> = 2 \left| \bigtriangleup \right>$?
\end{enumerate}  

 \item Considere el caso 3D en el cual  $\left| \hat{\bigtriangleup} \right>$ representa un tetraedro regular con la base representada por la figura \ref{FiguraAlgebraGeometrica} y $\left| \hat{\Box }\right>,$ un cubo, también con su base representada en el plano de la misma figura. 
  \begin{enumerate}
%%%  
  \item Exprese los vectores: $\left| \hat{\bigtriangleup} \right>, \left| \hat{\Box} \right>$ en términos de la base ortonormal estándar $\left\{ {\bf i}, {\bf j}, {\bf k} \right\}$  y el formalismo de álgebra geométrica.
%%%
  \item  Exprese $\left(\left| \hat{\bigtriangleup} \right> \bigodot \left| \Uparrow \right>\right) \bigodot \left| \bigtriangleup \right> $ en término de la base geométrica.
%%%
  \item Encuentre la norma de $\left| \hat{\bigtriangleup} \right>$ y la distancia entre  $\left| \hat{\bigtriangleup} \right>$ y $\mathbb{A}_{\pi/2, \left| -{\rm j} \right>}\left| \hat{\Box} \right>$.
%%
  \item Exprese $\left[\mathbb{A}_{\pi/2, \left| -{\rm j} \right>} ,\mathbb{B}_{\pi/4, \left| {\rm i} \right>} \right]\left| \hat{\Box} \right>$, con  $\left[\mathbb{A}_{\pi/2, \left| -{\rm j} \right>} ,\mathbb{B}_{\pi/4, \left| {\rm i} \right>} \right]= \mathbb{A}_{\pi/2, \left| -{\rm j} \right>} \mathbb{B}_{\pi/4, \left| {\rm i} \right>} -  \mathbb{B}_{\pi/4, \left| {\rm i} \right>} \mathbb{A}_{\pi/2, \left| -{\rm j} \right>}$.
\end{enumerate}
\end{enumerate}



%%%%%%%%%%%%%%%%%%
\item En Geometría Diferencial podemos considerar a $\mathds{R}^{3}$ como un conjunto de puntos (una variedad o espacio topológico), es decir, $\mathds{R}^{3}$ no es un espacio vectorial como se definió con anterioridad. 
En un punto $q$ cualquiera podemos generar un plano $\mathds{R}^{2}$, entendido también como un conjunto de puntos, y definir el siguiente conjunto $T_q \mathds{R}^{2}=$ \{el conjunto de todos los vectores geométricos (flechas) con origen en  $q$  que son tangentes al plano $\mathds{R}^{2}$\}. Notemos que la idea la podemos extender para cualquier superficie, por ejemplo, una esfera $\mathds{S}^{2}$ (embebida en $\mathds{R}^{3}$) y sobre la esfera  seleccionar un punto arbitrario $q$. A partir de este punto $q$ generar un plano y construir el conjunto $T_q \mathds{S}^{2}=$ \{el conjunto de todos los vectores geométricos con origen en  $q$  que son tangentes a la esfera $\mathds{S}^{2}$\}.

\begin{enumerate}
\item ¿Es $T_q \mathds{R}^{2}$ un espacio vectorial?
\item Claramente podemos seleccionar otro punto arbitrario, pero diferente de $q$, digamos $p$ y construir el conjunto $T_p \mathds{R}^{2}$. ¿Son estos dos espacios isomorfos? 
\end{enumerate}

Consideremos ahora el conjunto de todas las funciones diferenciables
$f: \mathds{R}^{3} \,\,\rightarrow\,\, \mathds{R}$. Esto es, que existen todas las derivadas parciales en $q \in \mathds{R}^{3}$:
\[
\partial_x f(x^i)\,,\, \partial_y f(x^i)\,,\, \partial_z f(x^i)\,\, \in \,\,\mathds{R}
\,\, \forall\,\, (x^i) \,\, \in\,\, \mathds{R}^{3}\,.
\]
Note que hemos supuesto un sistema de coordenadas cartesiano $x^i=(x,y,z)$ en $\mathds{R}^{3}$ y que las derivadas son evaluadas en $q$. 

Consideremos el espacio tangente $T_q \mathds{R}^{3}$ en un punto $q$ arbitrario y  un vector $\left| u_q\right> \in T_q \mathds{R}^{3}$ con componentes $\left(u^1,u^2,u^3\right)$, podemos definir el siguiente operador (operador derivada direccional) en $q$ siguiendo a 
$\left| u_q\right>$:
\[ 
\left| U_q\right>= \left(u^1\partial_x+u^2\partial_y+u^3\partial_z\right)_q \,.
\]
Por lo tanto:
\[
\left| U_q\right> f \equiv \left(u^1\partial_x+u^2\partial_y+u^3\partial_z\right)_q f = u^1(\partial_xf)_{q} +u^2(\partial_yf)_{q} +u^3(\partial_zf)_{q} \,.
\]
Es  posible entonces construir el conjunto de los operadores derivadas direccionales que actúan sobre las funciones $f$:
\[
\mathcal{D}_q(\mathds{R}^{3})= \{ \left| U_q\right> =  \left(u^1\partial_x+u^2\partial_y+u^3\partial_z\right)_q \,\, \forall\,\, 
\left| u_q\right> \,\, \in \,\,T_q \mathds{R}^{3} \} \,.
\]
\begin{description}
\item[]
c) ¿Es $\mathcal{D}_q$ un espacio vectorial? 
\item[]
d) ¿Los espacios  $\mathcal{D}_q$  y $T_q \mathds{R}^{3}$ son isomorfos? 
\end{description}

\item Resuelva los problemas anteriores utilizando {\bf Maxima}. 

\end{enumerate}


\section{Variedades lineales}
\label{VariedadesLineales}
\index{Variedades lineales}

Resulta de gran interés construir subespacios vectoriales a partir de una cantidad de vectores $\{ \left| \mathrm{w}_{i}\right>\}$. En este caso diremos que los vectores generan una variedad lineal. Es decir, si tenemos una cantidad de vectores: $\{\left| \mathrm{w}_{1}\right>, \left| \mathrm{w}_{2}\right>, \left| \mathrm{w}_{3}\right>, \dots \}$ $\in \textbf{\em V}$, entonces una variedad lineal generada por este conjunto se entenderá como el conjunto de todas las combinaciones lineales finitas:
\[
\alpha\left| \mathrm{w}_{1}\right>+\beta\left| \mathrm{w}_{2}\right>+\gamma\left| \mathrm{w}_{3}\right> +\cdots \,,
\]
donde los coeficientes: $\alpha, \beta, \gamma, \dots$ pertenecen al campo $\textbf{\em K}$. 

Se puede comprobar que esta variedad lineal es un subespacio de $\textbf{\em V}$. Claramente, todo subespacio que contenga los vectores $\{\left| \mathrm{w}_{1}\right>, \left| \mathrm{w}_{2}\right>, \left| \mathrm{w}_{3}\right>, \dots \}$ también contiene todas sus combinaciones lineales, por lo tanto, la variedad lineal generada de esta manera es el subespacio más pequeño que contiene al conjunto de estos vectores. 

Una variedad lineal sencilla de construir  en $\textbf{\em V}=\mathds{R}^{3}$ la constituye un par de vectores no colineales. La variedad lineal generada por estos dos vectores será el conjunto de todos los vectores paralelos al plano determinado por este par de vectores no colineales. Mientras que la variedad lineal generada por los vectores: $\{{\bf i}, {\bf j},{\bf k}\} \in \mathds{R}^{3}$  es el mismo espacio entero $\mathds{R}^{3}$.

Pasemos ahora a considerar el problema de construir una base para una variedad lineal y a partir de allí determinar la dimensión de la variedad.


\subsection{Dependencia/Independencia lineal}
\label{Dependencia lineal}
\index{Dependencia lineal}
\index{Independencia Lineal}

Siguiendo la misma línea de razonamiento que en las secciones \ref{IndependenciaVector3D} y \ref{IndependenciaVector3D2}, generalizamos el concepto de dependencia e independencia lineal de 
$\mathds{R}^{2}$ y $\mathds{R}^{3}$. Así:
\[
\left|  {0}\right> =C_{1}\ \left|  v_{1}\right> +C_{2}\ \left|  v_{2}\right> +C_{3}\ \left|  v_{3}\right> \cdots+C_{n}\ \left|  v_{n}\right>
=\sum_{i=1}^{n}C_{i}\ \left|  v_{i}\right> ,
\]
Las cantidades $C_{i}$ son llamados los coeficientes de la combinación lineal. 

Podemos afirmar que:
\begin{itemize}
\item  Si esta ecuación se cumple para algún conjunto de $\left\{
C_{i}\right\}  $ no nulos, se dirá que el conjunto de vectores
correspondiente $\left\{  \left|  v_{i}\right> \right\}  $ son
\textbf{linealmente dependientes.}

\item  Por el contrario, si esta ecuación \textbf{sólo} puede ser
satisfecha para todos los $C_{i}=0$,  entonces se dirá que el conjunto de
vectores correspondiente $\left\{\left| v_{i}\right>
\right\}  $ son \textbf{linealmente independientes.}
\end{itemize}
Notemos que la suma que aparece aquí es necesariamente una suma finita, y cuando un determinado conjunto de vectores es linealmente dependiente, entonces uno de ellos se puede escribir como combinación lineal de los demás.

\subsection{Bases de un espacio vectorial}
\label{BasesEspaciosVectoriales}
\index{Bases para Espacios vectoriales lineales}
\index{Espacios vectoriales lineales!Bases de}
Ahora bien, dado un espacio vectorial $\textbf{\em V}=\left\{  \left| v_{1}\right> ,\ \left|  v_{2}\right> ,\ \left| v_{3}\right> \cdots,\left|  v_{n}\right> \right\}  $, si encontramos que el conjunto de $\left\{  \left|  v_{n}\right> \right\}$ es linealmente dependiente, entonces siempre es posible despejar uno de los vectores en términos de los demás, vale decir:
\[
\left|  v_{n}\right> =\bar{C}_{1}\ \left|  v_{1}\right> +\bar{C}_{2}\ \left|  v_{2}\right> +\bar{C}_{3}\ \left|  v_{3}\right> \cdots+\bar{C}_{n-1}\ \left| v_{n-1}\right> =\sum_{i=1}^{n-1}\bar{C}_{i}\ \left| v_{i}\right> \,.
\]

Seguidamente podemos proceder a comprobar si $\left\{  \left|  v_{1}\right> ,\ \left|  v_{2}\right> ,\ \left| v_{3}\right> \cdots,\left|  v_{n-1}\right> \right\}$ es un conjunto de vectores linealmente independientes, es decir: $\bar{C}_{1}\ =\bar{C}_{2}\ =\bar{C}_{3}\ =\cdots=\bar{C}_{n-1} =0$. 

En caso de no serlo se procede otra vez a despejar uno de los vectores en términos de los anteriores y aplicar el criterio de independencia lineal:
\begin{align*}
\left|  v_{n-1}\right>  &  =\tilde{C}_{1}\ \left| v_{1}\right> +\tilde{C}_{2}\ \left|  v_{2}
\right> +\tilde{C}_{3}\ \left|  v_{3}\right> \cdots+\tilde{C}_{n-2}\ \left|  v_{n-2}\right> =\sum_{i=1}^{n-2}\tilde{C}_{i}\ \left|  v_{i}\right> \,,
\end{align*}
nuevamente se comprueba si se cumple:  $\tilde{C}_{1}  =\tilde{C}_{2}\ =\tilde{C}_{3}\ =\cdots=\tilde{C}_{n-2}=0$. 

En caso contrario, se repite este procedimiento hasta encontrar un conjunto: 
$\left\{  \left|{v}_{1}\right> ,\ \left|  v_{2}\right> ,\ \left| v_{3}\right> \cdots,\left|  v_{n-j}\right> \right\}$  de vectores linealmente independientes. Esto es:
$ \breve{C}_{1}\ =\breve{C}_{2}\ =\breve{C}_{3}\ =\cdots=\breve{C}_{n-j} =0$.

Por lo tanto:
\begin{align*}
\left|  v_{n-j+1}\right>  &  =\breve{C}_{1}\ \left| v_{1}\right> +\breve{C}_{2}\ \left|  v_{2}\right> +\breve{C}_{3}\ \left|  v_{3}\right> \cdots+\breve{C}_{n-j}\ \left|  v_{n-j}\right> =\sum_{i=1}^{n-j}\ \breve{C}_{i}\ \left|  v_{i}\right> \,.
\end{align*}

Diremos entonces que $\left\{  \left|  v_{1}\right>, \left|  v_{2}\right>, \left|  v_{3}\right>, \cdots,\left|  v_{n-j}\right> \right\}$ forman una base para el espacio vectorial  $\textbf{\em V}$. 

Es importante señalar, que la dimensión de $\textbf{\em V}$ será el número de vectores linealmente independientes, que para este caso será: dim$\textbf{\em V}= n-j$. 

Entonces,  se puede comprobar que, dado un vector arbitratio $\left| x \right> \in\textbf{\em V}$, se tiene que:
\[
\left| x \right> =\sum_{i=1}^{n-j}\ C_{i}\ \left|
{v}_{i}\right>  \,\,\forall\,\, \left| {x}\right> \in\textbf{\em V}\,,
\]
y el conjunto $\left\{ C_{1},C_{2},C_{3},\cdots C_{n-j}\right\}$ será
único. 

Diremos que el número mínimo de vectores: 
$\left|  v_{1}\right> ,\ \left|  v_{2}\right>,\ \left|  v_{3}\right>, \cdots,\left|  v_{n-j}\right>$ 
que expanden $\textbf{\em V}$ conforman una base de ese espacio vectorial, y que el número finito de cantidades $ C_{1},C_{2},C_{3},\cdots C_{n-j}\,,$   constituyen las {\it componentes} de $\left|  {x}\right>$ relativas a la base $\{\left|  v_{1}\right> ,\ \left|  v_{2}\right> ,\cdots,\left|  v_{n-j}\right>\}$. 

Queda claro que el vector cero, $\left| 0\right>$, es linealmente dependiente, y cualquier conjunto de vectores que lo contenga es un conjunto linealmente dependiente de vectores.

De lo anteriormente expuesto se puede concretar la siguiente definición para una base de un espacio vectorial $\textbf{\em V}$: 

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Definición}: Un conjunto finito de vectores: 
\[
\mathcal{B}=\left\{  \left|  v_{1}\right>, \left| v_{2}\right>, \left|  v_{3}\right>, \cdots ,\left|  v_{n}\right> \right\}  \in\mathbf{V} \,,
\]
se les denominará base del espacio vectorial $\textbf{\em V}$ si los vectores: $\left|{v}_{1}\right> ,\ \left|  v_{2}\right> ,\ \left|{v}_{3}\right>, \cdots,\left|  v_{n}\right>$, son linealmente independientes y (obviamente) expanden $\textbf{\em V}$.  Diremos entonces que $\mathcal{B}$ es un sistema generador de $\textbf{\em V}$.
\end{mdframed}

Es fácil darse cuenta que si $\textbf{\em V}$ lo expanden $n$ vectores linealmente independientes, cualquier otro vector $\left|  {x}
\right> \in\textbf{\em V}$ será linealmente dependiente. Igualmente, y fácilmente demostrable, es que todas las bases de un espacio vectorial $\textbf{\em V}$, de dimensión finita, tendrán el mismo número de elementos y ese número de elemento será la dimensión del espacio, es decir, la dim$\textbf{\em V}=$ número $n$ de vectores que forman una base de dicho espacio.

La base más familiar en el espacio tridimensional real es el conjunto de conformado por tres vectores ortogonales y unitarios: $\{\bf{i}, \bf{j},\bf{k} \}$. Por lo tanto, como ya sabemos, la dimensión del espacio vectorial  $\textbf{\em V}=\mathds{R}^{3}$ es $3$.  Al conjunto de vectores $\{\bf{i}, \bf{j},\bf{k} \}$ le podemos asociar tres ejes coordenados: $\{x, y, z \}$ (decimos que le anclamos un sistema de coordenadas), de manera que las componentes $C_{1}, C_{1}, C_{1}$ de un vector $\left|{v}\right>$ respecto a esta base son las proyecciones de $\left|{v}\right>$ a lo largo de los ejes coordenados. 

El concepto de base de un espacio vectorial es de fundamental importancia, ya que una vez especificada la base las operaciones sobre los elementos del  espacio vectorial abstracto se pueden realizar ahora sobre los números que representan las componentes del vector con respecto a la base. Esto significa que cuando sumamos dos vectores de un espacio vectorial abstracto $\textbf{\em V}$, sus componentes (respecto a una base) son sumadas. Cuando multiplicamos un vector de $\textbf{\em V}$ por un elemento $\alpha$ del campo $\textbf{\em K}$, todas sus componentes son multiplicadas por $\alpha$.

Adicionalmente, puede ser que dentro de un espacio vectorial $\textbf{\em V}$ se puedan encontrar subespacios y dentro de esos subespacios un conjunto de vectores base. 

Vale decir,  si $\forall\ \left| x \right>
\in\textbf{\em V}$:
\[
\left| x \right> =\underset{\textbf{\em S}_{1}}{\underbrace{C_{1}\ \left|  v_{1}\right> \cdots +C_{n-j}\ \left|{v}_{n-j}\right> }}
+\underset{\textbf{\em S}_{2}}{\underbrace{C_{n-j+1}\ \left|  v_{n-j+1}\right> \cdots C_{n-k}\ \left|{v}_{n-k}\right> }}+
\underset{\textbf{\em S}_{3}}{\underbrace{C_{n-k+1}\ \left|  v_{n-k+1}\right> \cdots C_{n}\ \left|{v}_{n}\right> }} \,,
\]
con: $\left| x \right> =\left|  x_{1}\right> +\left|  x_{2}\right> +\left|  x_{3}\right> \quad \mbox{y} \quad \left|  x_{1}\right> \in\textbf{\em S}_{1}; \quad\left|{x}_{2}\right> \in\textbf{\em S}_{2}; \quad\left| x_{3}\right> 
\in\textbf{\em S}_{3}$. Entonces diremos que $\textbf{\em V}$ es la
suma directa de $\textbf{\em S}_{1},\textbf{\em S}_{2}$ y $\textbf{\em S}_{3}$ y lo denotaremos como: $\textbf{\em V}=\textbf{\em S}_1 \oplus\textbf{\em S}_{2}\oplus\textbf{\em S}_{3}$.

También es bueno señalar  que, una vez fijada una base, las componentes de un vector según esa base, son únicas y que dada una base de un espacio vectorial, se pueden construir otras bases diferentes de ésta, como veremos más adelante.

\subsection{El determinante de Gram}
\label{DeterminanteGram}
\index{Gram!Determinante}
\index{Gram!Jorgen Pedersen Gram}
Dado un conjunto de vectores: $\left\{  \left|  v_{1}\right>, \left| v_{2}\right>, \left| v_{3}\right>,\cdots ,\left|  v_{n}\right> \right\}  \in \textbf{\em V}$, existe una forma directa de comprobar la independencia lineal del conjunto. 

Dado un vector $\left| {x}\right> \in\textbf{\em V}$ tendremos que  $\left| x \right> =\sum_{i=1}^{n}\ C_{i}\ \left|  v_{i}\right>$, entonces, al multiplicar por  $\left<{v}_{i}\right|$, resulta:
\[
\begin{array}
[c]{lc}
C_{1} \left<{v}_{1}\right.  \left|  v_{1}\right> +C_{2} \left<{v}_{1}\right.  \left|  v_{2}
\right> +C_{3} \left<{v}_{1}\right.  \left| {v}_{3}\right> +\cdots+C_{n} \left<{v}_{1}\right.\left|  v_{n}\right>  & =\left<{v}_{1}\right.\left| x \right> \\
C_{1} \left<{v}_{2}\right.  \left|  v_{1}\right> +C_{2} \left<{v}_{2}\right.  \left|  v_{2}
\right> +C_{3} \left<{v}_{2}\right.  \left| {v}_{3}\right> +\cdots+C_{n} \left<{v}_{2}\right.\left|  v_{n}\right>  & =\left<{v}_{2}\right.\left| x \right> \\
\vdots & \vdots\\
C_{1} \left<{v}_{n}\right.  \left|  v_{1}\right> +C_{2} \left<{v}_{n}\right.  \left|  v_{2}
\right> +C_{3} \left<{v}_{n}\right.  \left|{v}_{3}\right> +\cdots+C_{n} \left<{v}_{n}\right.
\left|  v_{n}\right>  & =\left<{v}_{n}\right.\left| x \right>
\end{array}
\]
donde las $C_{1},C_{2},C_{3},\cdots C_{n}$ son las incógnitas.  Por lo cual,  para que este sistema tenga solución se impone que:
\[
\left|
\begin{array}
[c]{ccccc}
\left<{v}_{1}\right.  \left|  v_{1}\right>  &
\left<{v}_{1}\right.  \left|  v_{2}\right>  &
\left<{v}_{1}\right.  \left|  v_{3}\right>  &
\cdots & \left<{v}_{1}\right.  \left|  v_{n}
\right> \\
\left<{v}_{2}\right.  \left|  v_{1}\right>  &
\left<{v}_{2}\right.  \left|  v_{2}\right>  &
\left<{v}_{2}\right.  \left|  v_{3}\right>  &
\cdots & \left<{v}_{2}\right.  \left|  v_{n}
\right> \\
\vdots &  & \ddots &  & \vdots\\
\left<{v}_{n}\right.  \left|  v_{1}\right>  &
\left<{v}_{n}\right.  \left|  v_{2}\right>  &
\left<{v}_{n}\right.  \left|  v_{3}\right>  &
\cdots & \left<{v}_{n}\right.  \left|  v_{n}
\right>
\end{array}
\right|  \neq0 \,.
\]

Esto es, que el determinante de Gram\footnote{{JORGEN PEDERSEN GRAM} (1850-1916 Dinamarca) Matemático danés, que alternaba su actividad de gerente de una importante compañía de seguros con las matemáticas (Probabilidad, análisis numérico y teoría de números). Es conocido mayormente por el método de ortogonalización, pero se presume que no fue él quien primero lo utilizó. Aparentemente fue ideado por Laplace y utilizado también por Cauchy en 1836. Gram murió arrollado por una bicicleta a la edad de 61 años.} sea  distinto de cero implica que el conjunto:  
$\left\{  \left| {v}_{1}\right> ,\ \left|  v_{2}\right> , \cdots,\left| {v}_{n}\right> \right\}$ es linealmente independiente. La inversa también es cierta.

Vamos a considerar un par de ejemplos de bases para algunos de los espacios vectoriales conocidos:
\index{Ejemplos de Bases de espacios vectoriales}
\index{Bases!Ejemplos de Bases de espacios vectoriales}
\index{Espacios vectoriales lineales!Ejemplos de Bases de espacios vectoriales}
\begin{enumerate}
\item El espacio vectorial $\textbf{\em V}^{n}$ tendrá dimensión $n$ y una de las posibles bases $\left\{  \left|  v_{1}\right> ,\ \left|  v_{2}\right> ,\ \left|  v_{3}\right> \cdots,\left| {v}_{n}\right> \right\}$ será:
\[
\left|  v_{1}\right> =\left(1,0,0,\cdots,0\right)\,, \,\,
\left|  v_{2}\right> =\left(0,1,0,\cdots,0\right)\,, \,\,
\left|  v_{3}\right> =\left(0,0,1,\cdots,0\right)\,, \dots  ,\,\, 
\left|  v_{n-j}\right>=\left(  0,0,0,\cdots,1\right) \,.
\]
Esta base se conoce con el nombre de base canónica.

\item  El espacio de polinomios, $\mathcal{P}^{n},$ de grado $g\leq n$ tendrá como una de las posibles bases al conjunto: $\left\{  1, t, t^{2}, t^{3},\cdots,t^{n}\right\}$, porque cualquier polinomio de grado $\leq n$ podrá ser expresado como combinación lineal de estos $n+1$ vectores.
Más aún, el espacio de \textbf{todos} los polinomios, $\mathcal{P}^{\infty},$ tendrá como una posible base al conjunto de funciones:$\left\{  1,t,t^{2},t^{3},\cdots,t^{n}\cdots\right\}$. En este caso $\mathcal{P}^{\infty}$ será infinito dimensional.
\end{enumerate}

\subsection{Ortogonalidad y bases ortogonales}
\label{OrtogonalidadBases}
\index{Bases ortogonales}
\index{Ortogonalidad}
En un espacio  vectorial con producto interno, dos vectores $\left| \mathrm{e}_{1}\right> \wedge\left|  \mathrm{e}_{2}\right> $
serán ortogonales si su producto interno se anula
\[
\left|  \mathrm{e}_{1}\right> \,\,\bot \,\, \left|  \mathrm{e}_{2}\right> \,\, \Leftrightarrow \,\ \left<\mathrm{e}_{2}\right.
\left|  \mathrm{e}_{1}\right> =0 \,.
\]

Se denomina un conjunto {\bf ortogonal} de vectores $\left\{  \left|  \mathrm{e}_{1}\right> ,\ \left|  \mathrm{e}_{2}\right> ,\ \left|
{e}_{3}\right> \cdots,\left|  \mathrm{e}_{n}\right> \right\}$ si:
\[
\left<\mathrm{e}_{i}\right.  \left|  \mathrm{e}_{j}\right> =
\delta_{ij}\left\|  \left|  \mathrm{e}_{j}\right> \right\|^{2} \,,\quad i,j=1,2,3,\cdots,n \quad\text{y con} \quad \delta_{ij}=
\left\{
\begin{array}
[c]{c}
0\qquad\text{si }i\neq j\\
1\qquad\text{si }i=j
\end{array}
\right.
\]
y se denominará conjunto {\bf ortonormal} si: $\left\|  \left| \mathrm{e}_{j}\right> \right\|^{2}=1$.

Un conjunto ortogonal de vectores $\left\{  \left|  \mathrm{e}_{1}
\right> ,\ \left|  \mathrm{e}_{2}\right> ,\ \left|  \mathrm{e}_{3}\right> , \cdots,\left|  \mathrm{e}_{n}\right> \right\} \in\textbf{\em V}$ es linealmente independiente. Más aún, para el caso particular de un espacio euclidiano este conjunto conforma una base ortogonal para $\textbf{\em V}$. 

La demostración es sencilla, para un determinado espacio vectorial $\textbf{\em V}$ una combinación lineal de los vectores: $\left\{  \left|  \mathrm{e}_{1}\right> ,\ \left| \mathrm{e}_{2}\right> ,\ \left|  \mathrm{e}_{3}\right>,
\cdots,\left|  \mathrm{e}_{n}\right> \right\}$ se anula. Veamos:
\[
\sum_{i=1}^{n}\ C_{i}\ \left|  \mathrm{e}_{i}\right> =\left| {0}\right> \,\, \Rightarrow \,\,\left\{
\begin{tabular}
[c]{lllll}
$\left<\mathrm{e}_{1}\right|  \left[  \sum_{i=1}^{n}\ C_{i}\ \left| \mathrm{e}_{i}\right> \right]  =0$ & $\,\, \Rightarrow \,\,$ & $\sum_{i=1}^{n}C_{i}\ \delta_{1i}=0$ & $\,\, \Rightarrow \,\,$ & $C_{1}=0$\\
$\left<\mathrm{e}_{2}\right|  \left[  \sum_{i=1}^{n}\ C_{i}\ \left| \mathrm{e}_{i}\right> \right]  =0$ & $\,\, \Rightarrow \,\,$ & $\sum_{i=1}^{n}C_{i}\ \delta_{2i}=0$ & $\,\, \Rightarrow \,\,$ & $C_{2}=0$\\
$\left<\mathrm{e}_{3}\right|  \left[  \sum_{i=1}^{n}\ C_{i}\ \left| \mathrm{e}_{i}\right> \right]  =0$ & $\,\, \Rightarrow \,\,$ & $\sum_{i=1}
^{n}C_{i}\ \delta_{3i}=0$ & $\,\, \Rightarrow \,\,$ & $C_{3}=0$\\
$\vdots$ & $\ddots$ & $\vdots$ &  & $\vdots$\\
$\left<\mathrm{e}_{n}\right|  \left[  \sum_{i=1}^{n}\ C_{i}\ \left| \mathrm{e}_{i}\right> \right]  =0$ & $\,\, \Rightarrow \,\,$ & $\sum_{i=1}^{n}C_{i}\ \delta_{ni}=0$ & $\,\, \Rightarrow \,\,$ & $C_{n}=0$
\end{tabular}
\right.
\]
con lo cual, queda claro que: $\left\{  \left|  \mathrm{e}_{1}\right>,\ \left|  \mathrm{e}_{2}\right> ,\ \left|  \mathrm{e}_{3}\right>, 
\cdots,\left|  \mathrm{e}_{n}\right> \right\} $ son un conjunto de vectores linealmente independientes. 

Si la dimensión de $\textbf{\em V}$ es $n$ $\left(\dim
\textbf{\em V}=n\right)$ y tenemos $n$ vectores linealmente independientes, entonces esos
$n$ vectores $\left\{  \left|  \mathrm{e}_{1}\right> ,\ \left| \mathrm{e}_{2}\right> ,\ \left|  \mathrm{e}_{3}\right>, 
\cdots,\left|  \mathrm{e}_{n}\right> \right\}$ forman una base ortogonal para $\textbf{\em V}$, y por lo tanto, las componentes de un vector en esa base se pueden expresar de manera simple:
\[
\forall  \left| x \right> \in\textbf{\em V}\,\, \Rightarrow \,\,\left| {x}\right> =\sum_{i=1}^{n}\ C_{i}\ \left|  \mathrm{e}_{i}\right>\,\, \Rightarrow \,\,\left<\mathrm{e}_{j}\right.  \left| {x}\right> =\left<\mathrm{e}_{j}\right|  \left[
\sum_{i=1}^{n}\ C_{i}\ \left|  \mathrm{e}_{i}\right> \right] \,\, \Rightarrow \,\, C_{j}=\frac{\left<\mathrm{e}_{j}\right.  \left| {x}\right> }{\left<\mathrm{e}_{j}\right.  \left|{e}_{i}\right> }=\frac{\left<\mathrm{e}_{j}\right.\left|x\right> }{\left\| \left|\mathrm{e}_{j}\right> \right\|^2}\,.
\]

En el caso de un conjunto ortonormal de vectores $\left\{  \left| \mathrm{\hat{e}}_{1}\right> ,\ \left|  \mathrm{\hat{e}}_{2}\right> ,\ \left| \mathrm{\hat{e}}_{3}\right> \cdots,\left|  \mathrm{\hat{e}}_{n}\right> \right\}  \in\textbf{\em V}^{n}$, con $\left\|  \left|  \mathrm{\hat{e}}_{j}\right> \right\|^{2}=1$, las componentes de cualquier vector quedan determinadas de una forma todavía más simple y con consecuencias mucho más impactantes
\[
\left\|  \left|  \mathrm{\hat{e}}_{i}\right> \right\|^{2}=1\,\, \Rightarrow \,\,
C_{i}=\left<\mathrm{\hat{e}}_{i}\right.  \left| x \right> \,\, \Rightarrow \,\,\left| x \right> =\sum_{i=1}^{n}\ C_{i} \ \left|  \mathrm{\hat{e}}_{i}\right> =\sum_{i=1}^{n}\ \left<\mathrm{\hat{e}}_{i}\right.  \left| x \right> \ \left| \mathrm{\hat{e}}_{i} \right> \equiv 
\underset{{1}}{\underbrace{\sum_{i=1}^{n}\ \left|  \mathrm{\hat{e}}_{i}\right> \left<\mathrm{\hat{e}}_{i}\right|   }}\left| x \right> \,.
\]

Es bueno recalcar la relación de cierre:\footnote{La relación de cierre expresa una propiedad importante de los vectores base: si los $\left|  \mathrm{\hat{e}}_{i}\right> $ se multiplican por la derecha por los $\left<\mathrm{\hat{e}}_{i}\right|$, el resultado, luego de sumar para todos los vectores, es el operador lineal unitario. Se dice también que el conjunto de vectores $\{\left|  \mathrm{\hat{e}}_{i}\right> \}$ forman un conjunto completo. } $\sum_{i=1}^{n}\ \left|  \mathrm{\hat{e}}_{i}\right> \left<\mathrm{\hat{e}}_{i}\right|  ={1}$, con lo cual es trivial demostrar la fórmula de Parseval:
\[
\forall  \left| x \right> , \left| y \right> \in \textbf{\em V}\,\, \Rightarrow \,\,\left<{y}\right.  \left|  {x} \right> \equiv
\left<{y}\right|  \left(  \sum_{i=1}^{n}\ \left|  \mathrm{\hat{e}}_{i}\right> \left<\mathrm{\hat{e}}_{i}\right| \right)  \left| x \right> =\sum_{i=1}^{n}\ \left<
{y}\right|  \left.  \mathrm{\hat{e}}_{i}\right> \left<\mathrm{\hat{e}}_{i}\right|  \left.  {x}\right> =\sum_{i=1}^{n}\ \left<{y}\right.  \left|  \mathrm{\hat{e}}_{i}\right>
\left<{x}\right.  \left|  \mathrm{\hat{e}}_{i}\right> ^{\ast} \,,
\]
para el caso de $\left|{x}\right> \equiv\left| y \right>$  se llega a la generalización del teorema de Pitágoras:
\[
\left<{x}\right.  \left| x \right> \equiv
\left\| \left| x \right> \right\|^{2}=\sum_{i=1}^{n}\left| \left<{x}\right.  \left|  \mathrm{\hat{e}}_{i}\right> \right|
^{2}\,.
\]

Consideremos un par de ejemplos de bases ortogonales en espacio funcionales:
\index{Bases!Ejemplos bases ortogonales}
\index{Ortogonales!Ejemplos de bases}
\index{Ejemplos de bases ortogonales}
\begin{enumerate}
\item  {\bf Funciones trigonométricas:}  
Uno de los ejemplos más emblemáticos es el caso de las funciones continuas, reales de variable real y definidas en $\left[  0,2\pi\right]  $, $\mathcal{C}_{\left[
0,2\pi\right]  }^{\infty}$, con el producto interno  definido por: $\left<{f}\right|  \left.  {g}\right> =\int_{0}^{2\pi}\mathrm{d}x\ f(x)  \ g(x)$. Ésto es, el conjunto de funciones  $\left\{  \left|  \mathrm{e}_{i}\right> \right\}$ representadas por:
\[
\label{BaseFuncTrigonometrica}
\left|  \mathrm{e}_{0}\right> =1,\quad\left|  \mathrm{e}_{2n-1} \right> =\cos(nx)\quad\text{y}\quad\left|  \mathrm{e}_{2n}\right>=
\mathrm{sen}(nx),\quad\text{con }n=1,2,3,\cdots
\]
Es claro que $\left\{  \left|  \mathrm{e}_{1}\right>, \left| \mathrm{e}_{2}\right>, \left|  \mathrm{e}_{3}\right>, \cdots ,\left|  \mathrm{e}_{n}\right> ,\cdots\right\}$ es un conjunto de funciones ortogonales por cuanto:
\[
\left<\mathrm{e}_{n}\right.  \left|  \mathrm{e}_{m}\right>
=\delta_{nm}\left\|  \left|  \mathrm{e}_{n}\right> \right\|^{2}\, \Rightarrow \,
\left\{
\begin{array}
[c]{rcl}
0\quad\text{si} & n\neq m &  \Rightarrow  \left\{
\begin{array}
[c]{c}
\int_{0}^{2\pi}\mathrm{d}x\ \mathrm{sen}(nx)\mathrm{sen}(mx)=0\\
\int_{0}^{2\pi}\mathrm{d}x\ \cos(nx)\mathrm{sen}(mx)=0\\
\int_{0}^{2\pi}\mathrm{d}x\ \cos(nx)\cos(mx)=0
\end{array}
\right. \\
&  & \\
\left\|  \left|  \mathrm{e}_{n}\right> \right\|^{2}\quad \text{si} & n=m & 
 \Rightarrow  \left\{
\begin{array}
[c]{lcl}
\int_{0}^{2\pi}\mathrm{d}x\ =2\pi & \text{si} & n=m=0\\
&  & \\
\int_{0}^{2\pi}\mathrm{d}x\ \cos^{2}(nx)=\pi & \text{si} & n=m=2k-1\\
&  & \\
\int_{0}^{2\pi}\mathrm{d}x\ \mathrm{sen}^{2}(nx)=\pi & \text{si} & n=m=2k
\end{array}
\right.
\end{array}
\right.
\]
con $k=1,2,3,\cdots$.

Podremos construir una base ortonormal de funciones: $\left\{  \left|  \mathrm{\hat{e}}_{1}\right>,\ \left|  \mathrm{\hat{e}}_{2}\right> ,\ \left|  \mathrm{\hat{e}}_{3}\right>
,\cdots,\left|  \mathrm{\hat{e}}_{n}\right> ,\cdots\right\}$ de la forma:
\[
\left|  \mathrm{\hat{e}}_{0}\right> =\frac{1}{\sqrt{2\pi}},\quad\left| \mathrm{\hat{e}}_{2n-1}\right> =\frac{1}{\sqrt{\pi}}\cos(nx)\quad
\text{y}\quad\left|  \mathrm{\hat{e}}_{2n}\right> =\frac{1}{\sqrt{\pi}
}\mathrm{sen}(nx).
\]

Por lo tanto, dada una función definida en el intervalo $\left[0,2\pi\right]$, podremos expresarla en términos de la base ortogonal como:
\[
\left|  {f}\right> =\sum_{i=1}^{\infty}\ C_{i}\ \left| \mathrm{e}_{i}\right> \,\, \Rightarrow \,\, C_{i}=\left<\mathrm{e}_{i}\right.  \left|  {f}\right> =\left\{
\begin{array}
[c]{lcl}
\int_{0}^{2\pi}\mathrm{d}x \ f(x)  =C_{0} & \text{si} & i=0\\
&  & \\
 \int_{0}^{2\pi}\mathrm{d}x\ f(x)  \ \cos(nx)\ =C_{2n-1} &
\text{si} & i=2n-1\\
&  & \\
 \int_{0}^{2\pi}\mathrm{d}x\ f(x)  \ \mathrm{sen}(nx)=C_{2n} &
\text{si} & i=2n
\end{array}
\right.
\]
donde los $C_{i}$ son los coeficientes de Fourier.

\item {\bf Polinomios de Legendre:} 
Otro de los ejemplos típicos son los llamados polinomios de Legendre,  $P_{n}(x)$,  definidos en el intervalo $\left[ -1,1\right] $.  Estos polinomios pueden ser generados a partir de la Fórmula de Rodrigues:\footnote{{BENJAMIN OLINDE RODRIGUES} (1794 Burdeos, Francia - 1851, París Francia) Banquero, matemático y activista político socialista francés durante la revolución francesa. De origen judío, y cuyas contribuciones fundamentales como la fórmula para la generación de polinomios de Legendre, permanecieron olvidadas por mucho tiempo.}
\[
P_{n}(x)=\frac{1}{n!2^{n}}\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}(x^{2} -1)^{n},\qquad n=0,1,2,.....
\]
con $P_{0}(x)=1$. Algunos de estos polinomios son los siguientes:
\[
P_{1}(x)=x\,,\,\, P_{2}(x)=\frac{1}{2}(3x^2-1)\,,\,\,
P_{3}(x)=\frac{x}{2}(5x^2-3)\,,\,\,P_{4}(x)=\frac{1}{8}(35x^4-30x^2+3)\,, \dots
\]
Como veremos más adelante, los polinomios de Legendre son solución de la ecuación diferencial:
\[
(1-x^{2})\ y^{\prime\prime}-2x\ y^{\prime}+\lambda(\lambda+1) y=0\,.
\]

Es fácil comprobar que los polinomios de Legendre $|{P}_{\alpha}>=P_{\alpha}(x)$ son mutuamente ortogonales con un producto interno definido como:
\[
\left<{P}_{n}\right. \left| {P}_{m}\right> =\int_{-1}^{1}P_{n}(x)P_{m}(x)\mathrm{d}x=\frac{2}{2n+1}\delta_{nm} \,,
\]
y con una norma definida por:
\[
\left\|  {P}_{n}\right\|^{2} = \left<{P}_{n}\right. \left|{P}_{n} \right> = \int_{-1}^{1}P_{n}^{2}(x)\mathrm{d}x = \frac{2}{2n+1}\,.
\]

Por lo tanto, cualquier función en el intervalo $\left[ -1,1\right]$ puede ser
expresada en esa base.
\[
f(x)=\left| {f}\right> = \sum_{k=0}^{\infty}a_{k}\ \left| {P}_{k}\right>  =\sum_{k=0}^{\infty}\frac{\left< {P}_{k} \right. \left|{F}\right> }
{\left< {P}_{k} \right. \left| {P}_{k} \right> }\ \left| {P}_{k} \right> \,.
\]

Si $f(x)$ es en particular un polinomio, entonces:
\[
f(x)=\sum_{n=0}^{m}b_{n}x^{n}=\sum_{k=0}^{\infty}a_{k} \left| {P}_{k} \right> =\sum_{n=0}^{\infty}a_{n}P_{n}(x) \,,
\]
y los coeficientes $a_{n}$ se determinan fácilmente a través de un sistema de ecuaciones algebraicas. Por ejemplo, para el caso de $f(x)=x^{2}$ tendremos:
\begin{align*}
f(x)  &  =x^{2}=a_{0}P_{0}(x)+a_{1}P_{1}(x)+a_{2}P_{2}(x)\\
&  =a_{0}+a_{1}x+\frac{1}{2}a_{2}(3x^{2}-1) =\frac{1}{3}P_{0}(x)+\frac{2}{3}P_{2}(x) \,.
\end{align*}

Quedará como ejercicio demostrar que para el caso de:
\[
g(x)=\sqrt{\frac{1-x}{2}} = \sum_{k=0}^{\infty}\frac{\left< P_{k}\right. \left| g \right> }{\left<P_{k}\right. \left|P_{k}\right> }\ \left| P_{k}\right> = \frac{2}{3} P_{0}(x) -2\sum_{n=1}^{\infty} \frac{P_{n}(x)}{(2n-1) (2n+3) } \,,
\]
con:
$
\left< P_{k} \right. \left| g \right> =\int_{-1}^{1} g(x) P_{k}(x)\, \mathrm{d}x = \int_{-1}^{1}\sqrt{\frac{1-x}{2}}P_{k}(x) \, \mathrm{d}x$.


\end{enumerate}

\subsection{Ortogonalización}
\label{OrtogonalizacionGram-Schmidt}
\index{Ortogonalización!Método Gram-Schmidt}
\index{Gram-Schmidt!Método Ortogonalización}
\index{Schmidt!Erhard Schmidt}
Hemos visto que un conjunto de vectores ortogonales forman una base para un espacio vectorial. Ahora bien, siempre es posible construir un conjunto de vectores ortogonales a partir de un conjunto de vectores linealmente independientes. El método de ``ortogonalización'' se conoce como el método de Gram-Schmidt\footnote{{ERHARD SCHMIDT} (1876, Estonia - 1959 Alemania). Matemático alemán fundador del primer instituto de matemáticas aplicadas de Berlín. Alumno de Hilbert, Schmidt hizo sus mayores contribuciones en el campo de ecuaciones integrales y teoría de funciones en el espacio de Hilbert.}, en honor de estos dos matemáticos alemanes que NO lo inventaron pero hicieron famoso al método que, al parecer, se le debe al matemático francés P.S. Laplace.

Consideremos, por ejemplo, un conjunto de vectores linealmente independientes, $\left\{  \left| v_{1}\right>, \left|  v_{2}\right>, \left| v_{3}\right>, \cdots ,\left|  v_{n}\right> \right\} $ que expanden un espacio euclidiano real de dimensión finita, $\textbf{\em E}^{n}$.  Entonces, siempre se podrá construir un conjunto ortogonal de vectores, $\left\{ \left|  \mathrm{e}_{1}\right>, \left| \mathrm{e}_{2}\right>, \left|  \mathrm{e}_{3}\right>, \cdots ,\left|  \mathrm{e}_{n}\right> \right\}$, que también expandan $\textbf{\em E}^{n}$, de la siguiente forma:
\begin{center}
$
\begin{array}
[c]{ll}
\left|  \mathrm{e}_{1}\right> \equiv\left|  v_{1}\right>
& \\
& \\
\left|  \mathrm{e}_{2}\right> \equiv\left|  v_{2}\right> -\frac{\left<{v}_{2}\right.  \left|  \mathrm{e}_{1}\right>
}{\left<\mathrm{e}_{1}\right.  \left|  \mathrm{e}_{1}\right>}\left|  \mathrm{e}_{1}\right>  & \qquad \setminus \quad\left<\mathrm{e}_{2}\right.  \left|  \mathrm{e}_{1}\right> =0\\
& \\
\left|  \mathrm{e}_{3}\right> \equiv\left|  v_{3}\right> -\frac{\left<{v}_{3}\right.  \left|  \mathrm{e}_{2}\right>}{\left<\mathrm{e}_{2}\right.  \left|  \mathrm{e}_{2}\right>}\left|  \mathrm{e}_{2}\right> -\frac{\left<{v}_{3}\right.  \left|  \mathrm{e}_{1}\right> }{\left<\mathrm{e}_{1}\right.  \left|  \mathrm{e}_{1}\right> }\left|  \mathrm{e}_{1}\right>  & \qquad \setminus \quad\left\{
\begin{array}
[c]{c}
\left<\mathrm{e}_{3}\right.  \left|  \mathrm{e}_{1}\right> =0\\
\left<\mathrm{e}_{3}\right.  \left|  \mathrm{e}_{2}\right> =0
\end{array}
\right. \\
& \\
\left|  \mathrm{e}_{4}\right> \equiv\left|  v_{4}\right> -\frac{\left<{v}_{4}\right.  \left|  \mathrm{e}_{3}\right>
}{\left<\mathrm{e}_{3}\right.  \left|  \mathrm{e}_{3}\right>}\left|  \mathrm{e}_{3}\right> -\frac{\left<{v}_{4}\right.  \left|  \mathrm{e}_{2}\right> }{\left<\mathrm{e}_{2}\right.  \left|  \mathrm{e}_{2}\right> }\left|  \mathrm{e}_{2}\right> -\frac{\left<{v}_{4}\right.  \left| \mathrm{e}_{1}\right> }{\left<\mathrm{e}_{1}\right.  \left| \mathrm{e}_{1}\right> }\left|  \mathrm{e}_{1}\right>  & \qquad \setminus \quad 
\left\{
\begin{array}
[c]{c}
\left<\mathrm{e}_{4}\right.  \left|  \mathrm{e}_{1}\right> =0\\
\left<\mathrm{e}_{4}\right.  \left|  \mathrm{e}_{2}\right> =0\\
\left<\mathrm{e}_{4}\right.  \left|  \mathrm{e}_{3}\right> =0
\end{array}
\right. \\
\vdots & \vdots\\
\left|  \mathrm{e}_{n}\right> \equiv\left|  v_{n}\right>
-\sum_{i=1}^{n-1}\frac{\left<{v}_{n}\right.  \left| \mathrm{e}_{i}\right> }{\left<\mathrm{e}_{i}\right.  \left| \mathrm{e}_{i}\right> }\left|  \mathrm{e}_{i}\right>  & \qquad
\setminus\quad\left\{
\begin{array}
[c]{c}
\left<\mathrm{e}_{4}\right.  \left|  \mathrm{e}_{1}\right> =0\\
\left<\mathrm{e}_{4}\right.  \left|  \mathrm{e}_{2}\right> =0\\
\left<\mathrm{e}_{4}\right.  \left|  \mathrm{e}_{3}\right> =0\\
\vdots\\
\left<\mathrm{e}_{4}\right.  \left|  \mathrm{e}_{n-1}\right> =0
\end{array}
\right.
\end{array}
$
\end{center}
Así, siempre es posible construir una base ortogonal a partir de un conjunto de vectores linealmente independientes y una definición de producto interno. Esta base será única en $\textbf{\em E}^{n}$ --para cada definición de producto interno-- y si existe otra, sus vectores serán proporcionales. Más aún, cada espacio vectorial $\textbf{\em V}^{n}$ de dimensión finita tendrá una base ortogonal asociada\footnote{Hemos construido la base ortogonal para un espacio de dimensión finita, pero el procedimiento es válido para espacios de dimensión infinita}.

\subsection{{\color{Fuchsia}Ejemplos}} 
\label{EjemplosDependenciaLineal}
\index{Dependencia lineal!Ejemplos de}
\index{Independencia Lineal!Ejemplos de}
\index{Ejemplos de dependencia/independencia lineal}

\subsubsection{Independencia lineal}
Presentamos  otros ejemplos sobre dependencia/independencia lineal.
\begin{enumerate}
\item Si consideramos el espacio vectorial $\textbf{\em V}=\left\{  \left|{v}_{1}\right>, \left|  v_{2}\right>, \left| {v}_{3}\right>, \cdots ,\left|  v_{n}\right> \right\}$ serán ejemplos de \textbf{independencia lineal}:

\begin{itemize}
\item $\left|  v_{k}\right> \equiv f(t)=t^{k}$, para $k=1,2,3,\cdots$. Es claro que un polinomio de grado $n+1$, no podrá ser expresado en términos un polinomio de grado $n$, en otras palabras: $t^{n+1}\neq\sum_{i=0}^{n}{C}_{i}\ t^{i}$\,.

\item $\left|  v_{k}\right> \equiv f(t) ={\Large e}^{a_{k}t}$,  con $a_{1},a_{2},a_{3},\cdots$ coeficientes constantes. También salta a la vista que no podremos expresar una de esas funciones exponenciales como una combinación lineal. 
\end{itemize}

\item Si consideramos: $\left|  v_{1}\right> =\cos^{2}(t), \left|  v_{2}\right> = \mbox{sen}^{2}(t)$  y $\left|{v}_{3}\right> =1$,  es claro que $\left|  v_{1}\right>, \left|  v_{2}\right>$ y $\left| {v}_{3}\right>$ son \textbf{linealmente dependientes} por cuanto: $\left|  v_{1}\right> +\left|  v_{2}\right>=\ \left|  v_{3}\right>$. Nótese que si:
\[
\left|  v_{1}\right> =\cos(t),\, \left|  v_{2}\right> =\mbox{sen}(t)\ \text{y} \ \left|  v_{3}\right> =1,
\]
entonces $\left|  v_{1}\right>, \left|  v_{2}\right>$ y $\left|  v_{3}\right> $ serán vectores \textbf{linealmente independientes}. 

\item Consideremos ahora otro ejemplo en $\mathcal{P}^{3}$:
\[
\left| {x}_1 \right> =1\,,\quad \left| {x}_2 \right>  =x-1\,, \quad 
\left| {x}_3 \right> = x^{2}\,, \quad \left| {x}_4 \right>  =x^{2}+2x+1\,.
\]
Podemos ver que este conjunto es linealmente dependiente 
ya que siempre podremos expresar:
\[
\left| {x}_4 \right>  =3\left| {x}_1 \right>  +2\left| {x}_2\right>  +\left| {x}_3\right>  \,.
\]
\end{enumerate}

\subsubsection{Ortogonalización}
\label{EjemploOrtogonalizacion1}
\index{Ortogonalización}

Consideremos un par de ejemplos del método de ortogonalización sencillos, y uno más elaborado:
\index{Ortogonalizacion!Ejemplos de}
\index{Ejemplos de ortogonalizacion}
\begin{enumerate}

\item  Para el caso de $\mathds{R}^{2}$ es muy claro. Si tenemos dos vectores
$\left|  v_{1}\right> $ y $\left|  v_{2}\right>$ linealmente independientes:
\[
\left|  v_{1}\right> =\left(
\begin{array}
[c]{r}
1\\
1
\end{array}
\right)\,, \quad\left|  v_{2}\right> =\left(
\begin{array}
[c]{c}
0\\
1
\end{array}
\right) \,,
\]
y si elegimos $\left|  \mathrm{e}_{1}\right> \equiv\left|  v_{2}\right>$, entonces, $\left|  \mathrm{e}_{2}\right>$ vendrá dado por:
\[
\left|  \mathrm{e}_{2}\right> \equiv\left|  v_{1}\right> -\frac{\left<{v}_{1}\right.  \left|  \mathrm{e}_{1}\right>}{\left<\mathrm{e}_{1}\right.  \left|  \mathrm{e}_{1}\right>}\left|  \mathrm{e}_{1}\right> \,\, \Rightarrow \,\, \left|  \mathrm{e}_{2}\right> \equiv
\left(
\begin{array}
[c]{r}
1\\
1
\end{array}
\right)  -\left(
\begin{array}
[c]{c}
0\\
1
\end{array}
\right)  =\left(
\begin{array}
[c]{r}
1\\
0
\end{array}
\right) \,,
\]
tal y como se esperaba, el otro vector ortogonal es el canónico.

\item  Un subespacio de $\textbf{\em V}^{4}$, expandido por los siguientes vectores
\[
\left|  v_{1}\right> =\left(
\begin{array}
[c]{r}
1\\
3\\
-1\\
2
\end{array}
\right)\,, \quad \left|  v_{2}\right> =\left(
\begin{array}
[c]{c}
2\\
0\\
1\\
3
\end{array}
\right) \,, \quad \left|  v_{3}\right> =\left(
\begin{array}
[c]{r}
-1\\
1\\
0\\
0
\end{array}
\right) \,,
\]
tendrá una base ortogonal asociada dada por:
\begin{align*}
\left|  \mathrm{e}_{1}\right>  &  \equiv\left|  v_{3}\right> =\left(
\begin{array}
[c]{r}
-1\\
1\\
0\\
0
\end{array}
\right) ; \,\,
\left|  \mathrm{e}_{2}\right>   \equiv\left|  v_{2}\right> -\frac{\left<{v}_{2}\right.  \left| \mathrm{e}_{1}\right> }{\left<\mathrm{e}_{1}\right.  \left| \mathrm{e}_{1}\right> }\left|  \mathrm{e}_{1}\right> =
\left(
\begin{array}
[c]{c}
2\\
0\\
1\\
3
\end{array}
\right)  -\left(  -1\right)  \left(
\begin{array}
[c]{r}
-1\\
1\\
0\\
0
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
1\\
1\\
1\\
3
\end{array}
\right) \\
& \\
\left|  \mathrm{e}_{3}\right>  &  \equiv\left|  v_{1}\right> -\frac{\left<{v}_{1}\right.  \left| \mathrm{e}_{2}\right> }{\left<\mathrm{e}_{2}\right.  \left|
{e}_{2}\right> }\left|  \mathrm{e}_{2}\right> -\frac{\left<{v}_{1}\right.  \left|  \mathrm{e}_{1}\right> }{\left<\mathrm{e}_{1}\right.  \left|  \mathrm{e}_{1}\right>}\left|  \mathrm{e}_{1}\right> =  
\left(\begin{array}
[c]{r}
1\\
3\\
-1\\
2
\end{array}\right)
-\left(  \frac{9}{12}\right)  \left(
\begin{array}
[c]{c}
1\\
1\\
1\\
3
\end{array}
\right)
-\left(  1\right)  \left(
\begin{array}
[c]{r}
-1\\
1\\
0\\
0
\end{array}
\right)  =\left(
\begin{array}
[c]{r}
\frac{5}{4}\\
\\
\frac{5}{4}\\
\\
-\frac{7}{4}\\
\\
-\frac{1}{4}
\end{array}
\right)\,.
\end{align*}
Y la base ortonormal asociada será:
\begin{align*}
\left|  \mathrm{\hat{e}}_{1}\right>  &  = \frac{\left|  \mathrm{e}_1 \right> }{\sqrt{\left<\mathrm{e}_{1}\right.  \left| \mathrm{e}_{1}\right> }}= \frac{\sqrt{2}}{2}  
\left(
\begin{array}
[c]{r}
-1\\
1\\
0\\
0
\end{array}
\right) ; \,\, 
\left|  \mathrm{\hat{e}}_{2}\right> =
\frac{\left| \mathrm{e}_{2}\right> }{\sqrt{\left<\mathrm{e}_{2}\right. \left|  \mathrm{e}_{2}\right> }}=  \frac{\sqrt{3}}{6}
\left(
\begin{array}
[c]{c}
1\\
1\\
1\\
3
\end{array}
\right) ; \,\,
\left|  \mathrm{\hat{e}}_{3}\right>  &  =\frac{\left|  \mathrm{e}_{3}\right> }{\left<\mathrm{e}_{3}\right.  \left|  \mathrm{e}_{3}\right> }=  \frac12 \left(
\begin{array}
[c]{r}
{1}\\
\\
{1}\\
\\
-\frac{7}{5}\\
\\
-\frac{1}{5}
\end{array}
\right) \,.
\end{align*}

En este ejemplo hemos mostrado que $\left\{ \left|  \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right> , \left|  \mathrm{e}_{3}\right> \right\}$ son linealmente independientes y, por lo tanto, base de un subespacio de $\textbf{\em V}^{4}$. Cabría preguntarse: ¿Cómo construimos un cuarto vector, linealmente independiente a los anteriores, que expanda todo $\textbf{\em V}^{4}$?

\item Suponga el espacio de polinomios, $\mathcal{P}^{n}$, de grado $g\leq n$ definidos en el intervalo $\left[-1,1\right]$. Este espacio vectorial tendrá como una de las posibles bases al conjunto 
$\left\{ \left| \pi_i \right> \right\} = \left\{ 1, t, t^{2}, t^{3}, \cdots, t^{n}\right\}$ con el producto interno  definido por:\footnote{En este punto es importante señalar la importancia de la definición de producto interno. Si esta definición hubiera sido $\left<{f}\right|  \left.  {g}\right> =\int_{-1}^{1}\mathrm{d}x\ f(x)  \ g(x)\sqrt{1-x^2}$ o $\left<{f}\right|  \left.  {g}\right> =\int_{-1}^{1}\mathrm{d}x\ \frac{f(x)  \ g(x)}{\sqrt{1-x^2}}$ las bases correspondientes a estas definiciones de producto interno serían distintas.}
\[
\left<\pi_i\right|  \left. \pi_j \right> =\int_{-1}^{1}\mathrm{d}t\ \pi_i(t)\pi_j(t)\,.
\]
\label{EjemploOrtogonalizacion2}

Procederemos  a construir una base ortogonal  $\{\left| P_i \right>\}$, y tomaremos como vector de inicio a $\left|  \pi_{0}\right>$:
\[
\left| P_{0}\right> \equiv \left|  \pi_{0}\right> =1\,.
\]

El siguiente vector será:
\[
\left|  P_{1}\right> \equiv\left|  \pi_1\right>
-\dfrac{\left<\pi_{1}\right.  \left|  P_{0} \right> }{\left<P_{0}\right.  \left|  P_{0}\right> }\left|  P_{0}\right> =t \,\, \Leftarrow \,\, \left\{
\begin{array}{l}
 \left<\pi_{1}\right.  \left|  P_{0}\right> 
=\int_{-1}^{1}\mathrm{d}t\ t=0  \\
\\
\left<P_{0}\right. \left|  P_{0}\right> =
\int_{-1}^{1}\mathrm{d}t=2 
\end{array}
\right.
\] 
El siguiente:
\[
\left|  P_{2}\right> \equiv 
\left|  \pi_{2}\right> 
-\dfrac{\left<\pi_{2}\right.  \left|  P_{1} \right> }{\left<P_{1}\right.  \left|  P_{1}\right> }\left|  P_{1}\right> 
-\dfrac{\left<\pi_{2}\right.  \left|  P_{0}\right> }{\left<P_{0}\right.  \left|  P_{0}\right> }\left| P_{0}\right> =t^{2}-\frac{1}{3} \,\, \Leftarrow \,\, \left\{
\begin{array}{l}
 \left<\pi_{2}\right.  \left|  P_{0}\right> 
=\int_{-1}^{1}\mathrm{d}t\ t^2=\frac23  \\
\\
\left<\pi_{2}\right. \left|  P_{1}\right> =
\int_{-1}^{1}\mathrm{d}t \ t^3=0 \\
\\
\left<P_{1}\right. \left|  P_{1}\right> =
\int_{-1}^{1}\mathrm{d}t\ t^2=\frac23 
\end{array}
\right.
\]

Para el cuarto:
\begin{eqnarray*}
\left|P_{3}\right> &\equiv&\left|  \pi_{3}\right> -\dfrac{\left<\pi_{3}\right.  \left| P_{2}
\right> }{\left<P_{2}\right.  \left|  P_{2}\right> }\left|  P_{2}\right> -\dfrac{\left<\pi_{3}\right.  \left| P_{1}\right> }{\left<P_{1}\right.  \left|  P_{1}\right> }\left| P_{1}\right> -\dfrac{\left<\pi_{3}\right. \left|  P_{0}\right> }{\left<P_{0}\right.
\left|  P_{0}\right> }\left| P_{0}\right> \\
&=&
t^{3}-\frac{3}{5}t \,\, \Leftarrow \,\, \left\{
\begin{array}{l}
 \left<\pi_{3}\right.  \left|  P_{0}\right> 
=\int_{-1}^{1}\mathrm{d}t\ t^3=0  \\
\\
\left<\pi_{3}\right. \left|  P_{1}\right> =
\int_{-1}^{1}\mathrm{d}t \ t^4=\frac25 \\
\\
\left<\pi_{3}\right. \left|  P_{2}\right> =
\int_{-1}^{1}\mathrm{d}t \ t^3[t^2-\frac13]=0 \\
\\
\left<P_{2}\right. \left|  P_{2}\right> =
\int_{-1}^{1}\mathrm{d}t\ [t^2-\frac13]^2=\frac{8}{45} 
\end{array}
\right.
\end{eqnarray*}

Queda como ejercicio comprobar la ortogonalidad de los vectores recién calculados:
\[
\left<P_{0}\right. \left|  P_{1}\right> =
\left<P_{1}\right. \left|  P_{2}\right> =
\left<P_{2}\right. \left|  P_{3}\right> = \cdots = 0 \,.
\]

Esta base ortogonal está formada por los polinomios de Legendre, discutidos en la sección \ref{OrtogonalidadBases}. Es decir, si ortogonalizamos una base de monomios  $\left\{ \left| \pi_i \right> \right\}$ mediante la definición de producto interno: $\left<{f}\right|  \left.  {g}\right> =\int_{-1}^{1}\mathrm{d}x\ f(x)  \ g(x)$, obtendremos la base de polinomios ortogonales de Legendre.

Podemos resumir los cálculos anteriores, construyendo también la base ortonormal a partir de los monomios  $\left\{ \left| \pi_i \right> \right\}$ como se muestra a continuación:
\[
\begin{array}
[c]{|c|ccc|c|}\hline
\left| \pi_{n}\right>  &  & \left| P_{n}\right> &  & \left| \hat{P}_{n}\right> \\ \hline 
1 &  & 1 &  & \frac{1}{\sqrt{2}}\\
t &  & t &  & \sqrt{\frac{3}{2}}\ t\\
t^{2} &  &  t^{2}-\frac{1}{3}  &  & \frac{1}{2}\sqrt{\frac{5}{2}}\left(  3t^{2}-1\right) \\
t^{3} &  & t^{3}-\frac{3}{5}t  &  & \frac{1}{2}\sqrt{\frac{7}{2}}\left(  5t^{3}-3t\right) \\
t^{4} &  &  t^{4}-\frac{6}{7}t^{2}+\frac{3}{35} &  & \frac{3}{8}\sqrt{\frac{1}{2}}\left(  35t^{4}-30t^{2}+3\right) \\
\vdots &  & \vdots &  & \vdots
\end{array}
\]
\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}} 
\subsubsection{Independencia Lineal}
\index{Maxima!Independencia lineal}
\index{Independencia lineal!Maxima}

En \ref{Dependencia lineal} vimos que si en la ecuación 
$
\left|  {0}\right> =C_{1}\ \left|  v_{1}\right> +C_{2}\ \left|  v_{2}\right> +C_{3}\ \left|  v_{3}\right> \cdots+C_{n}\ \left|  v_{n}\right>
$
 con todos los $C_{i}=0$,  entonces se dirá que el conjunto de vectores son linealmente independientes. Para el primer ejemplo de esa sección se obtuvo el siguiente sistema de ecuaciones:
\[
\begin{array}
[c]{ccccc}
C_{1}\  & +2C_{2}\  & -C_{3}\  & = & 0\\
3C_{1} &  & +C_{3}\  & = & 0\\
-C_{1}\  & +C_{2}\  &  & = & 0\\
2C_{1}\  & +3C_{2}\  &  & = & 0
\end{array}
\]
que procedemos a resolver usando el comando {\bf linsolve}:

\noindent
%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([C1+2*C2-C3=0, 3*C1+C3=0, -C1+C2=0, 2*C1+3*C2=0], [C1,C2,C3,C4]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
solve: dependent equations eliminated: (4)\\
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\left[ {\it C_1}=0 , {\it C_2}=0 , {\it C_3}=0 , {\it C_4}={\it \%r_1} \right] 
\end{math}

\subsubsection{Bases para espacios vectoriales}
\index{Maxima!Bases para espacios vectoriales}

En este ejercicio aprenderemos a calcular una base a partir de un conjunto de vectores perteneciente a un determinado espacio vectorial. Por ejemplo, si en  $\mathds{R}^5$ tenemos el siguiente conjunto de vectores:
\[
\left| v_{1}\right>=(1,2,3,4,5)\,, \,\, 
\left| v_{2}\right>=(0,-1,1,2,3)\,,\,\,  
\left| v_{3}\right>=(3,2,1,0,-1)\,, \,\,  
\left| v_{4}\right>=(-4,-3,-2,-1,0) \,.
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
v1:[1,2,3,4,5]; v2:[0,-1,1,2,3]; v3:[3,2,1,0,-1]; v4:[-4,-3,-2,-1,0];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o1) }
\left[ 1,2, 3,4,5 \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o2) }
\left[ 0,-1,1,2,3 \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o3) }
\left[3,2,1,0,-1 \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o4) }
\left[-4,-3,-2,-1,0 \right] 
\end{math}
\newline

Con los vectores dados construimos la siguiente matriz

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M:matrix(v1,v2,v3,v4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o5) }
\begin{pmatrix}1 & 2 & 3 & 4 & 5 \\ 0 & -1 & 1 & 2 & 3 \\ 3 & 2 & 1
  & 0 & -1 \\ -4 & -3 & -2 & -1 & 0 \\ 
 \end{pmatrix}
\end{math}
\newline

Como veremos más adelante, el Rango de una matriz indica el número máximo de vectores fila o columna linealmente independientes. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
rank(M);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o6) }
3
\end{math}
\newline

Podemos aplicar el método de eliminación gaussiana a la matriz ${\bf M}$ para obtener una nueva matriz escalonada. El cálculo además se hace normalizando el primer elemento no nulo de cada fila. Esto se logra con el comando {\bf echelon(M)}.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
echelon(M);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o7) }
\begin{pmatrix}1 & \frac{2}{3} & \frac{1}{3} & 0 & -\frac{1}{3} \\ 
 0 & 1 & -1 & -2 & -3 \\ 0 & 0 & 1 & \frac{5}{3} & \frac{7}{3} \\ 0
  & 0 & 0 & 0 & 0 \\ 
\end{pmatrix}
\end{math}
\newline

Por lo tanto, cada fila de la matriz anterior conformará un conjunto de vectores linealmente independiente. Los podemos aislar de la matriz con el comando  {\bf row(M,i)}, que devolverá la $i-$ésima fila
de la matriz ${\bf M}$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e1:row(%o7,1);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o8) }
\begin{pmatrix}1 & \frac{2}{3} & \frac{1}{3} & 0 & -\frac{1}{3} \\ 
 \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e2:row(%o7,2);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o9) }
\begin{pmatrix}0 & 1 & -1 & -2 & -3 \\ \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e3:row(%o7,3);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o10) }
\begin{pmatrix}0 & 0 & 1 & \frac{5}{3} & \frac{7}{3} \\ 
 \end{pmatrix}
\end{math}
\newline

Es trivial verificar que el conjunto: $\{\mathrm{e}1, \mathrm{e}2, \mathrm{e}3\}$ es linealmente independientes.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve(alpha*[1,2/3,1/3,0,-1/3]+beta*[0,1,-1,-2,-3]+
gamma*[0,0,0,5/3,7/3],[alpha,beta,gamma]);
\end{verbatim}}
\end{minipage}
{\tt solve: dependent equations eliminated: (3)}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o11) }
\left[ \left[ \alpha=0 , \beta=0 , \gamma=0 \right]  \right]
\end{math}
\newline


Consideremos los vectores ${\bf a}=(1,3)$ y ${\bf b}=(-1,1)$ ¿Serán linealmente independientes?

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve(alpha*[1,3]+beta*[-1,1],[alpha,beta]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o12) }
\left[ \left[ \alpha=0 , \beta=0 \right]  \right] 
\end{math}
\newline

Los vectores ${\bf a}=(1,2,3)$ y ${\bf b}=(4,8,12)$ ¿Serán linealmente independientes?

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve(alpha*[1,2,3]+beta*[4,8,12],[alpha,beta]);
\end{verbatim}}
\end{minipage}
{\tt solve: dependent equations eliminated: (2 3)}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o13) }
\left[ \left[ \alpha=-4\,{\it \%r_4} , \beta={\it \%r_4} \right]   \right] 
\end{math}
\newline

Aquí $\alpha=-4\beta$.

Sea ahora $\{{\bf e}_i\}=\{(1,1,1),(1,2,1),(0,0,2)\}$ una base para $\mathds{R}^3$. Vamos a calcular  las componentes del vector ${\bf a}=(3, 2, 1)$ respecto de esa base.

Primero verifiquemos si efectivamente es una base:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve(alpha*[1,1,1]+beta*[1,2,1]+gamma*[0,0,2],[alpha,beta,gamma]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\left[ \left[ \alpha=0 , \beta=0 , \gamma=0 \right]  \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve([3,2,1]-ax*[1,1,1]-ay*[1,2,1]-az*[0,0,2],[ax,ay,az]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
$$\left[ \left[ ax=4 , ay=-1 , az=-1 \right]  \right] 
\end{math}
\newline

Por lo tanto, ${\bf a}=4{\bf e}_1-{\bf e}_2-{\bf e}_3$. 

En la base canónica las componentes del vector {\bf a} serán:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve([3,2,1]-ax*[1,0,0]-ay*[0,1,0]-az*[0,0,1],[ax,ay,az]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
$$\left[ \left[ ax=3 , ay=2 , az=1 \right]  \right] 
\end{math}
\newline

Es decir, ${\bf a}=3{\bf i}+2{\bf j}+{\bf k}$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}

\subsubsection{Ortogonalización con Maxima}
\index{Maxima!Ortogonalización}
\index{Ortogonalización}

Anteriormente hicimos los cálculos para hallar una base ortogonal a partir del siguiente conjunto de vectores linealmente independientes:
\[
\left|  v_{1}\right> =\left(
\begin{array}
[c]{r}
1\\
3\\
-1\\
2
\end{array}
\right)  ;\quad\left|  v_{2}\right> =\left(
\begin{array}
[c]{c}
2\\
0\\
1\\
3
\end{array}
\right)  ;\quad\left|  v_{3}\right> =\left(
\begin{array}
[c]{r}
-1\\
1\\
0\\
0
\end{array}
\right) \,.
\]

La función {\bf gramschmidt}(x) es la que ejecuta el algoritmo de ortogonalización de Gram-Schmidt sobre el objeto x. Si x es una matriz el algoritmo actúa sobre sus filas.  
Antes de usar la función es  necesario cargar la librería {\bf eigen}. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load("eigen")$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
v:matrix ([-1, 1, 0, 0], [2, 0, 1,3], [1,3, -1,2]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\begin{pmatrix}-1 & 1 & 0 & 0 \\ 2 & 0 & 1 & 3 \\ 1 & 3 & -1 & 2
  \\ \end{pmatrix}
\end{math}
\newline

Notemos que el vector $\left| v_{3}\right>$ es el que hemos puesto en la primera fila de la matriz. Ahora procedemos al cálculo:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e:gramschmidt(v);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ \left[ -1 , 1 , 0 , 0 \right] , \left[ 1 , 1 , 1 , 3\right]  , 
\left[ \frac{5}{2^2} , \frac{5}{2^2} , -\frac{7}{2^2} ,  -\frac{1}{2^2} \right]  \right] 
\end{math}

El resultado viene en forma de una lista: 
$\left[\textrm{e}[1], \textrm{e}[2],\textrm{e}[3] \right]$. Simplificando resulta:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e:expand(%);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\left[ \left[ -1 , 1 , 0 , 0 \right]  , \left[ 1 , 1 , 1 , 3\right]  , 
\left[ \frac{5}{4} , \frac{5}{4} , -\frac{7}{4} , - \frac{1}{4} \right]  \right] 
\end{math}

Podemos verificar que son ortogonales:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
map(innerproduct, [e[1], e[2], e[3]], [e[2], e[3], e[1]]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ 0 , 0 , 0 \right]
\end{math}

Normalizamos ahora cada uno de los vectores:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e[1]/sqrt(innerproduct(e[1],e[1])); 
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\left[ -\frac{1}{\sqrt{2}} , \frac{1}{\sqrt{2}} , 0 , 0 \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e[2]/sqrt(innerproduct(e[2],e[2])); 
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\left[ \frac{1}{2\,\sqrt{3}} , \frac{1}{2\,\sqrt{3}} , \frac{1}{2\,\sqrt{3}} , \frac{\sqrt{3}}{2} \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e[3]/sqrt(innerproduct(e[3],e[3])); 
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\left[ \frac{1}{2} , \frac{1}{2} , -\frac{7}{10} , -\frac{1}{10}\right] 
\end{math}
\newline

La función {\bf gramschmidt}(x,F) nos permite definir un producto interno diferente a {\bf innerproduct}. Veamos como se hace para el otro ejemplo donde el conjunto de vectores linealmente independientes estaba dado por: $\left\{1, t, t^{2}, t^{3}, \cdots, t^{n}\right\}$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
producto(f,g):=integrate(f*g, t, a, b);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\mbox{producto(f,g)}:=\int_a^b f g \ \textrm{d}t 
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e:gramschmidt ([1, t, t^2,t^3,t^4], producto), a= -1, b=1;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\left[ 1 , t , \frac{3\,t^2-1}{3} , \frac{t\,\left(5\,t^2-3\right) }{5} , \frac{35\,t^4-30\,t^2+3}{35} \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e:expand(e);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\left[ 1 , t , t^2-\frac{1}{3} , t^3-\frac{3\,t}{5} , t^4-\frac{6\, t^2}{7}+\frac{3}{35} \right] 
\end{math}
\newline

Verifiquemos si son ortogonales:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
map(producto, [e[1], e[2], e[3]], [e[2], e[3], e[1]]), a= -1, b=1;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\left[ 0 , 0 , 0 \right]
\end{math}
\newline

Para normalizar hacemos lo siguiente:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
a: -1$ b:1$
\end{verbatim}}
\end{minipage}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e[1]/sqrt(producto(e[1], e[1]));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\frac{1}{\sqrt{2}}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e[2]/sqrt(producto(e[2], e[2]));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
\frac{\sqrt{3}\,t}{\sqrt{2}}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e[3]/sqrt(producto(e[3], e[3]));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
\frac{3\,\sqrt{5}\,\left(t^2-\frac{1}{3}\right)}{2^{\frac{3}{2}}}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e[4]/sqrt(producto(e[4], e[4]));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
\frac{5\,\sqrt{7}\,\left(t^3-\frac{3\,t}{5}\right)}{2^{\frac{3}{2}} }
\end{math}
\newline

\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}

\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Diga si los siguientes conjuntos
de vectores en $\mathcal{P}^{3}$ son o no  linealmente independientes.
\begin{enumerate}
\item  $\left|{x}_1\right>  =2x\,, \quad \left| {x}_2 \right>  = x^{2}+1\,, \quad 
\left| {x}_3\right>  = x+1\,, \quad \left| {x}_4\right>  =x^{2}-1$.

\item  $\left| {x}_1\right>  = x(x-1)\,, \quad \left| {x}_2 \right>  =x\,, \quad 
\left| {x}_3 \right>  =x^{3}\,, \quad \left|{x}_4\right>  =2x^{3}-x^{2}$.
\end{enumerate}

\item  Probar que los polinomios:
$\left| {x}_1 \right> =1\,,\,\, \left| {x}_2 \right> =x\,,\,\, \left| {x}_3 \right> = \frac{3}{2}x^{2} - \frac{1}{2}$ y $ \left| {x}_4 \right> = \frac{5}{2}x^{3}-\frac{3}{2}x$,   forman una base en $\mathcal{P}_{4}$.  Expresar $\left| {p} \right> = x^{2}\,,\,\, \left| {q} \right> = x^{3}$ en función de esa base.

\item  Encontrar la proyección perpendicular de los siguientes vectores en
$\mathcal{C}_{[-1,1]}$ (espacio de funciones continuas en el intervalo [-1,1]) al subespacio generado por los polinomios: $\{1, x, x^{2}-1\}$. Calcular la distancia de cada una de estas funciones al subespacio mencionado.

\begin{enumerate}
\item $f(x)=x^{n}\,,\,\,  n$ entero.
\item $f(x)=\mbox{sen}(x)$.
\item $f(x)=3x^{2}$.
\end{enumerate}

\item Utilizando el método de Gram-Schmidt encuentre una base ortonormal para los siguientes conjuntos de vectores:
\begin{enumerate}
\item $\left| v_1 \right>= (1, 0, 1)\,, \left| v_2 \right>= (0, 1, 1)$  y   $\left| v_3 \right>= (1, 0, 0) $.  En $\mathds{R}^3$.
\item $\left| v_1 \right>= (2, -4, 5, 2)\,,  \left| v_2 \right>= (-6, 5, -1, -6)$  y   $\left| v_3 \right>= (-10, 13, -4, -3)$.  En $\mathds{R}^4$.
\end{enumerate}

\item Utilizando {\bf Maxima} reproduzca el ejemplo 3 que expusimos en la página \pageref{EjemploOrtogonalizacion2}. Es decir, suponga el espacio de polinomios, $\mathcal{P}^{n}$, de grado $g\leq n$ definidos en el intervalo $\left[-1,1\right]$. Este espacio vectorial tendrá como una de las posibles bases al conjunto  $\left\{ \left| \pi_i \right> \right\} = \left\{ 1, t, t^{2}, t^{3}, \cdots, t^{n}\right\}$, pero en este caso con el producto interno  definido por:  $\left<{f}\right|  \left.  {g}\right> =\int_{-1}^{1}\mathrm{d}x\ f(x)  \ g(x)\sqrt{1-x^2}$. Encuentre la base ortogonal correspondiente. A esta nueva base se le conoce como polinomios de Chebyshev de segunda especie\footnote{\url{https://en.wikipedia.org/wiki/Chebyshev_polynomials}}.  

\item Otra vez, utilizando {\bf Maxima}, reproduzca el ejercicio anterior, pero con la definición de producto interno:  $\left<{f}\right|  \left.  {g}\right> =\int_{-1}^{1}\mathrm{d}x\ \frac{f(x)  \ g(x)}{\sqrt{1-x^2}}$.  A esta nueva base se le conoce como polinomios de Chebyshev de primera especie. 

\item Resuelva los problemas 1-4 utilizando  {\bf Maxima}.

\end{enumerate}

\section{Aproximación de funciones}
\label{AproximacionFunciones}
\index{Aproximación de funciones}

Armados de los conocimientos previos podemos pasar a aplicarlos en un intento de aproximar funciones. La aproximación de una función tiene varias facetas y seguramente en cursos anteriores hemos hecho este tipo de aproximaciones una buena cantidad de veces cuando necesitábamos convertir una expresión, que nos resultaba muy complicada, en otras más sencillas y casi equivalentes. Por ejemplo, cuando aplicamos la aproximación lineal: $f(x)\approx f(x_0)+f'(x_0)(x-x_0)$, con $x$ muy cercano a $x_0$ ¿Cuántas veces no hemos utilizado la aproximación: $\mbox{sen}(x)\approx x$? 
También aproximamos funciones cuando en el laboratorios nos veíamos en la necesidad de ``ajustar con la mejor curva'' una serie de puntos experimentales. 

Consideremos primero algunos conceptos importantes.

\subsection{Complementos ortogonales y descomposición ortogonal}
\label{ComplementosOrtogonales}
\index{Ortogonal!Complemento}
\index{Ortogonal!Descomposición}
\index{Descomposición ortogonal}
Sea un subespacio $\textbf{\em S}\subset\textbf{\em V}$. Un elemento $\left| {\bar{v}}_{i}\right> \in\textbf{\em V}$ se dice ortogonal a
$\textbf{\em S}$ si $\ \left<{s}_{k}\right.  \left|  {\bar v}_{i}\right> =0
\,\,\forall \,\, \left|  {s}_{k}\right> \in\textbf{\em S}$,  es decir, es ortogonal a todos los elementos de $\textbf{\em S}$. El conjunto $\left\{ \left|  {\bar{v}}_{1}\right>, \left|  {\bar{v}}_{2}\right>, \left|  {\bar{v}}_{3}\right>, \cdots ,\left|{\bar{v}}_{m}\right> \right\}$ de todos los elementos ortogonales a $\textbf{\em S}$, se denomina $S-$perpendicular y se denota como $\textbf{\em S}^{\bot}$. Es fácil demostrar que $\textbf{\em S}^{\bot}$ es un subespacio, aún si $\textbf{\em S}$ no lo es.


Dado  un espacio euclidiano de dimensión infinita $\textbf{\em V}:\left\{  \left|  v_{1}\right>, \left|  v_{2}\right>, \left|  v_{3}\right>, \cdots ,\left|{v}_{n}\right>, \cdots\right\}$ y un subespacio $\textbf{\em S}\subset\textbf{\em V}$ con dimensión finita: $\dim\textbf{\em S}=m$. Entonces, $\forall\ \left|{v}_{k}\right> \in\textbf{\em V}$ puede expresarse como la suma de dos vectores $\left|  {s}_{k}\right> \in\textbf{\em S}\,\,\wedge \,\, \left|  {s}_{k}\right> ^{\bot}\in\textbf{\em S}^{\bot}$. Esto es:
\[
\left|  v_{k}\right> =\left|  {s}_{k}\right>
+\left|  {s}_{k}\right> ^{\bot} \,,\quad\left|  {s}_{k}\right> \in\textbf{\em S}\quad\wedge\quad\left|  {s}_{k}\right> ^{\bot}\in\textbf{\em S}^{\bot} \,.
\]
La norma de $\left|  v_{k}\right> $ se calcula a través del teorema de Pitágoras generalizado:
\[
\left\|  \left|  v_{k}\right> \right\|^{2}=\left\|  \left| {s}_{k}\right> \right\|^{2}+\left\|  \left|  {s}_{k}\right> ^{\bot}\right\|^{2}\,.
\]

La demostración es sencilla. Primero se prueba que la descomposición ortogonal $\left|  v_{k}\right> =\left|  {s}_{k}\right> +\left|  {s}_{k}\right> ^{\bot}$ es siempre posible. Para ello recordamos que $\textbf{\em S}\subset\textbf{\em V}$ es dedimensión finita, por lo tanto existe una base ortonormal $\left\{\left|  \mathrm{\hat{e}}_{1}\right>, \left|  \mathrm{\hat{e}}_{2}\right>, \left|  \mathrm{\hat{e}}_{3}\right>, \cdots , \left|  \mathrm{\hat{e}}_{m} \right> \right\}$ para $\textbf{\em S}$. Es decir, dado un $\left|{v}_{k}\right>$ definimos los elementos $\left|  {s}_{k}\right> $ y $\left|  {s}_{k}\right> ^{\bot}$ como sigue: 
\[
\left|  {s}_{k}\right> =\sum_{i=1}^{m}\left<{v}_{k}\right.  \left|  \mathrm{\hat{e}}_{i}\right> \left|  \mathrm{\hat{e}}_{i}\right> \quad\wedge\quad\left|  {s}_{k}\right> ^{\bot}=\left|  v_{k}\right> -\left|  {s}_{k}\right> \,.
\]

Nótese que $\left<{v}_{k}\right.  \left|  \mathrm{\hat{e}}_{i}\right> \left|  \mathrm{\hat{e}}_{i}\right>$ es la proyección de $\left|  v_{k}\right> $ a lo largo de $\left| \mathrm{\hat{e}}_{i}\right> $ y $\left|  {s}_{k}\right>$ se expresa como la combinación lineal de la base de $\textbf{\em S}$, por lo tanto,  está en $\textbf{\em S}$. Por otro lado:
\[
^{\bot}\left<{s}_{k}\right.  \left|  \mathrm{\hat{e}}_{i}\right> =\left<{v}_{k}{-s}_{k}\right.  \left|  \mathrm{\hat{e}}_{i}\right> =\left<{v}_{k}\right.  \left|  \mathrm{\hat{e}}_{i}\right> -\left<{s}_{k}\right.  \left|  \mathrm{\hat{e}}_{i}\right> =\left<{v}_{k}\right.  \left|  \mathrm{\hat{e}}_{i}\right> -\left[  \sum_{j=1}^{m}\left<{v}_{k}\right.\left|  \mathrm{\hat{e}}_{j}\right> \left<\mathrm{\hat{e}}_{j}\right| \right]  \left|  \mathrm{\hat{e}}_{i}\right> =0\,\, \Rightarrow \,\,\left|{s}_{k}\right> ^{\bot}\ \bot\ \left|  \mathrm{\hat{e}}_{j} \right> \,,
\]
lo cual indica que $\left|  {s}_{k}\right> ^{\bot}\ \in \textbf{\em S}^{\bot}$. 

Podemos ir un poco más allá y ver que la descomposición $\left| v_{k}\right> =\left|  {s}_{k}\right> +\left| {s}_{k}\right> ^{\bot}$ es única en $\mathbf{V}$. Para ello suponemos que existen dos posibles descomposiciones, vale decir:
\[
\left|  v_{k}\right> =\left|  {s}_{k}\right> +\left|  {s}_{k}\right> ^{\bot}\,\, \wedge \,\,\left| v_{k}\right> =\left|  {t}_{k}\right> +\left|
{t}_{k}\right> ^{\bot}\,, \quad\text{con }\left|  {s}_{k}\right> \wedge\left|  {t}_{k}\right> \in\textbf{\em S}
\,\, \wedge \,\, \left|  {s}_{k}\right> ^{\bot}\,\, \wedge \,\,\left| {t}_{k}\right> ^{\bot}\in\textbf{\em S}^{\bot}\,.
\]
Por lo tanto:
\[
\left|  v_{k}\right> -\left|  v_{k}\right>=\left(  \left|  {s}_{k}\right> +\left|  {s}_{k}\right> ^{\bot}\right)  -\left(  \left|  {t}_{k}\right>+\left|  {t}_{k}\right> ^{\bot}\right)  =0\,\, \Rightarrow \,\,\left|  {s}_{k}\right> -\left|  {t}_{k}\right>=\left|  {t}_{k}\right> ^{\bot}-\left|  {s}_{k}\right> ^{\bot} \,.
\]


Pero $\left|  {s}_{k}\right> -\left|  {t}_{k}\right> \in \textbf{\em S}$, es decir, ortogonal a todos los elementos de $\textbf{\em S}^{\bot}$ y $\left|  {s}_{k}\right> -\left|  {t}_{k}\right> =\left|  {t}_{k}\right> ^{\bot}-\left| {s}_{k}\right> ^{\bot}$. Con lo cual $\left|  {s}_{k}\right> -\left|  {t}_{k}\right> \equiv\left| {0}\right>$, que es el único elemento que es ortogonal a si mismo y en consecuencia la descomposición $\left|  v_{k}\right> =\left|  {s}_{k}\right> +\left|  {s}_{k}\right> ^{\bot}$ es única. 

Finalmente, con la definición de norma:
\[
\left\|  \left|  v_{k}\right> \right\|^{2}=\left\|  \left|{s}_{k}\right> +\left|  {s}_{k}\right> ^{\bot}\right\|^{2}=\left(  \left<{s}_{k}\right|  +\left<{s}_{k}\right|  ^{\bot}\right)  \left(  \left|  {s}_{k}\right> +\left|  {s}_{k}\right> ^{\bot}\right) =\left<{s}_{k}\right.  \left|  {s}_{k}\right>
+^{\bot}\left<{s}_{k}\right.  \left|  {s}_{k}
\right> ^{\bot} = \left\|  \left|  {s}_{k}\right> \right\|^{2}+\left\|  \left|  {s}_{k}\right> ^{\bot}\right\|^{2}\,.
\]

Así, dado $\textbf{\em S}^{m}$ un subespacio de $\textbf{\em V}$ de dimensión finita y dado un $\left|  v_{k}\right> \in\textbf{\em V}$ el elemento:
\[
\left|  {s}_{k}\right> \in\textbf{\em S} \,\, \Rightarrow \,\, \left| {s}_{k}\right> =
\sum_{i=1}^{m}\left<{v}_{k}\right.\left|  \mathrm{e}_{i}\right> \left|  \mathrm{e}_{i}\right> \,,
\]
será la proyección de $\left|  v_{k}\right>$ en $\textbf{\em S}$.

En general, dado un vector $\left| x \right> \in\textbf{\em V}$ y un subespacio de $\textbf{\em V}$ con dimensión finita, $\textbf{\em S}^{m}\subset\textbf{\em V}$ y $\dim\textbf{\em S}=m$, entonces la distancia de $\left| x \right>
$ a $\textbf{\em S}^{m}$ es la norma de la componente de $\left| x \right>$,  perpendicular a $\textbf{\em S}^{m}$.  Más aún, esa distancia será mínima y $\left|{x}\right>_{\textbf{\em S}^{m}}$ la proyección de $\left| x \right> $, en $\textbf{\em S}^{m}$ será el elemento de $\textbf{\em S}^{m}$ más próximo a $\left|  x\right>$ y, por la mejor aproximación.


\subsection{Condiciones para la aproximación de funciones}
\label{CondicionesAproximacionFunciones}
Sea $\textbf{\em V}=\left\{  \left|  v_{1}\right>, \left|  v_{2}\right>, \left|  v_{3}\right>, \cdots ,\left|{v}_{n}\right>,\cdots \right\}$ un espacio euclidiano de dimensión infinita, y  un subespacio $\textbf{\em S}^{m}\subset\textbf{\em V}$, con dimensión finita, $\dim\textbf{\em S}=m$, y sea un elemento $\left|  v_{i}\right> \in\mathbf{V.}$ La proyección de $\left|  v_{i}\right> $ en $\textbf{\em S}^{m},\left|{s}_{i}\right> ,$ será el elemento de $\textbf{\em S}^{m}$ más próximo a $\left|  v_{k}\right> $. En otras palabras:
\[
\left\|  \left|  v_{i}\right> -\left|  {s}_{i}\right> \right\|  \leq 
\left\|  \left|  v_{i}\right>-\left|  {t}_{i}\right> \right\|  \,\, \forall \,\, \left|{t}_{i}\right> \in\textbf{\em S}\,.
\]

La demostración se sigue así:
\[
\left|  v_{i}\right> -\left|  {t}_{i}\right>
= \left(  \left|  v_{i}\right> -\left|  {s}_{i}\right> \right)  +\left(  \left|  {s}_{i}\right>-\left|  {t}_{i}\right> \right)  \,\, \Rightarrow \,\,\left\|\left|  v_{i}\right> -\left|  {t}_{i}\right>\right\|^{2}=
\left\|  \left|  v_{i}\right> -\left|{s}_{i}\right> \right\|^{2}+\left\|  \left|  {s}_{i}\right> -\left|  {t}_{i}\right> \right\|^{2}\,,
\]
ya que $\left|  v_{i}\right> -\left|  {s}_{i}
\right> =\left|  {s}_{k}\right> ^{\bot}\in\textbf{\em S}^{\bot
}\,\, \wedge \,\,\left|  {s}_{i}\right> -\left|  {t}\
_{i}\right> \in\textbf{\em S}$,  y vale el teorema de Pitágoras
generalizado. 

Ahora bien, como:
\[
\left\|  \left|  {s}_{i}\right> -\left|  {t}_{i}\right> \right\|^{2} \geq 0 \,\, \Rightarrow \,\, 
\left\|  \left| v_{i}\right> -\left|  {t}_{i}\right> \right\|^{2}\geq\left\|  \left|  v_{i}\right> -\left|  {s}_{i}\right> \right\|^{2}\,\, \Rightarrow \,\, 
\left\|  \left| v_{i}\right> -\left|  {t}_{i}\right> \right\| \geq \left\|  \left|  v_{i}\right> -\left|  {s}_{i}\right> \right\| \,. \quad \blacktriangleleft 
\]

En la sección \ref{OrtogonalidadBases} consideramos la expansión de funciones continuas, reales de variable real,  definidas en $\left[  0,2\pi\right]$, $\mathcal{C}_{\left[ 0,2\pi\right]  }^{\infty}$, mediante funciones trigonométricas y con el producto interno definido por:
\[ 
\left<{f}\right|  \left.{g}\right> =\int_{0}^{2\pi}\mathrm{d}x\ f(x) \ g(x)\,.
\] 
En ese entonces consideramos, para ese espacio vectorial, una base ortogonal definida por:
\[
\left|  \mathrm{e}_{0}\right> =1, \quad  \left|  \mathrm{e}_{2n-1} \right> = \cos(nx) \quad \text{y} \quad \left|  \mathrm{e}_{2n}\right>=\mbox{sen}(nx)\,, \quad \text{con }n=1,2,3,\cdots
\]

Por lo tanto, cualquier función definida en el intervalo $\left[0, 2\pi\right]$ puede expresarse en términos de esta base como mostramos a continuación:
\[
\left|  {f}\right> =\sum_{i=1}^{\infty} C_{i} \left| \mathrm{{e}}_{i}\right>\,.
\]

Los $C_{i}$ son los coeficientes de Fourier. Es decir, cualquier función puede ser expresada como una serie de Fourier de la forma:
\[
f(x)  =\frac{1}{2}a_{0}+\sum_{k=1}^{\infty} \left[ a_{k}\cos(kx)+b_{k}\mbox{sen}(kx)\right]\,,
\]
donde:
\[
a_{k}=\frac{1}{\pi}\int_{0}^{2\pi}\mathrm{d}x\ f(x)  \cos(kx)\,\, \wedge \,\, 
b_{k}=\frac{1}{\pi}\int_{0}^{2\pi}\mathrm{d}x\ f(x)\mbox{sen}(kx)\,.
\]

Es claro que para la aproximación de funciones por funciones trigonométricas cuyos coeficientes son los coeficientes de Fourier constituyen la mejor aproximación. Por lo tanto, de todas las funciones $\mathcal{F}(x)  \in\mathcal{C}_{\left[  0,2\pi\right]  }^{\infty}$ las funciones trigonométricas, $\mathcal{T}(x)$ minimizan la desviación cuadrática media: 
\[
\int_{0}^{2\pi}\mathrm{d}x\ \left(  f(x)  -\mathcal{P}(x)  \right)^{2}\geq\int_{0}^{2\pi}\mathrm{d}x\ \left( f(x)  -\mathcal{T}(x)  \right)^{2} \,.
\]

\subsection{El método de mínimos cuadrados}
\label{MetodosMinimosCuadrados}
\index{Método de mínimos cuadrados}
\index{Mínimos cuadrados!Método de}
Una de las aplicaciones más importantes en la aproximación de funciones es el método de mínimos cuadrados. La idea es determinar el valor más aproximado de una cantidad física, \textsf{c}, a partir de un
conjunto de medidas experimentales: $\left\{  x_{1},x_{2},x_{3},\cdots ,x_{n}\right\}$. Para ello asociamos ese conjunto de medidas con las componentes de un vector $\left| x \right> \equiv \left(  x_{1},x_{2},x_{3},\cdots ,x_{n}\right) $ en $\mathds{R}^{n}$ y supondremos que su mejor aproximación, que llamaremos $\textsf{c} \left|1 \right>  \equiv \left(  \textsf{c},\textsf{c},\textsf{c},\cdots ,\textsf{c}\right) $,  será la proyección perpendicular de $\left| x \right>$ (las medidas) sobre el subespacio generado por $\left| 1 \right>  $. Esto es:
\[
\left| x \right> = \textsf{c}\left| 1 \right> \,\, \Rightarrow \,\, \textsf{c}=\frac{\left<{x}\right.  \left| 1 \right> }{\left<1\right.  \left|1 \right> }=\frac{x_{1}+x_{2}+x_{3},\cdots+x_{n}}{n}\,, 
\]
que no es otra cosa que construir -de una manera sofisticada-  el promedio aritmético de las medidas. Es claro que la proyección perpendicular de $\left| x \right>$ sobre $\left| 1 \right>$ hace mínima la distancia entre el subespacio perpendicular generado por $\left| 1 \right>$ y el vector $\left| x \right>$, con ello también hará mínimo su cuadrado:
\[
\left[  d\left(  \left| x \right> ,\textsf{c}\left| 1 \right> \right)  \right]^{2} =
\left\|  \left| x-\textsf{c}\right> \right\|^{2}=\left< x-\textsf{c}\right.  \left| x- \textsf{c} \right> =\sum_{i=1}^{n}\left(x_{i} -\textsf{c}\right)^{2}\,.
\]

Esta manera sofisticada, que se deriva del formalismo utilizado, muestra el significado del promedio aritmético como medida más cercana al valor ``real'' de una cantidad obtenida a partir de una serie de medidas experimentales.  

Obviamente, este problema se puede generalizar para el caso de dos (o $n$) cantidades si extendemos la dimensión del espacio y los resultados experimentales se expresarán como un vector de $2n$ dimensiones
\[
\left| x \right> =\left(  x_{11},x_{12},x_{13},\cdots
x_{1n},x_{21},x_{22},x_{23},\cdots x_{2n}\right)\,,
\]
mientras que los vectores que representan las cantidades más aproximadas serán:
\[
\textsf{c}_{1}\left| 1_{1}\right> =
\left( \underset{n}{\underbrace{\textsf{c}_{1}\text{\textsf{,c}}_{1} \text{\textsf{,c}}_{1} \text{\textsc{,}}\cdots,\textsf{c}_{1}},}\underset{n}{\underbrace{\; 0,0,0,\cdots0} }\right)  
\quad \wedge \quad  \textsf{c}_{2} \left|  {1}_{2}\right> =
\left(  0,0,0,\cdots,0,\; \textsf{c}_{2} ,\textsf{c}_{2} \textsf{c}_{2},\cdots\textsf{c}_{2} \right)\,.
\]

Ahora $\left\{ \left|  1_{1}\right> , \left|  1_{2}\right> \right\}$ expanden un subespacio vectorial sobre el cual $\left| x \right>$ tiene como proyección ortogonal a:
\textsf{c}$_{1} \left|  1_{1}\right> +$\textsf{c}$_{2} \left| 1_{2}\right>$ y consecuentemente $\left|  {x-}\textsf{c}_{1} 1_{1} -\textsf{c}_{2} 1_{2}\right> $ será perpendicular a $\left\{  \left|  1_{1}\right> ,\left| 1_{2}\right> \right\}$. Por lo tanto:
\[
\textsf{c}_{1} =\frac{\left<{x}\right.  \left| 1_{1}\right> }{\left< 1_{1}\right.  \left|
1_{1}\right> } = \frac{x_{11}+x_{12}+x_{13},\cdots+x_{1n}}
{n} \quad \wedge \quad \textsf{c}_{2} =\frac{\left< x \right.  \left|  1_{2}\right> }{\left< 1_{2}\right.  \left|  1_{2}\right> }=\frac{x_{21}+x_{22}+x_{23},\cdots+x_{2n}}{n}\,.
\]

Quizá la consecuencia más conocida de esta forma de aproximar funciones es el ``ajuste'' de un conjunto de datos experimentales 
$\left\{  \left( x_{1},y_{1}\right), \left(  x_{2},y_{2}\right), \left(  x_{3},y_{3}\right), \cdots, \left(  x_{n},y_{n}\right)  \right\} $ a la ecuación de una recta
$y=\textsf{c}x$. En este caso, el planteamiento del problema se reduce a encontrar el vector \textsf{c}$ \left| x \right>$, en el subespacio $\textbf{\em S}\left(  \left| x \right> \right)$, que esté lo más cercano posible al vector $\left| y \right>$. 

Como en el caso anterior, queremos que la distancia entre $\left|y\right> $ y su valor más aproximado $\textsf{c}\left|x\right>$ sea la menor posible. Por lo tanto,
$\left\|  \left|  \textsf{c}x-y\right> \right\|^{2}$ será la menor posible y $\left|  {\textsf{c}} {x-y}\right> $ será perpendicular a $\textbf{\em S}\left(\left| x \right> \right)$, por lo que:
\[
\left<{x}\right.  \left|  {\textsf{c}} {x-y}\right> =0 \,\, \Rightarrow \,\, {\textsf{c}} =\frac{\left<{x}\right.  \left| y \right> }{\left< x \right. \left| x\right> }=
\frac{x_{1}y_{1}+x_{2}y_{2}+x_{3}y_{3}+ \cdots+x_{n}y_{n}}{x_{1}^{2}+x_{2}^{2}+x_{3}^{2}+ \cdots+x_{n}^{2}}\,.
\]

Si la recta a ``ajustar'' es $y=\textsf{c}x +\textsf{b}$ el procedimiento será uno equivalente: proyectar sobre los vectores y obtener ecuaciones. Si representamos $\left| \textsf{b} \right> = \textsf{b} \left| 1 \right> $, tendremos:
\[
\label{MinimoCuadGeneral}
\left|y\right> = \textsf{c}\left| x \right> +\left|\textsf{b}\right>\,\, \Rightarrow \,\,
\left\{
\begin{array}{l}
  \left< x\right.\left|y\right>   =  \textsf{c} \left< x\right.\left|x\right> +   \left< x\right.\left| \textsf{b}\right> \,\, \Rightarrow \,\, 
  \sum_{i=1}^{n}x_{i}y_{i} = \textsf{c}  \sum_{i=1}^{n}x_{i}^{2} + \textsf{b}\sum_{i=1}^{n}x_{i} \\
    \\      
\left< \textsf{b}\right.\left|y\right>  =  \textsf{c} \left< \textsf{b}\right.\left|x\right> +   \left< \textsf{b}\right.\left|\textsf{b}\right>  \,\, \Rightarrow \,\,
 \sum_{i=1}^{n} y_{i} = \textsf{c}  \sum_{i=1}^{n}x_{i} + \textsf{b}n
\end{array}
\right.  
\]

Que no es otra cosa que un sistema de dos ecuaciones con dos incógnitas: $\textsf{c}$ y $\textsf{b}$. Por simplicidad y conveniencia continuaremos con el análisis del caso $\textsf{b} = 0$, vale decir, aproximando las medidas experimentales a una recta que pase por el origen de coordenadas $y=\textsf{c}x$. El lector podrá extender el procedimiento para caso $\textsf{b} \neq 0$. 

Para tratar de entender (y extender) lo antes expuesto, consideremos tres ejemplos que muestran la versatilidad del método y la ventaja de disponer de una clara notación. Primeramente, mostraremos el caso  más  utilizado para construir el mejor ajuste lineal a un conjunto de datos experimentales. Buscaremos la mejor recta que describe ese conjunto de puntos. Luego mostraremos la aplicación del método para buscar la mejor función multilineal, es decir, que ajustaremos la mejor función de $n$ variables con una contribución lineal de sus argumentos: 
$y = y(x_{1}, x_{2}, \cdots , x_{n} ) = \textsf{c}_{1}x_{1} + \textsf{c}_{2}x_{2} + \cdots +\textsf{c}_{n}x_{n}$. Este caso tendrá una complicación adicional, por cuando realizaremos varias mediciones de las variables. En los ejemplos de la sección \ref{EjemplosMinimosCuadrados} analizaremos varios casos en los cuales extendemos el método.

%%%%%%%
\subsection{Interpolación polinomial de puntos experimentales}
\label{InterpolacionPolinomialPuntosExperimentales}
\index{Interpolación polinomial puntos experimentales}
\index{Polinomial!Interpolación de puntos experimentales}
\index{Puntos Experimentales!Interpolación polinomial}

Muchas veces nos encontramos con la situación en la cual tenemos un conjunto de (digamos $n$) medidas o puntos experimentales $\{ (x_{1},y_{1} ),(x_{2},y_{2}),\cdots , (x_{n},y_{n}) \}$ y para modelar ese experimento quisiéramos una función que ajuste estos puntos, de manera que: $\{ (x_{1},y_{1} = f(x_{1}) ),(x_{2},y_{2} = f(x_{2}) ),\cdots , (x_{n},y_{n} = f(x_{n}) ) \}$.

El tener una función nos provee la gran ventaja de poder intuir o aproximar los puntos que no hemos medido. La función candidata más inmediata es un polinomio y debemos definir el grado del polinomio y la estrategia que aproxime esos puntos. Puede ser que no sea lineal el polinomio y queramos ajustar esos puntos a un polinomio tal que éste pase por los puntos experimentales. Queda entonces por decidir la estrategia. Esto es, si construimos la función como ``trozos'' de polinomios que ajusten a subconjuntos:  $\{ (x_{1},y_{1} = f(x_{1})),(x_{2},y_{2}= f(x_{2})),\cdots , (x_{m},y_{m} = f(x_{m})) \}$ con $m < n$ de los puntos experimentales.  En este caso tendremos una función de interpolación para cada conjunto de puntos. También podemos ajustar la función a todo el conjunto de puntos experimentales y, en ese caso el máximo grado del polinomio que los interpole será de grado $n -1$.  

Para encontrar este polinomio lo expresaremos como una  combinación lineal de polinomios de Legendre, una base ortogonal que hemos discutido con anterioridad en la secciones \ref{OrtogonalidadBases} y \ref{EjemplosDependenciaLineal}. Esto es:
\[
f(x) =  \sum_{k=0}^{n-1} C_{k} \left|P_{k}\right> =  \sum_{k=0}^{n-1} C_{k} P_{k}(x) 
\,\, \Rightarrow \,\,
\left\{ 
\begin{array}{l}
  y_{1} = f(x_{1})  =  C_{0} P_{0}(x_{1}) +C_{1} P_{1}(x_{1}) +\cdots + C_{n-1} P_{n-1}(x_{1})   \\
  y_{2} = f(x_{2})  =  C_{0} P_{0}(x_{2}) +C_{1} P_{1}(x_{2}) +\cdots + C_{n-1} P_{n-1}(x_{2})   \\     
   \vdots \\
     y_{n} = f(x_{n})  =  C_{0} P_{0}(x_{n}) +C_{1} P_{1}(x_{n}) +\cdots + C_{n-1} P_{n-1}(x_{n})      
\end{array}
\right.
\] 
que no es otra cosa que un sistema de $n$ ecuaciones con $n$ incógnitas: los coeficientes $\{ C_{0}, C_{1}, \cdots  C_{n-1}  \}$.

Al resolver el sistema de ecuaciones y obtener los coeficientes, podremos obtener la función polinómica que interpola esos puntos. Una expansión equivalente se pudo haber logrado con cualquier otro conjunto de polinomios ortogonales, ya que ellos son base del espacio de funciones. 

%%%%%%%
\subsection{{\color{Fuchsia}Ejemplos}}

\subsubsection{Series de Fourier}
Un ejemplo sencillo para aprender el mecanismo del cálculo de los coeficientes para la aproximación de funciones en series de Fourier lo podemos hacer para la siguiente función.
\[
f(x)= x^2 \,, \qquad -\pi \leq  x \leq \pi \,.
\]

Los coeficientes serán:
\[
a_0=\frac{1}{\pi}\int_{-\pi}^{\pi}x^2\mathrm{d}x=\frac{2\pi^2}{3} \,,\, \quad 
a_n=\frac{1}{\pi}\int_{-\pi}^{\pi}x^2 \cos(nx)\mathrm{d}x=
\frac{4\cos(n\pi)}{n^2}=\frac{4(-1)^n}{n^2} \,,\,\quad  b_{n}=0 \,\, \forall \,\, n.
\] 

Se puede verificar que si $f(x)$ es par $(f(-x)=f(x))$ sólo la parte que contiene $\cos(nx)$ contribuirá a la serie. 

Si $f(x)$ es impar $(f(-x)=-f(x))$, lo hará sólo la parte que contiene $\mbox{sen}(nx)$. 

Por lo tanto:
\[
f(x)= x^2= \frac{\pi^2}{3}+4\sum_{n=1}^{\infty} 
\frac{(-1)^n}{n^2}\cos(nx) = \frac{\pi^2}{3} 
- 4\cos \left( x \right) +\cos \left( 2\,x \right) -\frac49\,\cos
 \left( 3\,x \right) +\frac{1}{4}\,\cos \left( 4\,x \right) - \cdots  \,.
\]

\subsubsection{Mínimos cuadrados}
\label{EjemplosMinimosCuadrados}
\index{Maxima!Mínimos Cuadrados}
\index{Mínimos Cuadrados}
Mostraremos como se puede utilizar el método de mínimos cuadrados para ajustar un conjunto de datos experimentales a un polinomio de cualquier grado. Veamos estos tres primeros casos:
\begin{enumerate}
\item La situación más simple será el conjunto de datos experimentales: $(x_{1},y_{1}), (x_{2},y_{2}), \cdots, (x_{n},y_{n})$, y nos preguntamos 
¿Cuál será la recta, $y=\textsf{c}x$, que ajusta más acertadamente a estos puntos? Es inmediato darnos cuenta que la pendiente de esa recta será: 
\[
\left| y \right> = \textsf{c} \left| x \right> \,\, \Rightarrow  \,\, 
\textsf{c} =\frac{\left<{x}\right.  \left| y \right>}{\left<{x}\right.  \left| x\right>}\,.
\]
Consideremos los puntos experimentales: 
$\left\{  \left(1,2\right) ,\left(  3,2\right)  ,\left(  4,5\right)  ,\left(  6,6\right)  \right\}$, entonces:
\[
\left| y \right> = \textsf{c} \left| x \right> \,\, \Rightarrow  \,\,
\left(
\begin{array}
[c]{c}
2\\
2\\
5\\
6
\end{array}
\right)  = \textsf{c}\left(
\begin{array}
[c]{c}
1\\
3\\
4\\
6
\end{array}
\right) \,\, \Rightarrow  \,\,
\textsf{c} =\frac{\left<{x}\right.  \left| y \right>}{\left<{x}\right.  \left| x\right> }
= \frac{2+6+20+36}{1+9+16+36}=\frac{32}{31} = 1.03226\,.
\]
¿Cuál sería el ajuste para el caso de una recta $y=\textsf{c}x +\textsf{b}$? Queda como ejercicio para el lector utilizar el esquema descrito en las ecuaciones  (\ref{MinimoCuadGeneral}) y encontrar los parámetros $\textsf{c}$ y $\textsf{b}$ para este nuevo ajuste.

\item  El segundo caso que consideraremos será la generalización a una contribución lineal de varios parámetros, i.e. un ``ajuste'' multilineal a la mejor función, $y = y(x_{1}, x_{2}, \cdots , x_{m} ) = \textsf{c}_{1}x_{1} + \textsf{c}_{2}x_{2} + \cdots +\textsf{c}_{m}x_{m}$. El procedimiento anterior puede generalizarse de forma casi inmediata:
\[
\label{MinimosCuadMultilinal}
\left| y \right> = \sum_{j=1}^{m} \textsf{c}_{j} \left| x_{j} \right> \,\, \Rightarrow \,\,
\left\{ 
\begin{array}{rcl}
  \left< x_{1}\right.\left| y\right> & =  &  \textsf{c}_{1}  \left< x_{1}\right. \left| x_{1} \right> +
  \textsf{c}_{2}  \left< x_{1}\right. \left| x_{2} \right> + \cdots + \textsf{c}_{m}  \left< x_{1}\right. \left| x_{j} \right>  \\
  \left< x_{2}\right.\left| y\right> & =  &  \textsf{c}_{1}  \left< x_{2}\right. \left| x_{1} \right> +
  \textsf{c}_{2}  \left< x_{2}\right. \left| x_{2} \right> + \cdots + \textsf{c}_{m}  \left< x_{2}\right. \left| x_{j} \right>  \\  
   \vdots &  & \vdots  \\
  \left< x_{m}\right.\left| y\right> & =  &  \textsf{c}_{1}  \left< x_{m}\right. \left| x_{1} \right> +
  \textsf{c}_{2}  \left< x_{m}\right. \left| x_{2} \right> + \cdots + \textsf{c}_{m}  \left< x_{m}\right. \left| x_{j} \right>  \,.      
\end{array}
\right.
\]

Es decir, un sistema de $m$ ecuaciones con $m$ incógnitas que corresponden a los ``pesos'', $\textsf{c}_{1}, \textsf{c}_{2}, \cdots ,\textsf{c}_{m}$, de las contribuciones a la función multilineal.

Supongamos ahora un paso más en este afán de generalizar  y consideremos que ejecutamos $n$ experimentos con $n>m$. De esta forma nuestro problema adquiere un grado mayor de riqueza y las medidas experimentales estarán representadas ahora por:
\[
\left(  y_{1},x_{11},x_{12},\cdots x_{1m}; \; y_{2},x_{21},x_{22},\cdots x_{2m}; \; y_{3},x_{31},x_{32},\cdots x_{3m}; \cdots
y_{n},x_{n1},x_{n2},x_{n3},\cdots x_{nm}\right) \,,
\]
por lo tanto, podremos construir $m$ vectores:
\[
\left|  \tilde{x}_{1}\right> =\left(  x_{11},\cdots, x_{1n}\right) ;  \;
\left|  \tilde{x}_{2}\right> =\left(  x_{21},\cdots, x_{2n}\right);  \;
\cdots\left|  \tilde{x}_{m}\right> =\left(  x_{m1},\cdots, x_{mn}\right);  \; \left| y \right> =\left(  y_{m1},\cdots , y_{n}\right) \, ,
\]
y el conjunto $\left\{  \left|  \tilde{x}_{1}\right>, \left| \tilde{x}_{2}\right>, \cdots , \left|  \tilde{x}_{m}\right>\right\}$ expande el subespacio $\textbf{\em S}\left(  \left|  \tilde{x}_{1}\right>,\left|  \tilde{x}_{2}\right> ,\cdots\left| \tilde{x}_{m}\right> \right)$, que alberga la aproximación de $\left| y \right>$. Otra vez, la distancia de este subespacio al vector $\left| y \right>$, será mínima, y por lo tanto, será la mejor aproximación:
\[
\left[  d\left(  \textbf{\em S}\left(  \tilde{\textsf{c}}_{i} \left| \tilde{x}_{i}\right> \right), \left| y \right> \right) \right]^{2} = 
\left<\textbf{\em S}\left(  \tilde{\textsf{c}}_{i} \left|  \tilde{x}_{i}\right> \right) -y \right. \left| \textbf{\em S}\left(  \tilde{\textsf{c}}_{i} \left|  \tilde{x}_{i}\right> \right)  -y \right> \,,
\]
y  $\left|  \textbf{\em S}\left(  \tilde{\textsf{c}}_{i} \left| \tilde{x} \right> \right)  -y\right> $ será ortogonal a los $ \left|  \tilde{x}_{i}\right> $:
\[
\left< \tilde{x}_{j}\right.  \left|  \textbf{\em S}\left(  \tilde{\textsf{c}}_{i} \left| \tilde{x} \right> \right)  -y \right>  \equiv 
\left< \tilde{x}_{i}\right.  \left|  \sum_{i=1}^{m} \textsf{c}_{i} \left| \tilde{x} \right> -y\right> =0 \quad \forall \quad i,j=1,2,3,\cdots m \,.
\]

Finalmente, el problema queda de la misma manera que en el caso multilineal (\ref{MinimosCuadMultilinal}):
\[
\label{EcuacionesNormales}
\left| y \right> = \sum_{j=1}^{m} \tilde{\textsf{c}}_{j} \left| \tilde{x}_{j} \right> 
\,\, \Rightarrow \,\, 
\left\{ 
\begin{array}{rcl}
  \left< \tilde{x}_{1}\right.\left| y\right> & =  &  \tilde{\textsf{c}}_{1}  \left<  \tilde{x}_{1}\right. \left|  \tilde{x}_{1} \right> +
  \tilde{\textsf{c}}_{2}  \left<  \tilde{x}_{1}\right. \left|  \tilde{x}_{2} \right> + \cdots + \tilde{\textsf{c}}_{m}  \left<  \tilde{x}_{1}\right. \left|  \tilde{x}_{j} \right>  \\
  \left<  \tilde{x}_{2}\right.\left| y\right> & =  &  \tilde{\textsf{c}}_{1}  \left<  \tilde{x}_{2}\right. \left|  \tilde{x}_{1} \right> +
  \tilde{\textsf{c}}_{2}  \left<  \tilde{x}_{2}\right. \left|  \tilde{x}_{2} \right> + \cdots + \tilde{\textsf{c}}_{m}  \left<  \tilde{x}_{2}\right. \left|  \tilde{x}_{j} \right>  \\  
   \vdots &  & \vdots  \\
  \left<  \tilde{x}_{m}\right.\left| y\right> & =  &  \tilde{\textsf{c}}_{1}  \left<  \tilde{x}_{m}\right. \left|  \tilde{x}_{1} \right> +
  \tilde{\textsf{c}}_{2}  \left<  \tilde{x}_{m}\right. \left|  \tilde{x}_{2} \right> + \cdots + \tilde{\textsf{c}}_{m}  \left<  \tilde{x}_{m}\right. \left|  \tilde{x}_{j} \right>  .      
\end{array}
\right.
\]

\item  Se puede extender el razonamiento anterior y generar un ajuste ``lineal no lineal''. Esto es: el ajuste lineal es en los coeficientes, pero la funcionalidad de la ley a la cual queremos ajustar los datos puede ser un polinomio de cualquier orden. Ese es el caso de una parábola que ajusta, por ejemplo, al siguiente conjunto de puntos:
\[
\left\{  \left(  0,1\right)  ,\left(  1,3\right)  ,\left(  2,7\right) ,\left(  3,15\right)  \right\}  
\quad\Leftrightarrow\quad y=ax^{2}+bx+c \,.
\]

Las ecuaciones toman la siguiente forma:
\[
\begin{array}
[c]{rrrr}
1= & 0 & +0 & +c\\
3= & a & +b & +c\\
7= & 4a & +2b & +c\\
15= & 9a & +3b & +c
\end{array}
\]
y los vectores construidos a partir de los datos experimentales serán:
\[
\left| x_{1}\right>=\left(0,1,4,9\right) \,, \quad\left|x_{2}\right> =\left(0,1,2,3\right)\,,\quad\left| x_{3}\right> =\left(  1,1,1,1\right)\,, \quad\left| {y}\right> =\left(  1,3,7,15\right) \,.
\]

Una vez más, la ecuación vectorial sería: $\left|{y}\right> = a\left|{x}_{1}\right>  + b\left|{x}_{2}\right> +c\left|{x}_{3}\right> $, y las ecuaciones normales (\ref{EcuacionesNormales}) para este sistema se construyen como:
\[
\left.
\begin{array}
[c]{rrrr}
136= & 98a & +36b & +14c\\
62= & 36a & +14b & +6c\\
26= & 14a & +6b & +4c
\end{array}
\right\}  \,\, \Rightarrow \,\, \left\{
\begin{array}
[c]{c}
a=-6\\
\\
b=\frac{113}{5}\\
\\
c=-\frac{32}{5}
\end{array}
\right\}  \,\, \Rightarrow \,\, y=-6x^{2}+\frac{113}{5}x-\frac{32}{5}\,.
\]
\item Para finalizar analicemos el caso típico de aproximación por mínimos cuadrados. Se sospecha que una determinada propiedad de un material cumple con la ecuación $y=ax_{1}+bx_{2}$ y realizamos 4 mediciones independientes obteniendo:
\[
\left(
\begin{array}
[c]{c}
y_{1}\\
x_{11}\\
x_{12}
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
15\\
1\\
2
\end{array}
\right) \,, \quad \left(
\begin{array}
[c]{c}
y_{2}\\
x_{21}\\
x_{22}
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
12\\
2\\
1
\end{array}
\right) \,, \quad \left(
\begin{array}
[c]{c}
y_{3}\\
x_{31}\\
x_{32}
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
10\\
1\\
1
\end{array}
\right)  ;\quad\left(
\begin{array}
[c]{c}
y_{4}\\
x_{41}\\
x_{42}
\end{array}
\right)  =\left(
\begin{array}
[c]{r}
0\\
1\\
-1
\end{array}
\right) \,.
\]

Es claro que tenemos un subespacio de $m=2$ dimensiones, que hemos hecho $n=4$ veces el experimento y que los vectores considerados arriba serán:
\[
\left|  x_{1}\right> =\left(  1,2,1,1\right)\,, \quad 
\left|{x}_{2}\right> =\left(  2,1,1,-1\right)\,, \quad 
\left|{y}\right> =\left(  15,12,10,0\right) \,.
\]

Por lo tanto, vectorialmente tendremos: $\left|{y}\right> = a\left|{x}_{1}\right> + b\left|{x}_{2}\right>$, es decir, las ecuaciones normales (\ref{EcuacionesNormales}) se escribirán de la siguiente manera:

\[
\left.
\begin{array}
[c]{ccc}
7a & +4b & =49\\
4a & +7b & =52
\end{array}
\right\}  \,\, \Rightarrow \,\,\left\{
\begin{array}
[c]{c}
a=\frac{45}{11}\\
\\
b=\frac{56}{11}
\end{array}
\right\} \,\, \Rightarrow \,\, 11y=45x_{1}+56x_{2} \,.
\]

\end{enumerate}


%%%%
\subsubsection{Aproximación polinómica}
Consideremos los puntos experimentales representados en la figura \ref{FigPuntExpInterp}. 

Es importante hacer notar que debido a que los polinomios de Legendre están definido en el intervalo $[-1,1]$ los puntos experimentales deberán reescalarse a ese intervalo para poder encontrar el polinomio de interpolación como combinación lineal de los polinomios de Legendre. Esto se puede hacer con la ayuda del siguiente cambio de variable:
\[
x=\frac{(b-a)t +b+a}{2}, \quad \mathrm{d}x = \frac{b-a}{2} \mathrm{d}t \,,
\]
donde $a$ y $b$ son los valores mínimos y máximos de los datos, respectivamente.

En este caso podemos ver que $a=2$ y $b=12$, y por lo tanto:
$x= 5t+7$ y $\mathrm{d}x =5\mathrm{d}t$ $\,\, \Rightarrow \,\, $
$t= (x-7)/5$ y $\mathrm{d}t =\mathrm{d}x/5$.

Necesitamos encontrar los coeficientes de la serie 
\[
f(x) =  \sum_{k=0}^{n-1} C_{k} P_{k}(x) \,
\]

El sistema de ecuaciones que resulta es el siguiente:

\begin{figure}[t]
\begin{center}
\includegraphics[width=5.8in]{VOLUMEN_1/02_Espacios_Lineales/Figuras/Figura2_1.jpg}
\caption{A la izquierda los puntos experimentales: $\{ (2, 8), (4,10), (6,11), (8,18), (10,20), (12,34) \}$ y a la derecha la función polinómica que los interpola.}
\label{FigPuntExpInterp}
\end{center}
\end{figure}

\[
\begin{array}{r l}
\left(-1,8\right) \,\, \Rightarrow \,\, & 8=C_{{0}}-C_{{1}}+C_{{2}}-C_{{3}}+C_{{4}}-C_{{5}}     \\ \\
\left(-\frac35,10\right) \,\, \Rightarrow \,\,  &  10=C_{{0}}- \frac{3}{5}\,C_{{1}}+ \frac{1}{25} \,C_{{2}}+{\frac {9}{25}}\,C_{{3}}-{\frac {51}{125}}\,C_{{4}}+{\frac {477}{3125}}\,C_{{5}}    \\ \\
\left(-\frac15,11\right) \,\, \Rightarrow \,\,  & 11=C_{{0}}- \frac{1}{5} \,C_{{1}}-{\frac {11}{25}}\,C_{{2}}+{\frac {7}{25}}\,C_{{3}}+{\frac {29}{125}}\,C_{{4}}-{\frac {961}{3125}}\,C_{{5}}  \\ \\   
\left(\frac15,18\right) \,\, \Rightarrow \,\,  &18=C_{{0}}+ \frac{1}{5} \,C_{{1}}-{\frac {11}{25}}\,C_{{2}}-{\frac {7}{25}}\,C_{{3}}+{\frac {29}{125}}\,C_{{4}}+{\frac {961}{3125}}\,C_{{5}}	\\ \\
\left(\frac35,20\right) \,\, \Rightarrow \,\,  &20=C_{{0}}+ \frac{3}{5}\,C_{{1}}+ \frac{1}{25} \,C_{{2}}-{\frac {9}{25}}\,C_{{3}}-{\frac {51}{125}}\,C_{{4}}-{\frac {477}{3125}}\,C_{{5}}	 \\ \\ 
\left(1,34\right) \,\, \Rightarrow \,\,  & 34=C_{{0}}+C_{{1}}+C_{{2}}+C_{{3}}+C_{{4}}+C_{{5}}
\end{array}
\]

Al resolver este sistema obtendremos:
\[
C_{0} ={\frac {2249}{144}}, \quad C_{1} ={\frac {3043}{336}}, \quad C_{2} ={\frac {1775}{504}},  \quad C_{3} =-{\frac {175}{216}}, \quad C_{4} ={\frac {625}{336}}, \quad C_{5} ={\frac {14375}{3024}}\,,
\] 
con lo cual:
\[
\mathcal{P}(x) = f(x) ={\frac {2249}{144}}+{\frac {3043}{336}}\,x+{\frac {1775}{504}}\,{\it P} \left( 2,x \right) -{\frac {175}{216}}\,{\it P} \left( 3,x \right) +{\frac {625}{336}}\,{\it P} \left( 4,x \right) +{\frac {14375}{3024}}\,
{\it P} \left( 5,x \right)\,.
\] 
y la interpolación queda representada en la figura \ref{FigPuntExpInterp}.

 

Nótese que mientras más puntos experimentales se incluyan para la interpolación, el polinomio resultante será de mayor grado y, por lo tanto incluirá oscilaciones que distorsionarán una aproximación más razonable. Por ello, la estrategia de hacer la interpolación a trozos, digamos de tres puntos en tres puntos, generará un mejor ajuste, pero será una función (un polinomio) continua a trozos.

\newpage
\subsection{{\color{red}Practicando con Maxima}} 

\subsubsection{Series de Fourier}
\index{Maxima!Series de Fourier}
\index{Series de Fourier}
Existe una librería llamada {\bf fourie} en {\bf Maxima} que contiene instrucciones para el cálculo simbólico de  series de Fourier de una función $f(x)$ en el intervalo $[-l,l]$: {\bf fourier} $(f, x, l)$.  También encontraremos un conjunto de comandos   en el paquete para calcular los coeficientes y para manipular las expresiones resultantes.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(fourie)$
\end{verbatim}}
\end{minipage}
\newline

En el ejemplo anterior aproximamos la función: 
\[
f(x)=x^2 \,.
\]

Veamos como se trabaja con el programa para calcular la serie de Fourier. Los resultados aparecerán en la forma de listas temporales y entre ellas los coeficientes. Las listas temporales serán indicadas con la notación $(\% {\tt t} \, )$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f:x^2;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
x^2
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
fourier(f,x,%pi);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t3) }
a_{0}={{\pi^2}\over{3}}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t4) }
a_{n}={{2\,\left({{\pi^2\,\sin \left(\pi\,n\right)}\over{n}}-{{2\,
 \sin \left(\pi\,n\right)}\over{n^3}}+{{2\,\pi\,\cos \left(\pi\,n
 \right)}\over{n^2}}\right)}\over{\pi}}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t5) }
b_{n}=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ { \%t_3} , { \%t_4} , { \%t_5} \right] 
\end{math}
\newline

Lo anterior se puede simplificar  con el comando {\bf foursimp}:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
foursimp(%);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t6) }
a_{0}={{\pi^2}\over{3}}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t7) }
a_{n}={{4\,\left(-1\right)^{n}}\over{n^2}}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t8) }
b_{n}=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\left[ {\%t_6} , { \%t_7} , { \%t_8} \right] 
\end{math}
\newline

Podemos evaluar la lista de los coeficiente hasta el término $k$. Aquí lo haremos hasta $k=4$ y el resultado lo lo asignaremos a la variable $F$. Por otro lado, 
usaremos $(\% {\tt o} 8 )$, la última salida, como entrada para siguiente comando.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
F:fourexpand(%o8,x,%pi,4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
{{\cos \left(4\,x\right)}\over{4}}-{{4\,\cos \left(3\,x\right)
 }\over{9}}+\cos \left(2\,x\right)-4\,\cos x+{{\pi^2}\over{3}}
\end{math}
\newline 

Construiremos ahora en un mismo gráfico la función original y los primeros $5$ términos de la serie, de esta manera podremos  comparar el resultado de la aproximación. Las opciones para realizar los diferentes gráficos en {\bf Maxima} se pueden consultar en el manual de programa. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
wxplot2d([F,f], [x,-%pi,%pi],[legend, "F", "x^2"])$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\end{math}
\begin{figure}[h]\nonumber
\begin{center}
\includegraphics[height=2.6in,width=4.5in]{VOLUMEN_1/02_Espacios_Lineales/Figuras/Figura2_0}
\end{center}
\end{figure}
\newline

Veamos que sucede si escribimos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
totalfourier(f,x,%pi);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t11) }
a_{0}=\frac{\pi^2}{3}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t12) }
a_{n}=\frac{2\,\left(\frac{\pi^2\,\sin \left(\pi\,n\right)}{n}-
 \frac{2\,\sin \left(\pi\,n\right)}{n^3}+\frac{2\,\pi\,\cos \left(\pi
 \,n\right)}{n^2}\right)}{\pi}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t13) }
b_{n}=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t14) }
a_{0}=\frac{\pi^2}{3}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t15) }
a_{n}=\frac{4\,\left(-1\right)^{n}}{n^2}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%t16) }
b_{n}=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
4\,\sum_{n=1}^{\infty }{\frac{\left(-1\right)^{n}\,\cos \left(n\,x
 \right)}{n^2}}+\frac{\pi^2}{3}
\end{math}
\newline

En este caso fueron aplicados de manera simultánea los comandos {\bf fourier} y {\bf foursimp} para finalmente presentar la serie en forma de una sumatoria.  

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}
%\newpage

\subsubsection{Mínimos Cuadrados}  

{\bf Maxima} puede estimar  los parámetros que mejor se ajusten a una función $f=(x,y)$  para un conjunto de datos, utilizando el método de mínimos cuadrados. El programa buscará primero una solución exacta, si no la encuentra buscará una aproximada. El resultado lo presentará como una lista de ecuaciones. La función a utilizar será {\bf lsquares}. 

Vamos a considerar los ejemplos estudiados con anterioridad:

\begin{enumerate}
\item En el primer ejemplo los datos eran los siguientes:
\[
\left(x,y\right)=  \left(1,2\right),\left(3,2\right)  ,\left(4,5\right),\left(6,6\right)\,.
\] 

Es necesario hacer  uso de la librería {\bf lsquares} y los los datos deben introducirse en forma de matriz.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(lsquares)$ 
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
datos: matrix([1,2],[3,2],[4,5],[6,6])$
\end{verbatim}}
\end{minipage}
\newline

Por conveniencia, para más adelante hacer un gráfico, convertimos la matrix ``datos'' en una lista. Esto es sencillo si utilizamos el comando {\bf args}:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
datosL:args(datos);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ \left[ 1 , 2 \right]  , \left[ 3 , 2 \right]  , \left[ 4 , 5
\right]  , \left[ 6 , 6 \right]  \right]
\end{math}
\newline

Supondremos entonces que los puntos se ajustan a un polinomio  lineal del tipo:  $y=ax$. El parámetro $a$  se calcula con la función { \bf lsquares estimates}. Es importante prestar bastante atención a la sintaxis del siguiente comando.
 
%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
param: lsquares_estimates(datos,[x,y],y=a*x,[a]), numer;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\left[ \left[ a=1.032258064516129  \right] 
  \right] 
\end{math}
\newline

Este será entonces el valor del parámetro $a$ de la ecuación de la recta $y=ax$ que pasa por el origen. Notemos también que le hemos asignado el valor del parámetro $a$ a la variable ${\tt param}$.

Lo que haremos ahora es escribir la ecuación de dicha recta. Podemos hacer uso de la instrucción {\bf ev} que nos permite evaluar una expresión. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
y:ev(a*x,first(param));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
1.032258064516129\,x
\end{math}
\newline

Procederemos ahora a graficar los datos experimentales $vs$ el ajuste por mínimos cuadrados en un mismo gráfico. Recordemos que el conjunto de puntos lo tenemos en la forma de una lista, que hemos denominada más arriba como ${\tt datosL}$. Mientras que al ajuste que hemos calculado, es decir, la recta: $1.032258064516129\,x$' le hemos asignado la variable denominada $y$. 

Es recomendable consultar el manual del programa, en la parte  que tiene que ver con gráficos,  {\bf plot2d},  {\bf plot3d}, para identificar la sintaxis que aparecerá en la siguiente instrucción.
%\newpage

%%%%%% INPUT:
\begin{minipage}[h]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[h]{\textwidth}{\color{blue}
\begin{verbatim}
wxplot2d([[discrete,datosL], y], [x,0,10],[style, [points,5,2], [lines,2,1]],
[point_type, plus], [legend,"Datos","y=ax"],[xlabel, "x"],[ylabel, "y"])$
\end{verbatim}}

\end{minipage}
%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\end{math}
\begin{figure}[h]\nonumber
\begin{center}
\includegraphics[height=2.5in,width=4.5in]{VOLUMEN_1/02_Espacios_Lineales/Figuras/Figura2_2.jpg}
\end{center}
\end{figure}
\newline

Nota: Se deja como ejercicio repetir éste mismo  cálculo pero usando un ajuste para  los datos de la forma: $y=ax+b$.

\item Consideremos el conjunto de datos:
$\left|  {x}_{1}\right> =\left(  1,2,1,1\right)\,,\,\, \left|{x}_{2}\right> =\left(  2,1,1,-1\right) \,,\,\, \left|{y}\right> =\left(  15,12,10,0\right)$. Vamos a suponer que ajustan de la manera siguiente: $\left|{y}\right> = a\left|{x}_{1}\right> + b\left|{x}_{2}\right>$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
datos2: matrix([1,2,15],[2,1,12],[1,1,10],[1,-1,0])$
\end{verbatim}}
\end{minipage}
\newline

Cambiemos ligeramente la notación por: $z=ax+by$ y calculemos los parámetros $a$ y $b$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
param: lsquares_estimates(datos2,[x,y,z], z=a*x+b*y,[a,b]), numer;
\end{verbatim}}
\end{minipage}
%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\left[ \left[ a=4.090909090909091 , b=5.090909090909091 \right] 
\right] 
\end{math}

\item Para el tercer ejemplo se consideraron los siguientes datos:
\[
\left\{  \left(0,1\right), \left(  1,3\right), \left(2,7\right), \left(3,15\right)  \right\}  
\quad\Leftrightarrow\quad y=ax^{2}+bx+c \,.
\]

Haremos con {\bf Maxima} el cálculo directo usando un ajuste cuadrático para los datos suministrados.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
datos3: matrix([0,1],[1,3],[2,7],[3,15])$
\end{verbatim}}
\end{minipage}
%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
datosL3: args(datos3)$
\end{verbatim}}
\end{minipage}
%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
param: lsquares_estimates(datos3,[x,y], y=a*x^2+b*x+c,[a,b,c]), numer;
\end{verbatim}}
\end{minipage}
%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\left[ \left[ a=1.5 , b=0.1 , c=1.1 \right]  \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
y2:ev(a*x^2+b*x+c,first(param))$
\end{verbatim}}
\end{minipage}
\newline

Como hicimos anteriormente, graficamos los datos y el ajuste cuadrático en una misma figura.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
wxplot2d([[discrete,datosL3], y2], [x,0,4],[style, [points,5,2], [lines,2,1]],
[point_type, plus], [legend,"Datos","y=ax^2+bx+c"],[xlabel, "x"],[ylabel, "y"])$
\end{verbatim}}
\end{minipage}
%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
\end{math}
\begin{figure}[h]\nonumber
\begin{center}
\includegraphics[height=2.6in,width=4.5in]{VOLUMEN_1/02_Espacios_Lineales/Figuras/Figura2_3.jpg}
\end{center}
\end{figure}
\end{enumerate}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}

\subsubsection{Polinomios ortogonales}
\index{Maxima!Polinomios ortogonales}
\index{Polinomios ortogonales}
{\bf Maxima} contiene la librería {\bf orthopoly} que nos permite acceder a la evaluación simbólica y numérica de los diferentes tipos de polinomios ortogonales: Chebyshev, Laguerre, Hermite, Jacobi, Legendre... 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load (orthopoly)$
\end{verbatim}}
\end{minipage}
\newline

Por ejemplo, para obtener los primeros 6 polinomios de Legendre escribimos los siguientes comandos: 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[legendre_p(0,x),legendre_p(1,x),legendre_p(2,x), 
legendre_p(3,x),legendre_p(4,x),legendre_p(5,x)]$
\end{verbatim}}
\end{minipage}
\newline

Simplificamos con {\bf ratsimp}:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ratsimp (%);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ 1 , x , \frac{3\,x^2-1}{2} , \frac{5\,x^3-3\,x}{2} , \frac{
 35\,x^4-30\,x^2+3}{8} , \frac{63\,x^5-70\,x^3+15\,x}{8} \right] 
\end{math}
%\newpage

Los diferentes polinomios de Legendre se pueden visualizar de la manera siguiente:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
wxplot2d(%,[x,-1,1],[legend, "P0", "P1","P2", "P3","P4", "P5" ])$
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\end{math}
\begin{figure}[h]\nonumber
\begin{center}
\includegraphics[height=2.6in,width=4.5in]{VOLUMEN_1/02_Espacios_Lineales/Figuras/Figura2_4.jpg}
\end{center}
\end{figure}

Ahora bien, con los datos de la figura \ref{FigPuntExpInterp} se planteó un sistema de ecuaciones lineales:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu1:C0-C1+C2-C3+C4-C5=8$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu2:C0-3/5*C1+1/25*C2+9/25*C3-51/125*C4+477/3125*C5=10$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu3:C0-1/5*C1-11/25*C2+7/25*C3+29/125*C4-961/3125*C5=11$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu4:C0+1/5*C1-11/25*C2-7/25*C3+29/125*C4+961/3125*C5=18$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu5:C0+3/5*C1+1/25*C2-9/25*C3-51/125*C4-477/3125*C5=20$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu6:C0+C1+C2+C3+C4+C5=34$
\end{verbatim}}
\end{minipage} 
\newline

Resolvemos el sistema anterior:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([ecu1,ecu2,ecu3,ecu4,ecu5,ecu6], [C0,C1,C2,C3,C4,C5]);\end{verbatim}}
\end{minipage} 

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\left[ {\it C_0}=\frac{2249}{144} , {\it C_1}=\frac{3043}{336} , 
 {\it C_2}=\frac{1775}{504} , {\it C_3}=-\frac{175}{216} , {\it C_4}=
 \frac{625}{336} , {\it C_5}=\frac{14375}{3024} \right] 
\end{math}
\newline

Para asignar cada solución a la variable correspondiente podemos hacer lo siguiente:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[C0,C1,C2,C3,C4,C5]:[rhs(%[1]),rhs(%[2]),rhs(%[3]),rhs(%[4]),rhs(%[5]),rhs(%[6])];
\end{verbatim}}
\end{minipage} 

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\left[ \frac{2249}{144} , \frac{3043}{336} , \frac{1775}{504} , -
 \frac{175}{216} , \frac{625}{336} , \frac{14375}{3024} \right] \end{math}
%\newpage

Por lo tanto, la función aproximada será:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f:C0+C1*legendre_p(1,x)+C2*legendre_p(2,x)+C3*legendre_p(3,x)
+C4*legendre_p(4,x)+legendre_p(5,x)*C5$
\end{verbatim}}
\end{minipage} 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f:expand(%);
\end{verbatim}}
\end{minipage} 

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\frac{14375\,x^5}{384}+\frac{3125\,x^4}{384}-\frac{8375\,x^3}{192}-
\frac{325\,x^2}{192}+\frac{7367\,x}{384}+\frac{1863}{128}
\end{math}
\newline

Procedemos a introducir los datos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
datos:[[-1,8],[-3/5,10],[-1/5,11],[1/5,18],[3/5,20],[1,34]];
\end{verbatim}}
\end{minipage} 

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
\left[ \left[ -1 , 8 \right]  , \left[ -\frac{3}{5} , 10 \right] 
  , \left[ -\frac{1}{5} , 11 \right]  , \left[ \frac{1}{5} , 18
  \right]  , \left[ \frac{3}{5} , 20 \right]  , \left[ 1 , 34
  \right]  \right] 
\end{math}
\newline

Para finalizar, haremos la gráfica con los datos y con la interpolación:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
wxplot2d([[discrete,datos],f], [x,-1,1],[style, [points,5,2], [lines,2,1]],
[point_type, plus],[legend, false],[xlabel, "x"],[ylabel, "y"])$
\end{verbatim}}
\end{minipage} 

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
\end{math}
\begin{figure}[h]\nonumber
\begin{center}
\includegraphics[height=2.6in,width=4.5in]{VOLUMEN_1/02_Espacios_Lineales/Figuras/Figura2_5.jpg}
\end{center}
\end{figure}

\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}

\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item 
Demuestre que con las identidades siguientes: 
\[
\cos(kx)= \frac{e^{ikx}+e^{-ikx}}{2} \,,\quad 
\mbox{sen}(kx)= \frac{e^{ikx}-e^{-ikx}}{2i} \,,
\]
la serie de Fourier definida para funciones en el intervalo $(t, t+2\pi)$ se escribe en su forma compleja como:
\[
f\left(x\right) =\frac{1}{2}a_{0}+\sum_{k=1}^{\infty} \left[ a_{k}\cos
(kx)+b_{k}\mathrm{sen}(kx)\right] \,\,\Rightarrow\,\, f \left(x\right) =
\sum_{k=-\infty}^{\infty}  c_k e^{ikx}\,,
\]
con:
\[
c_k=\frac{1}{2\pi}\int_{t}^{t+2\pi} f(x)e^{-{i k x}}\mathrm{d}x\,.
\]

Y que para funciones definidas en el intervalo $(l, l+2L)$ como:
\[
f\left(x\right) =\sum_{k=-\infty}^{\infty}  c_k e^{\frac{i k\pi x}{L}} \,, \quad
\mbox{con} \,\, c_k=\frac{1}{2L}\int_{l}^{l+2L} f(x)e^{-\frac{i k\pi x}{L}}
\mathrm{d}x \,.
\]

Nota: Para los siguientes ejercicios supondremos la utilización del programa {\bf Maxima}.

\item Para las siguientes funciones determine la serie de Fourier calculando los coeficientes como en la sección 
\ref{AproximacionFunciones} y compare los resultados con los cálculos hechos en el ambiente de manipulación simbólica. 
\begin{enumerate}
\item $f(x)=x\, \mbox{sen}(x)  $, si $ -\pi < x < \pi$.
\item $f(x)=e^{x}  $, si $ -\pi < x < \pi$.
\item $f(x)=x  $, si $ 0 < x < 2$.
\item $f(x)=2x-1  $, si $ 0 < x < 1$.
\end{enumerate}
 
\item  Considere el espacio vectorial, $\mathcal{C}^{\infty}_{[-1,1]}$, de funciones reales, continuas y continuamente diferenciables definidas en el intervalo $[-1,1]$. Es claro que una posible base de este espacio de funciones la constituye el conjunto de monomios $\left\{ 1, x, x^{2}, x^{3}, x^{4}, \cdots  \right\}$ por cuanto estas funciones son linealmente independientes. 
\begin{enumerate}
  \item Si suponemos que este espacio vectorial está equipado con un producto interno definido por $\left< f |g \right> = \int_{-1}^{1}\; \mathrm{d}x \; f(x) g(x) $, muestre que esa base de funciones no es ortogonal.
  
  \item Utilizando la definición de producto interno $\left< f |g \right> = \int_{-1}^{1}\; \mathrm{d}x \; f(x) g(x) $ ortogonalize la base $\left\{ 1, x, x^{2}, x^{3}, x^{4}, \cdots  \right\}$ y encuentre los 10 primeros vectores ortogonales, base  para $\mathcal{C}^{\infty}_{[-1,1]}$. Esta nueva base de polinomios ortogonales se conoce como los polinomios de Legendre. 

  \item Modifique un poco la definición de producto interno $\left< f |g \right> = \int_{-1}^{1}\; \mathrm{d}x \; f(x) g(x) \sqrt{(1-x^{2})}$ y ortogonalize la base $\left\{ 1, x, x^{2}, x^{3}, x^{4}, \cdots  \right\}$ y encuentre otros 10 primeros vectores ortogonales base para el mismo $\mathcal{C}^{\infty}_{[-1,1]}$. Esta nueva base de polinomios ortogonales se conoce como los polinomios de Chebyshev. 

\item Suponga la función $h(x) = \mathrm{sen}(3x)(1-x^{2})$:
\begin{enumerate}
  \item Expanda la función $h(x)$ en términos de la base de monomios y de polinomios de Legendre, grafique, compare y encuentre el grado de los polinomios en los cuales difieren las expansiones.
 \item Expanda la función $h(x)$ en términos de la base de monomios y de polinomios de Chebyshev, grafique, compare y encuentre el grado de los polinomios en los cuales difieren las expansiones.
 \item Expanda la función $h(x)$ en términos de la base de polinomios de Legendre y de Chebyshev, grafique, compare y encuentre el grado de los polinomios en los cuales difieren las expansiones.
\item Estime en cada caso el error que se comete como función del grado del polinomio (o monomio) de la expansión. 
\end{enumerate}
¿Qué puede concluir respecto a la expansión en una u otra base?
\end{enumerate}

\item  Parecido al ejercicio anterior, considere el espacio vectorial, $\mathcal{C}^{\infty}_{[0,1]}$, de funciones reales, continuas y continuamente diferenciables definidas en el intervalo $[0,1]$. Es claro que otra posible base de este espacio de funciones la constituye el conjunto de funciones exponenciales $\left\{ 1, \mathrm{e}^{x}, \mathrm{e}^{2x}, \mathrm{e}^{3x}, \mathrm{e}^{4x}, \cdots  \right\}$ por cuanto, al igual que el caso anterior estas funciones también son linealmente independientes. Adicionalmente, considere aquí la función $g(x) = \cos(3x^{3})(1-x^{2})$, 
\begin{enumerate}
\item Suponga que este espacio vectorial está equipado con un producto interno definido por $\left< f |g \right> = \int_{0}^{1}\; \mathrm{d}x \; f(x) g(x) $, y muestre que esa base de funciones $\left\{ 1, \mathrm{e}^{x}, \mathrm{e}^{2x}, \mathrm{e}^{3x}, \mathrm{e}^{4x}, \cdots  \right\}$ no es ortogonal.

  \item Utilizando la definición de producto interno ortogonalize la base $\left\{ 1, \mathrm{e}^{x}, \mathrm{e}^{2x}, \mathrm{e}^{3x}, \mathrm{e}^{4x}, \cdots  \right\}$ y encuentre los 7 primeros vectores ortogonales, base  para $\mathcal{C}^{\infty}_{[0,1]}$, i.e. $\left\{ 1, E_{1}(x), E_{2}(x), E_{3}(x), E_{4}(x), \cdots  \right\}$. 

\item Encuentre el valor de los coeficientes, $C_{i}$ de la expansión  $g_{M}(x) \approx \sum_{i=0}^{7} C_{i} x^{i} $ utilizando la definición de producto interno anterior y estime también el error $\epsilon_M$ en esta aproximación $g_{M}(x) = \sum_{i=0}^{7} C_{i} x^{i} +\epsilon_M$. 
\item Expanda en serie de Taylor la función anterior $g_{T}(x) \approx \sum_{n=0}^{7} \left.\frac{\mathrm{d} g(x)}{\mathrm{d} x}\right|_{x = 0}  \frac{x^{n}}{n!} $, estime el error $\epsilon_T$ y compárelo con el caso anterior.
\item Ahora encuentre el valor de los coeficientes, $\tilde{C}_{i}$ de la expansión  $g_{E}(x) \approx \sum_{m=0}^{7} \tilde{C}_{m} \mathrm{e}^{mx} $ utilizando la definición de producto interno anterior y, del mismo modo, estime también el error $\epsilon_E$ en esta aproximación $g_{E}(x) = \sum_{m=0}^{7} \tilde{C}_{m} \mathrm{e}^{mx}  +\epsilon_E$. ?` Qué puede concluir de la comparación de los errores $\epsilon_M$, $\epsilon_T$ y $\epsilon_E$ ?
\item A continuación encuentre el valor de los coeficientes, $\bar{C}_{i}$ de la expansión  $g_{Eo}(x) \approx \sum_{m=0}^{7} \bar{C}_{m} E_{m}(x) $ y estime también el error $\epsilon_{Eo}$ en esta aproximación $g_{Eo}(x) = \sum_{m=0}^{7} \bar{C}_{m} E_{m}(x)  +\epsilon_{Eo}$. ?` Otra vez, qué puede concluir de la comparación de los errores $\epsilon_M$, $\epsilon_T$, $\epsilon_E$ y $\epsilon_{Eo}$?
\item Grafique las funciones $g(x)$, $g_M(x)$, $g_T(x)$, $g_E(x)$ y $g_{Eo}(x)$ y compare con los errores.
\end{enumerate}
\item Al medir la temperatura a lo largo de una barra material obtenemos los
siguientes valores:
\begin{center}
\begin{tabular}
[c]{|l|rrrrrrrrr|} \hline\hline
$x_{i}\,(cm)$ & $1,0$ & $2,0$ & $3,0$ & $4,0$ & $5,0$ & $6,0$ & $7,0$ &
$8,0$ & $9,0$  \\ \hline
$T_{i}\,(^{\circ}C)$ & $14,6$ & $18,5$ & $36,6$ & $30,8$ & $59,2$ & $60,1$
& $62,2$ & $79,4$ & $99,9$ \\ \hline \hline
\end{tabular}
\end{center}

Encuentre, mediante el método de los mínimos cuadrados los coeficientes que mejor ajustan a la recta $T=ax+b$.


\item Los precios de un determinado producto varían como se muestra a continuación:
\begin{center}
\begin{tabular}
[c]{|l|rrrrrrrr|} \hline\hline
Año & $2007$ & $2008$ & $2009$ & $2010$ & $2011$ & $2012$ & $2013$ & $2014$   \\ \hline
Precio & $133.5$ & $132.2$ & $138.7$ & $141.5 $ & $144.2$ & $144.5$
& $148.6$ & $153.8$  \\ \hline \hline
\end{tabular}
\end{center}
Realice una interpolación polinomial que permita modelar estos datos.

\end{enumerate}



