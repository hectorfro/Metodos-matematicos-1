\chapter{Matrices, determinantes y autovectores}
\label{CapMatricesDeterminantesAutovectores}

\section*{La ruta de este capítulo}

En el capítulo anterior definimos los funcionales lineales como un {\it morfismo} de un espacio vectorial lineal $\textbf{\em V}$ a un espacio unidimensional $\textbf{\em K}$. Esta misma idea se puede extender a morfismos de un espacio vectorial $\textbf{\em V}_{1}$ a un espacio vectorial $\textbf{\em V}_{2}$, sobre el mismo campo $\textbf{\em K}$. Desarrollaremos estos conceptos a través de los operadores lineales en la sección \ref{OperadoresLineales}, y en la sección \ref{Tiposdeoperadores} señalaremos algunos de los operadores lineales de mayor relevancia. En la sección \ref{RepresentacionMatricialOpe} estableceremos una correspondencia uno-a-uno entre operadores lineales y matrices y la dependencia de ésta correspondencia con las bases del espacio vectorial. Luego, en la sección \ref{zoologicodematrices}  presentaremos un conjunto de matrices fundamentales junto con sus  operaciones. En la sección \ref{SistemasEcuacionesLineales}, veremos algunos de los métodos más comunes para la resolución de sistemas de ecuaciones lineales. Y para finalizar, las secciones  \ref{SecAutovectoresAutovalores} y \ref{autovectoresmatricesimportantes} estarán dedicadas al importante tema de la representación espectral de un operador: el problema de autovalores y autovectores. 

\section{Operadores lineales}
\label{OperadoresLineales}
\index{Operador Lineal}
\index{Lineal!Operador}

Definiremos como operador lineal (o transformación lineal) a una operación que asocia un vector $\left| {v}\right> \in \textbf{\em V}_{1}$ un vector $\left|  {v}^{\prime}\right> \in \textbf{\em V}_{2}$ y que respeta la linealidad, es decir esta función de $\textbf{\em V}_{1} \rightarrow \textbf{\em V}_{2}$ cumple con:
\begin{equation}
\label{OperadorLineal}
\left|  {v}^{\prime}\right> =\mathbb{T}\left|  {v} \right> \quad /  \quad 
\mathbb{T}\left[  \alpha\ \left|  {v}_{{1}}\right> +\beta\ \left|  {v}_{2}\right> \right]  =
\alpha\ \mathbb{T}\left|  {v}_{{1}}\right> +\beta\ \mathbb{T}\left|  {v}_{2}\right> \,\, \forall \,\,
\quad \left|  {v}_{{1} }\right> \text{ y }\left|  {v}_{2}\right> \in\textbf{\em V}_{1} \,.
\end{equation}

Sencillamente, algo que actúe sobre una suma de vectores en  
$\textbf{\em V}_{1}$  y que es equivalente a la suma de sus actuaciones sobre los vectores suma, es decir, a la suma en $\textbf{\em V}_{2}$. 

Podemos ilustrar este tipo de transformación con varios ejemplos:
\begin{enumerate}
\item  Las siguientes transformaciones: 
$\left|x^{\prime}\right>=\mathbb{T}\left| x \right> \,\, \rightarrow \,\,
\left(x^{\prime},y^{\prime},z^{\prime}\right)  =
\mathbb{T}\left\{\left(x,y,z\right)  \right\}$,  claramente son lineales

\begin{itemize}
\item $\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(x,2y,3z\right) $
\[ 
\mathbb{T}\left[\alpha \left(x_1,y_1,z_1\right) +\beta \left(x_2,y_2,z_2\right)  \right]= 
\alpha \mathbb{T}\left[ \left(x_1,y_1,z_1\right)  \right]  +
\beta   \mathbb{T}\left[\left(x_2,y_2,z_2\right)  \right] \,,
\]
entonces:
\begin{eqnarray*}
\mathbb{T}\left[\left( \alpha x_1+\beta x_2,\alpha y_1+\beta y_2,\alpha z_1+\beta z_2\right)  \right] &=&
\alpha\left(x_1, 2y_1,3z_1\right)  +\beta \left(x_2,2y_2,3z_2\right) \\
\left( \alpha x_1+\beta x_2, 2\left[  \alpha y_1+\beta y_2\right]  ,3\left[ \alpha z_1+\beta z_2 \right]  \right)  & =&
\left( \alpha x_1+\beta x_2, 2\left[ \alpha y_1+\beta y_2 \right]  ,3\left[ \alpha z_1+\beta z_2 \right]  \right) \,.
\end{eqnarray*}

\item $\mathbb{T}\left\{  \left(x,y,z\right)  \right\} =\left( z,y,x\right)$
\[ 
\mathbb{T}\left[\alpha \left(x_1,y_1,z_1\right) +\beta \left(x_2,y_2,z_2\right)  \right]= 
\alpha \mathbb{T}\left[ \left(x_1,y_1,z_1\right)  \right]  +
\beta   \mathbb{T}\left[\left(x_2,y_2,z_2\right)  \right] \,,
\]
igual que en el caso anterior:
\begin{eqnarray*}
\mathbb{T}\left\{  \left(\alpha x_1+\beta x_2, \alpha y_1+\beta y_2, \alpha z_1+\beta z_2 \right)  \right\} &=&
\alpha \left( z_1, y_1, x_1\right)  +\beta \left( z_2, y_2, x_2\right) \\
\left(  \alpha z_1+\beta z_2, \alpha y_1+\beta y_2, \alpha x_1+\beta x_2\right)   & =&
\left(  \alpha z_1+\beta z_2, \alpha y_1+\beta y_2, \alpha x_1+\beta x_2\right) \,.
\end{eqnarray*}

\end{itemize}

\item  Cosas tan sencillas como la multiplicación por un número es una transformación (u operador) lineal, esto es, una transformación $\mathbb{T}:\textbf{\em V} \rightarrow \textbf{\em V}$ tal que
\[
\mathbb{T}\left|  {v}\right> =\left|  {v}^{\prime }\right> =\lambda \left|  {v}\right> \,\, \Rightarrow  \,\, 
\mathbb{T}\left[  \alpha\left|  {v}\right> +\beta \left|  {w}\right> \right]  =  \alpha\mathbb{T}\left|  {v}\right> +\beta\mathbb{T}\left|  {w}\right> = \alpha\lambda\left| v\right> +\beta\lambda\left| {w}\right> \,.
\]
Obviamente, si $\lambda=1$ tenemos la transformación identidad que transforma todo vector en sí mismo; si $\lambda=0$ tendremos la transformación cero, vale decir que lleva a todo $\left| {v} \right> \in\textbf{\em V}$ a al elemento cero $\left|  {0}\right> $.

\item  La definición de producto interno también puede ser vista como una transformación (operador) lineal 
$\mathbb{T} :\textbf{\em V}\rightarrow \mathds{R} $
\[
\mathbb{T}\left|  {v}\right> =\lambda \rightleftharpoons \left< {u}\right.  \left|  {v}\right> \equiv \lambda \, .
\]
Otra vez:
\[
\mathbb{T}\left[  \alpha\left|  {v}\right> +\beta\left|  {w}\right> \right]  =\left< {u}\right|  \left[  \alpha\left| {v}\right> +\beta\left|  {w}\right> \right]
=\alpha\left< {u}\right.  \left|  {v}\right> +\beta\left< {u}\right.  \left|  {w}\right> \, ,
\]
por lo tanto es lineal. Esto implica que también la proyección de un determinado $\left| {v}\right> \in\textbf{\em V}$ sobre un subespacio $\textbf{\em S}$ es un operador lineal, y lo denotaremos como:
\[
\left[  \left|  {s}\right> \left< {s}\right| \right]  \ \left|  {v}\right> =\left< {s}\right.
\left|  {v}\right> \left|  {s}\right> =\left| {v}_{s}\right> \,,\quad\text{con }\left|  {s}\right>  \text{\ y }\left|  {v}_{s}\right> \in\textbf{\em S}\,.
\]
Esta idea se extiende fácilmente para un proyector $\mathbb{T}:\textbf{\em V}_{m}\mathbf{\rightarrow S}_{n}$ con $m>n$, de tal modo que para un vector $\left| {v}\right> \in\textbf{\em V}_{m}$
\[
\mathbb{P}_{m}\left|  {v}\right> \equiv\left(  \left|{u}_{i}\right> \left< {u}^{i}\right|  _{m}\right)
\left|  {v}\right> =\left< {u}^{i}\right.  \left| {v}\right> _{m}\left|  {u}_{i}\right> =\left| {v}_{m}\right> \, ,
\]
con $\left\{  \left< {u}_{i}\right|  \right\} $ una base de $\textbf{\em S}_{n}$. Es claro que estamos utilizando la convención de Einstein para la suma de índices.

\item  Las ecuaciones lineales también pueden verse como transformaciones lineales. Esto es, considere una transformación lineal $\mathbb{T} :\textbf{\em V}_{n} \mathbf{\rightarrow V}_{m}$. Por lo tanto asociaremos
\[
\left|  {y}\right> =\mathbb{T}\left| {x}\right> \,\, \Rightarrow \,\,  \left(  y^{1},y^{2},y^{3},\cdots,y^{m}\right)
=\mathbb{T}\left[ \left(  x^{1},x^{2},x^{3},\cdots,x^{n}\right)  \right] \, ,
\]
a través de $n\times m$ números, $a_{j}^{i}$, organizados de la siguiente forma:
\[
y^{i}=a_{j}^{i}\ x^{j} \qquad \text{con }
\left\{
\begin{array}
[c]{c}
i=1,2,\cdots,m\\
j=1,2,\cdots,n
\end{array}
\right.
\]
una vez más,
\begin{align*}
\mathbb{T}\left[  \alpha\left|  {v}\right> +\beta\left| {w}\right> \right]   &  =\mathbb{T}\left[  \alpha\left( v^{1},v^{2},v^{3},\cdots,v^{n}\right)  +\beta\left(  w^{1},w^{2},w^{3}, \cdots, w^{n}\right)  \right]  = \alpha a_{j}^{i}\ v^{j}+\beta a_{j}^{i}\ w^{j} \\
&  =\mathbb{T}\left[  \left(  \alpha v^{1}+\beta w^{1},\alpha v^{2}+\beta w^{2},\alpha v^{3}+\beta w^{3},\cdots,\alpha v^{n}+\beta w^{n}\right) \right] \\
&  =a_{j}^{i}\ \left(  \alpha v+\beta w\right)  ^{j}=\alpha a_{j}^{i}\ v^{j}+\beta a_{j}^{i}\ w^{j}=a_{j}^{i}\left(  \alpha v^{j}+\beta w^{j}\right) \, .
\end{align*}

\item  La derivada es un operador lineal 
\[
\left|  {v}^{\prime}\right> =\mathbb{T}\left|  {v} \right> \quad \rightarrow \quad \left|  {y}^{\prime}\right> =\mathbb{D}\left|  {y}\right>  \quad \rightarrow \quad \mathbb{D} \left[ y(x) \right]  \equiv \frac{\mathrm{d}}{\mathrm{d}x}\left[  y(x)  \right]  \equiv 
\frac{\mathrm{d} y(x)  }{\mathrm{d}x}\equiv y^{\prime}(x) 
\, ,
\]
es claro que
\[
\mathbb{D}\left[  \alpha f(x)  +\beta g(x)  \right]
=\alpha\mathbb{D}[f(x)]  +\beta\mathbb{D}[g(x)]
\equiv \alpha f^{\prime}(x)  +\beta g^{\prime}(x) \,.
\]
Igualmente podemos asociar un operador diferencial de cualquier orden a una derivada del mismo orden, esto es
\begin{eqnarray*}
\left|  {y}^{\prime\prime}\right>  & =&
\mathbb{D}^{2}\left| {y}\right> \,\, \rightarrow \,\, 
{\mathbb{D}}^{2}\left[  y(x)  \right]  \equiv 
\frac{\mathrm{d}^{2}y(x)}{\mathrm{d}x^{2}}\equiv y^{\prime\prime}(x)\,, \\
\left|  {y}^{\prime\prime\prime}\right>  &  = &
\mathbb{D}^{3}\left|  {y}\right> \,\, \rightarrow \,\,  
{\mathbb{D}}^{3}\left[  y(x)  \right]  \equiv
\frac{\mathrm{d}^{3}y(x)}{\mathrm{d}x^{3}}\equiv y^{\prime \prime \prime}(x) \,, \\
&  \vdots & \\
\left|  {y}^{\left(  n\right)  }\right>  &  = &
\mathbb{D}^{n}\left|  {y}\right> \,\, \rightarrow \,\, 
{\mathbb{D}}^{n}\left[  y(x)  \right]  \equiv
\frac{\mathrm{d}^{n}y(x)  }{\mathrm{d}x^{n}}\equiv y^{(n)  }(x) \,.
\end{eqnarray*}

\item  Del mismo modo, cualquier ecuación diferencial lineal es un ejemplo de operador lineal, digamos
\[
y^{\prime\prime}-3\ y^{\prime}+2\ y=\left(\mathbb{D}^{2}-3\mathbb{D}+ 2\right)  y(x) \,,
\]
es claro que si $y(x) =\alpha f(x)  +g(x)$ la linealidad es evidente:
\begin{gather*}
\left(  \alpha f(x)  +g(x)  \right)  ^{\prime\prime}-3\ \left(  \alpha f(x)  +g(x)  \right)  ^{\prime}+2\ \left(  \alpha f(x)  +g(x)  \right)
=\alpha\left(  f^{\prime\prime}-3\ f^{\prime}+2\ f\right)  +g^{\prime\prime}-3\ g^{\prime}+2\ g\\
\uparrow\downarrow\\
\left(  \mathbb{D}^{2}-3\mathbb{D}+2\right)  \left(  \alpha f(x) +g(x)  \right)  =\left(  \mathbb{D}^{2}-3\mathbb{D}+2\right) \alpha f(x)  +\left(  \mathbb{D}^{2}-3\mathbb{D}+2\right)
g(x)\, ;
\end{gather*}

\item  La integral también es un operador lineal: 
$g(x)  =\int_{a}^{x}\ f(t)\mathrm{d}t\,\, \leftrightarrows \,\, \mathbb{T}\left[  f(t)\right] $. 

\item  Otros ejemplos típicos son los operadores de transformaciones integrales
\[
F(s)=\int_{a}^{b}\mathcal{K}\left(  s,t\right)  \ f(t)\mathrm{d}t
\,\, \leftrightarrows \,\,  \mathbb{T}\left[ f(t)\right] \,,
\]
donde $\mathcal{K}\left(  s,t\right)$ es una función conocida de $s$ y $t$, denominada el \textit{núcleo} de la transformación. Si $a$ y $b$ son finitos la transformación se dirá finita, de lo contrario infinita. 

Así, si $\ f(t)=\alpha f_{1}(t)+f_{2}(t)$ con $f_{1}(t)$ y $f_{2}(t)$ $\in\mathcal{C}_{\left[  a,b\right]  }^{\infty}$ es obvio que:
\begin{eqnarray*}
F(s)  &  = &\int_{a}^{b}\mathcal{K}\left(s,t\right)  \left[  \alpha f_{1}(t)+\beta f_{2}(t)\right] = 
\alpha \int_{a}^{b}\mathcal{K}\left(s,t\right)  \ f_{1}(t)\mathrm{d}t+\beta \int_{a}^{b}\mathcal{K}\left(  s,t\right)  \ f_{2}(t)\mathrm{d}t \,, \\ 
\\ 
F(s) &  =& \alpha F(s_{1})+\beta F(s_{2})
\quad \leftrightarrows\quad
\mathbb{T}\left[  \alpha f_{1}(t)+ \beta f_{2}(t)\right] =\alpha\mathbb{T}\left[f_{1}(t)\right]  +\beta \mathbb{T}\left[  f_{2}(t)\right] \,.
\end{eqnarray*}

\index{Transformada!Integral}
\index{Integral!Transformada}
\index{Transformada!Laplace}
\index{Laplace!Transformada de}
\index{Transformada!Fourier}
\index{Fourier!Transformada de}
\index{Transformada!Hankel}
\index{Hankel!Transformada de}
\index{Transformada!Mellin}
\index{Mellin!Transformada de}
Dependiendo de la selección del núcleo y los limites tendremos distintas transformaciones integrales, en Física las más comunes son:
\begin{center}
\begin{tabular}
[c]{lcc} \hline \hline
\textbf{Nombre} & $F(s)=\mathbb{T}\left\{  f(t)\right\}  $ & $f(t)=\mathbb{T}^{-1}\left\{  F(s)\right\} $  \\ \hline \hline
&  & \\
Laplace & $F(s)=\int_{0}^{\infty}{ e}^{-st}f(t)\mathrm{d}t$ &
$\ f(t)=\frac{1}{2\pi i}\int_{\gamma-i\infty}^{\gamma+i\infty}{ e}
^{st}F(s)\mathrm{d}s$\\
&  & \\
Fourier de senos y cosenos & $F(s)=
{\displaystyle\int_{0}^{\infty}}
\left.
\begin{array}
[c]{c}
{\mbox{ sen}(st)}\\
{ \cos(st)}
\end{array}
\right.  f(t)\mathrm{d}t$ & $f(t)=
{\displaystyle\int_{0}^{\infty}}
\left.
\begin{array}
[c]{c}
{\mbox{ sen}(st)}\\
{ \cos(st)}
\end{array}
\right.  F(s)\mathrm{d}s$\\
&  & \\
Fourier compleja & $F(s)=
{\displaystyle\int_{-\infty}^{\infty}}
{ e}^{i st}f(t)\mathrm{d}t$ & $f(t)=
{\displaystyle\int_{-\infty}^{\infty}}
{ e}^{-i st}F(s)\mathrm{d}s$\\
&  & \\
Hankel & $F(s)=
{\displaystyle\int_{0}^{\infty}}
tJ_{n}{ (st)}f(t)\mathrm{d}t$ & $f(t)=
{\displaystyle\int_{0}^{\infty}}
sJ_{n}{ (ts)}F(s)\mathrm{d}s$\\
&  & \\
Mellin & $F(s)=
{\displaystyle\int_{0}^{\infty}}
t^{s-1}\ f(t)\mathrm{d}t$ & $f(t)=\frac{1}{2\pi i}\int_{\gamma-i\infty
}^{\gamma+i\infty}s^{-t}\ F(s)\mathrm{d}s$ \\ \\
\hline\hline
\end{tabular}
\end{center}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Espacio vectorial de operadores lineales}
\label{EspacioVectorialOpLineales}
\index{Espacio vectorial!Operadores lineales}
\index{Operador lineal!Espacio vectorial}

Es posible definir un conjunto de operadores lineales $\left\{\mathbb{A},\mathbb{B} ,\mathbb{C}\cdots\ \right\}: \textbf{\em V}_{1}\mathbf{\rightarrow}\textbf{\em V}_{2}$ y constituir un espacio vectorial lineal si se dispone entre ellos de la operación suma y la multiplicación por un número. Así, claramente, dado $\left\{\mathbb{A},\mathbb{B},\mathbb{C}\cdots\ \right\}$, y definida
\[
\left( \lambda \mathbb{A+B}\right)  \left|  {v}\right> \equiv
\lambda\mathbb{A}\left|  {v}\right> +\mathbb{B}\left|  {v}\right>
\quad /  \quad
\left\{
\begin{array}
[c]{c}
\mathbb{A}\left[  \alpha\ \left|  {v}_{{1}}\right> +\beta\ \left|  {v}_{2}\right> \right]  =\alpha\ \mathbb{A}
\left|  {v}_{{1}}\right> +\beta\ \mathbb{A}\left| {v}_{2}\right> \\
\\
\mathbb{B}\left[  \alpha\ \left|  {v}_{{1}}\right> +\beta\ \left|  {v}_{2}\right> \right]  =\alpha\ \mathbb{B}
\left|  {v}_{{1}}\right> +\beta\ \mathbb{B}\left| {v}_{2}\right>
\end{array}
\right.
\]
es directo comprobar que:
\begin{align*}
\left(  \lambda\mathbb{A+B}\right)  
\left[  \alpha\ \left|  {v}_{{1}}\right> +\beta\ \left|  {v}_{2}\right>\right]   &  =\lambda\mathbb{A}\left[  \alpha\ \left|  {v}_{{1}
}\right> +\beta\ \left|  {v}_{2}\right> \right]  +\mathbb{B}\left[ \alpha\ \left|  {v}_{{1}}\right> +\beta\ \left| {v}_{2}\right> \right]  \\
&  =\lambda\left(  \alpha\ \mathbb{A}\left|  {v}_{{1}}\right> +\beta\ \mathbb{A}\left|  {v}_{2}\right> \right)  {+}
\alpha\ \mathbb{B}\left|  {v}_{{1}}\right> +\beta \ \mathbb{B}\left|  {v}_{2}\right> \\
&  =\lambda\left(  \alpha\ \mathbb{A}\left|  {v}_{{1}}\right>
{+}\alpha\ \mathbb{B}\left|  {v}_{{1}}\right>
\right)  +\beta\ \mathbb{A}\left|  {v}_{2}\right> +\beta
\ \mathbb{B}\left|  {v}_{2}\right> \\
&  =\lambda \alpha \left( \mathbb{A}\left|  {v}_{{1}}\right> + 
\mathbb{B}\left|  {v}_{{1}}\right>\right)  +\beta\left( \mathbb{A}
\left|  {v}_{2}\right> + \mathbb{B}\left|  {v}_{2}\right> \right)\,. 
\end{align*}

Igualmente, se cumple que
$\left(  \mathbb{A+B}\right)  +\mathbb{C}  = \mathbb{A+}\left(  \mathbb{B}+\mathbb{C}\right) $,  
con $\mathbb{A}$, $\mathbb{B}$ y $\mathbb{C}$ lineales en $\textbf{\em V}$, 
\begin{align*}
\left[  \left(  \mathbb{A+B}\right)  +\mathbb{C}\right]  \left| {v}\right>  &  =\left(  \mathbb{A+B}\right)  {\left|
{v}\right> +\mathbb{C}\left|  {v}\right> }\qquad
\forall\mathbf{\quad}\left|  {v}\right> \in\textbf{\em V}_{1}\\
&  =\mathbb{A} \left|  {v}\right> +\mathbb{B}\left|  {v}\right> +\mathbb{C}\left|  {v}\right> \\
&  =\mathbb{A}\left|  {v}\right> +\left(  \mathbb{B+C}\right) \left|  {v}\right> \\
&  =\left[  \mathbb{A+}\left(  \mathbb{B}+\mathbb{C}\right)  \right]  \left| {v}\right>\,,
\end{align*}
del mismo modo se puede comprobar fácilmente $\mathbb{A+B}=\mathbb{B+A}$.   

Ahora bien, si definimos la transformación cero de $\textbf{\em V}_{1} \rightarrow \textbf{\em V}_{2}$ tal que $\mathbb{O}\left|  {v}\right>= \left|  {0}\right> \,\, \forall \,\,  \left| {v}\right> \in \textbf{\em V}_{1}$,  que  asigna el vector $\left|  {0}\right> \in\textbf{\em V}_{2} \,\, \forall \,\,   \left|  {v}\right> \in\textbf{\em V}_{1}$, entonces el operador lineal $\mathbb{O}$ será el elemento neutro respecto a la suma de operadores. 

Finalmente, el elemento simétrico queda definido por
\[
\left( -\mathbb{A}\right)  \left|  {v}\right> =-\mathbb{A} \left|  {v}\right> \,\,\, \Rightarrow \,\,
\left( \mathbb{A} -\mathbb{A}\right)  \left|  {v}\right> =\mathbb{O}\left| {v}\right> =\left|  {0}\right>\,.
\]
Con ello queda demostrado que los operadores lineales forman un espacio vectorial el cual de ahora en adelante denominaremos $\mathcal{L}\left( \textbf{\em V}_{1},\textbf{\em V}_{2} \right)$. 

\subsection{Composición de operadores lineales}
%%%%%%%%%%%%
\label{ComposicionOperLineales}
\index{Composición de Operadores Lineales}
\index{Operador Lineal!Composición}
%%%%%%%%%%%%
El producto o composición de dos operadores lineales, $\mathbb{A}$ y
$\mathbb{B}$ se denotará $\mathbb{AB}$ y significará que primero se aplica $\mathbb{B}$ y al resultado se aplica $\mathbb{A}$. Esto es:
\[
\mathbb{AB}\left|  {v}\right> =\mathbb{A}\left(  \mathbb{B} \left|  {v}\right> \right)  =\mathbb{A}\left|  {\tilde{v}}\right> =\left|  {\tilde{v}}^{\prime}\right> \,.
\]
La composición de funciones cumple con las siguientes propiedades:
\begin{align*}
\left(  \mathbb{AB}\right)  \mathbb{C}  &  = \mathbb{A}\left(  \mathbb{BC} \right)  ; \qquad\alpha\left(  \mathbb{AB}\right)  =
\left( \alpha\mathbb{A}\right)  \mathbb{B=A}\left(  \alpha{\mathbb{B}}\right) ;\\
\left(  \mathbb{A}_{1}+\mathbb{A}_{2}\right)  \mathbb{B}  &  =\mathbb{A}_{1}\mathbb{B+A}_{2}\mathbb{B}\,; \qquad\mathbb{A}\left(  \mathbb{B}_{1}+\mathbb{B}_{2}\right)  =\mathbb{AB}_{1}+\mathbb{AB}_{2} \,.
\end{align*}
Es decir, que la composición de operadores es asociativa y distributiva a la suma y que conmuta respecto a la multiplicación por números.

Por otro lado si $\mathbb{I}$ es el operador identidad: 
$\mathbb{I}\left|  {v}\right> =\left|  {v}\right> \,\, \Rightarrow \,\, \mathbb{AI=IA=A}$. 

En general $\mathbb{AB\neq BA}$, por lo tanto podemos construir el conmutador de estos operadores como:
\[
\left[  \mathbb{A,B}\right]  =\mathbb{AB-BA} \quad /  \quad 
\left[ \mathbb{AB-BA}\right]  \left|  {v}\right> =\mathbb{AB}\left|
{v}\right> - \mathbb{BA}\left|  {v}\right> \,.
\]

Es inmediato comprobar algunas de las propiedades más útiles de los
conmutadores:
\begin{align*}
\left[  \mathbb{A,B}\right] & =-\left[  \mathbb{B,A}\right] \\
\left[  \mathbb{A}, \left(  \mathbb{B+C}\right)  \right]   &  =\left[
\mathbb{A,B}\right]  +\left[  \mathbb{A,C}\right] \\
\left[\mathbb{A,BC}\right] & =\left[\mathbb{A,B}\right] \mathbb{C+B}
\left[\mathbb{A,C}\right] \\
\left[ \mathbb{A},\left[  \mathbb{B,C}\right]  \right]  &  =
-\left[\mathbb{B},\left[  \mathbb{C,A}\right] \right]  
-\left[\mathbb{C},\left[\mathbb{A,B}\right] \right] \,.
\end{align*}

Un par de ejemplos inmediatos se derivan de la composición de operadores:
\begin{enumerate}
\item \textbf{Potencias de operadores}. Uno de los ejemplos más útiles en la composición de operadores lo constituyen las potencias de los operadores, las cuales provienen de la aplicación consecutiva de un mismo operador,
\[
\mathbb{A}^{0}=\mathbb{I}\,;\qquad\mathbb{A}^{1}=\mathbb{A}\,;\qquad  \mathbb{A}^{2}=
\mathbb{AA}\,;\qquad\mathbb{A}^{3}=\mathbb{A}^{2}\mathbb{A}=\mathbb{AAA}\,\,\, \cdots
\]
es claro que las potencias de operadores cumplen las propiedades estándares de las potencias de números
\[
\mathbb{A}^{n+m}=\mathbb{A}^{n}\mathbb{A}^{m}{;} \qquad
\left(\mathbb{A}^{n}\right)^{m}=\mathbb{A}^{nm}\,.
\]
Llamaremos \textit{operadores nilpotentes de grado }$n$ a los operadores
$\mathbb{A}^{n}\neq{ 0}$, del tipo $\mathbb{A}^{n}\left| {v}\right> =\left|  {0}\right> \, \forall \,\left| {v}\right> \in\textbf{\em V}_{1}$  y $\left|  {0}\right> \in\textbf{\em V}_{2}$. Es decir, un operador que lleva cualquier vector $\left|  {v}\right> $ al elemento neutro de $\textbf{\em V}_{2}$. El ejemplo más notorio es el operador diferencial
\[
\mathbb{D}^{n}\left|  {P}^{n-1}\right> =\left| {0}
\right> \quad\leftrightarrows\quad\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}
}P_{n-1}(x)=\frac{\mathrm{d}^{n}}{\mathrm{d}x^{n}}\left[  a_{i}x^{i}\right]
=0\,,
\]
con $\left| {P}^{n-1}\right>$ perteneciente al espacio de polinomios de grado $n-1$.

\item \textbf{Operador ecuaciones diferenciales}. Si consideramos el espacio de funciones  $f(x)$ $\in\mathcal{C}_{\left[a,b\right]}^{\infty}$ podemos construir un operador diferencial
\[
\left[  a_{0}+a_{1}\mathbb{D}+a_{2}\mathbb{D}^{2}+\cdots
+a_{n}\mathbb{D}^{n}\right]  \left|  {f}\right>  \,\, \Rightarrow \,\,
\left(  a_{0}+a_{1}\frac{\mathrm{d}}{\mathrm{d}x}
+a_{2}\frac{\mathrm{d}^{2}}{\mathrm{d}x^{2}}+\cdots+a_{n}\frac{\mathrm{d}^{n}
}{\mathrm{d}x^{n}}\right)  f(x)\,,
\]
con $\left\{a_{0},a_{1},a_{2},\cdots a_{n}\right\}$ coeficientes
constantes. De este modo, por ejemplo:
\[
\left(  \mathbb{D}^{2}-3\mathbb{D}+2\right)  y=\left(  \mathbb{D}-1\right)\left(  \mathbb{D}-2\right)y 
\,\, \Rightarrow \,\, \left(\frac{\mathrm{d}^{2}}{\mathrm{d}x^{2}}-3\frac{\mathrm{d}}{\mathrm{d}x}+2\right)
y(x)  =y^{\prime\prime}-3\ y^{\prime}+2\ y \,.
\]
\end{enumerate}

\subsection{Proyectores}
La notación de Dirac se hace particularmente conveniente para representar proyectores. Hasta ahora, hemos relacionado un funcional lineal, un \textit{bra} $\left< {w}\right|$ del espacio dual
$\textbf{\em V}^{\ast}$, con un vector \textit{ket} $\left| v \right> $ del espacio vectorial directo $\textbf{\em V}$ a través de su producto interno $\left< {w}\right|  \left.  {v}\right>$, el cual es, en general, un número complejo. Ahora escribiremos esta relación entre vectores y formas diferenciales de una manera diferente: la relación entre $\left< {w} \right|$ y $\left|{v}\right> $ un \textit{ket} $\left|{\Psi}\right>$ o un \textit{bra} $\left< {\Phi}\right|$ arbitrarios puede ser:
\[
\left|  {v}\right> \left< {w}\right| 
\,\, \Rightarrow \,\, 
\left\{
\begin{array}
[c]{c}
\left|  {v}\right> \left< {w}\right|  \left.
{\Psi}\right> \\
\\
\left< {\Phi}\right.  \left|  {v}\right>
\left< {w}\right|
\end{array}
\right.
\]

La primera será la multiplicación del vector $\left|  {v}\right>$ por el número complejo $\left< {w}\right|
\left.  {\Psi}\right>$, mientras que la segunda relación será la multiplicación de la forma $\left< {w}\right| $ por el complejo $\left< {\Phi}\right.  \left|  {v}\right>$. Es imperioso señalar que el orden en la escritura de los vectores y formas es crítico, sólo los números complejos $\lambda$ se pueden mover con impunidad a través de estas relaciones.
\begin{align*}
\lambda\left|  {v}\right>  &  =\left|  \lambda{v}
\right> =\left|  {v}\right> \lambda, \quad \lambda \left< {w}\right|  = \left< \lambda{w}\right|
= \left< {w}\right|  \lambda\\
\left< {w}\right|  \lambda\left|  {v}\right>  & = \lambda\left< {w}\right|  \left.  {v}\right>
=\left< {w}\right|  \left.  {v}\right> \lambda
\qquad \text{y} \qquad \mathbb{A}\left|  \lambda{v}\right>
= \mathbb{A}\lambda\left|  {v}\right> =\lambda\mathbb{A}\left|
{v}\right> \,.
\end{align*}

Por lo tanto, dado un vector $\left|  {v}\right>$, podemos construir un proyector $\mathbb{P}_{\left|  {v}\right> }$ a lo largo del vector $\left|  {v}\right> $,
\[
\mathbb{P}_{\left|  {v}\right> }\equiv\left|  {v} \right> \left< {v}\right|\,,  \quad 
\text{con }\left< {v}\right|  \left.  {v}\right> =1\,,
\]
siempre y cuando este operador lineal cumpla:
\begin{eqnarray*}
\mathbb{P}_{\left|  {v}\right> }\left[  \alpha\ \left| {z}_{{1}}\right> +\beta\ \left|  {z}_{2}\right> \right]   &  = &\alpha\ \mathbb{P}_{\left|  {v} \right> }\left|  {z}_{{1}}\right> +\beta\ \mathbb{P}_{\left|  {v}\right> }\left|  {z}_{2}\right>\,, \\
\left|  {v}\right> \left< {v}\right|  \left[ \alpha\ \left|  {z}_{{1}}\right> +\beta\ \left| {z}_{2}\right> \right]   &  = &\left|  {v}\right> \left< {v}\right|  \alpha\ \left|  {z}_{1}\right> +\left|  {v}\right> \left< {v} \right|  \beta\ \left|  {z}_{2}\right> =\alpha\ \left| {v}\right> \left< {v}\right.  \left|  {z}
_{{1}}\right> +\beta\ \left| v\right> \left< {v}\right.\left| z_{2}\right> \,,
\end{eqnarray*}
además:
\begin{eqnarray*}
\mathbb{P}_{\left|  {v}\right> }^{2}  &  =& \mathbb{P}_{\left| {v}\right> }\quad\Longleftrightarrow\quad\left(  \left| {v}\right> \left< {v}\right|  \right)  \left( \left|  {v}\right> \left< {v}\right|  \right)=\left|  {v}\right> \left< {v}\right| \\
\mathbb{P}_{\left|  {v}\right> }\ \mathbb{P}_{\left| {v}\right> }\left|  {z}\right>  &  = & 
\left(  \left| {v}\right> \left< {v}\right|  \right)  \left( \left|  {v}\right> \left< {v}\right|  \right)
\left|  {z}\right> =\left|  {v}\right> \underset {1}{\underbrace{\left< {v}\right.  \left|  {v}
\right> }}\left< {v}\right.  \left|  {z} \right> = \left|  {v}\right> \left< {v}\right.
\left|  {z}\right> =\mathbb{P}_{\left|  {v}\right> }\left|  {z}\right>\,.
\end{eqnarray*}

Así, el operador $\mathbb{P}_{\left|  {v}\right> }$ actuando sobre el vector $\left|  {\Psi}\right> $ representará la proyección de $\left|  {\Psi}\right> $ a lo largo de $\left| {v}\right> $
\[
\mathbb{P}_{\left|  {v}\right> }\ \left|  {\Psi}\right> =\left|  {v}\right> \left< {v}\right|  \left.  {\Psi}\right> \equiv\left< {v} \right|  \left.  {\Psi}\right> \left|  {v}\right> \,.
\]

Es inmediato construir un proyector de un vector sobre un subespacio $\textbf{\em S}_{q}$.  

Sea: $\left\{  \left|  {\mathrm{e}}_{1}\right>, \left| \mathrm{e}_{2}\right>, \left|  {\mathrm{e}}_{3}\right>,\cdots,\left|  {\mathrm{e}}_{q}\right> \right\} $ un conjunto ortonormal de vectores que expande $\textbf{\em S}_{q}$, por lo tanto, definiremos el proyector $\mathbb{P}_{q}$ al proyector sobre el subespacio $\textbf{\em S}_{q}$ de la forma:
\[
\mathbb{P}_{q}=\left|  {\mathrm{e}}_{i}\right> \left< {\mathrm{e}}^{i}\right|  _{q}\,,
\]
es claro que  $ \forall \,\, \left| {v}\right> \in\textbf{\em V}$ se tiene:
\[
\mathbb{P}_{q}^{2}\left|  {v}\right> =\mathbb{P}_{q}\mathbb{P}_{q}\left|  {v}\right> \,\, \Rightarrow \,\,
\mathbb{P}_{q}^{2}\left|  {v}\right> =\left(  \left| {\mathrm{e}}_{i}\right> \left< {\mathrm{e}}^{i}\right|  _{q}\right)
\left(  \left|  {\mathrm{e}}_{j}\right> \left< {\mathrm{e}}^{j}\right|  _{q}\right)  \left|  {v}\right> =\left|
{\mathrm{e}}_{i}\right> \overset{\delta_{j}^{i}}{\overbrace{\left< {\mathrm{e}}^{i}\right.  \left|  \mathrm{e}_{j}\right> }}\left< {\mathrm{e}}^{j}\right.  \left|  {v}\right>=
\left|  {\mathrm{e}}_{j}\right> \left< {\mathrm{e}}^{j}\right.  \left|  {v}
\right> \equiv \mathbb{P}_{q}\left|  {v}\right> \,,
\]
es decir, $\mathbb{P}_{q}^{2}= \mathbb{P}_{q}$.

\subsection{{\color{Fuchsia}Ejemplos}}
\begin{enumerate}
\item Dados dos operadores lineales tales que: 
$
\mathbb{A}\mathbb{B} = -\mathbb{B} \mathbb{A}, \quad \mathbb{A}^{2} = \mathbb{I}, \quad \mathbb{B}^{2} = \mathbb{I}$  y  $ \left[\mathbb{A}, \mathbb{B}  \right] = 2i\mathbb{C}\,.
$ 
\begin{enumerate}
\item Mostraremos que: $ \mathbb{C}^{2} = \mathbb{I}$ y que $\left[\mathbb{B}, \mathbb{C}  \right] = 2i\mathbb{A}$.
\begin{eqnarray*}
\left[\mathbb{A}, \mathbb{B}  \right] &=& \mathbb{A}\mathbb{B}  -\mathbb{B} \mathbb{A}  = 2i\mathbb{C}  \,\, \Rightarrow \,\, 2 \mathbb{A}\mathbb{B}  = 2i\mathbb{C}  \,\, \Rightarrow \,\,  \mathbb{A}\mathbb{B} \mathbb{A}\mathbb{B}  = -\mathbb{C}^{2}  \,\, \Rightarrow \,\,  -\mathbb{A}\mathbb{A}\mathbb{B} \mathbb{B}  = -\mathbb{C}^{2}  \,\, \Rightarrow \,\,  \mathbb{I}   = \mathbb{C}^{2} \\ 
 \left[\mathbb{B}, \mathbb{C}  \right]  &=& -i\left(\mathbb{B}\mathbb{A}\mathbb{B}  -\mathbb{A}\mathbb{B} \mathbb{B} \right)   = 2i\mathbb{A}   \,\, \Rightarrow \,\, -i\left(\mathbb{B}\mathbb{A}\mathbb{B}  -\mathbb{A}\right)   = 2i\mathbb{A}  \,\, \Rightarrow \,\, -i\left(-\mathbb{B}\mathbb{B}\mathbb{A}  -\mathbb{A}\right)   = 2i\mathbb{A}
\end{eqnarray*}

\item Seguidamente, evaluamos $\left[  \left[  \left[ \mathbb{A}, \mathbb{B} \right] ,\left[\mathbb{B}, \mathbb{C}  \right] \right],  \left[ \mathbb{A}, \mathbb{B} \right] \right] $.
\begin{eqnarray*}
\left[  \left[  \left[ \mathbb{A}, \mathbb{B} \right] ,\left[\mathbb{B}, \mathbb{C}  \right] \right],  \left[ \mathbb{A}, \mathbb{B} \right] \right] 
&=& \left[  \left[  2i\mathbb{C}  , 2i\mathbb{A}\right],  2i\mathbb{C} \right] = 8\left[  \left[  \mathbb{A}\mathbb{B}  , i\mathbb{A}\right],   \mathbb{A}\mathbb{B} \right] = 8i \left[ \left(  \mathbb{A}\mathbb{B} \mathbb{A} -  \mathbb{A}   \mathbb{A}\mathbb{B}   \right),  \mathbb{A}\mathbb{B} \right] \\
& =& 8i  \left[ -2\mathbb{B} , \mathbb{A}\mathbb{B} \right] = 16i \left( \mathbb{B} \mathbb{A}\mathbb{B} - \mathbb{A}\mathbb{B}\mathbb{B} \right) = 32i\mathbb{A} \,.
\end{eqnarray*} 
\end{enumerate}

\item  Dados dos operadores vectoriales $\mathbb{{A}}$ y $\mathbb{{B}}$ que conmutan entre ellos y con $\mathbb{{L}}$, tales que:
\[
\left[  \mathbb{{A}},\mathbb{{B}}\right]  = \left[  \mathbb{{A}},\mathbb{{L}}\right]  =
\left[  \mathbb{{L}},\mathbb{{B}}\right] =0\,.
\]
Demostramos entonces la relación:
\[
\left[ \mathbb{A}\cdot \mathbb{L}, \mathbb{B} \cdot \mathbb{L}\right] =
i\hbar\left(   \mathbb{A}\times \mathbb{B}\right)  \cdot \mathbb{L} \,.
\]
Veamos:
\begin{align*}
\left[   \mathbb{A}\cdot \mathbb{L}, \mathbb{B}\cdot
 \mathbb{L}\right]   & =i\hbar \varepsilon_{klm}{A}^{l}{B}^{m}{L}^{k}=
{A}^{l}{B}^{m}i\hbar\varepsilon
_{klm}{L}^{k}={A}^{l}{B}^{m}\left[  {L}
_{l},{L}_{m}\right]  ={A}^{l}{B}^{m}{L}
_{l}{L}_{m}-{A}^{l}{B}^{m}{L}_{m}b^{m}{L}_{l}\\
&  ={A}^{l}{L}_{l}{B}^{m}{L}_{m}-{B}
^{m}{L}_{m}{A}^{l}{L}_{l} \,.
\end{align*}

\item  En Mecánica Clásica la cantidad de momento angular viene definida como $\mathbf{L}=\mathbf{r}\times\mathbf{p}$. Para pasar a Mecánica Cuántica se asocia $\mathbf{r}$ y $\mathbf{p}$ con los operadores posición y cantidad de movimiento los cuales, al operar sobre la función de onda nos proveen:
\[
\begin{array}
[c]{ccc}
\left< {r}\right|  \mathbb{X}\left|  \psi \right> =x\left< {r}\right.  \left|  \psi\right> =x\ \psi( \mathbf{r} )   & \quad & \left< {r}\right|  \mathbb{P}_{x}\left|  \psi\right> =\left(  -i\hbar\ \dfrac{\partial}{\partial x}\right)  \left< {r}\right.  \left|  \psi\right>
=-i\hbar\ \dfrac{\partial}{\partial x}\psi(\mathbf{r})  \\
&  & \\
\left< {r}\right|  \mathbb{Y}\left|  \psi\right> = y\left< {r}\right.  \left|  \psi\right> =y\ \psi\left( \mathbf{r}\right)   &  & \left< {r}\right|  \mathbb{P}_{y}\left| \psi \right> =\left(  -i\hbar\ \dfrac{\partial}{\partial y}\right)
\left< {r}\right.  \left|  \psi\right> =-i\hbar\ \dfrac{\partial}{\partial y}\psi(\mathbf{r})  \\
&  & \\ 
\left< {r}\right|  \mathbb{Z}\left|  \psi\right> = z\left< {r}\right.  \left|  \psi\right> =z\ \psi\left( \mathbf{r}\right)   &  & \left< {r}\right|  \mathbb{P}_{z}\left| \psi\right> =\left(  -i\hbar\ \dfrac{\partial}{\partial z}\right) \left< {r}\right.  \left|  \psi\right> =-i\hbar \ \dfrac{\partial}{\partial z}\psi(\mathbf{r}) \,.
\end{array}
\]
En coordenadas cartesianas, en la representación de coordenadas 
$\left\{  \left|  {r}\right> \right\}$ tendremos que:
\[
\left< {r}\right|  \mathbb{R}\left|  \psi\right> = \mathbf{r}\ \psi(\mathbf{r})  \quad\text{y} \quad \left<{r}\right|  \mathbb{P}_{x}\left|  \psi\right> =-i\hbar \ \boldsymbol{\nabla}\ \psi(\mathbf{r}) \,.
\]

De forma que en Mecánica Cuántica las componentes cartesianas del operador cantidad de movimiento angular son 
\begin{align*}
\left< {r}\right|  \mathbb{{L}}\left|  \psi\right>  & =-i\hbar\ \left(  \mathbf{r\times}\boldsymbol{\nabla}\right)  \psi(\mathbf{r})\,,  \\
& \\
\left< {r}\right|  \mathbb{{L}}\left|  \psi\right>  &
=-i\hbar\ \left(  y\dfrac{\partial}{\partial z}-z\dfrac{\partial}{\partial
y}\right)  \psi(\mathbf{r})  \mathbf{i}-
i\hbar\ \left(z\dfrac{\partial}{\partial x}-x\dfrac{\partial}{\partial z}\right)
\psi(\mathbf{r})  \mathbf{j}-i\hbar\ \left(
x\dfrac{\partial}{\partial y}-y\dfrac{\partial}{\partial x}\right)
\psi(\mathbf{r})  \mathbf{{k}} \,.
\end{align*}

Utilizando las definiciones anteriores mostraremos que el conmutador de las componentes cartesianas de la cantidad de movimiento angular cumple con:
\[
\left[  \mathbb{L}_{x},\mathbb{L}_{y}\right]  \left|  \psi\right>
=i\hbar\mathbb{L}_{z}\left|  \psi\right>\,,
\]
donde: 
$\mathbb{L}^{1}=\mathbb{L}_{1}=\mathbb{L}_{x};$ $\mathbb{L}^{2}
=\mathbb{L}_{2}=\mathbb{L}_{y};$ $\mathbb{L}^{3}=\mathbb{L}_{3}=\mathbb{L}_{z}$.  En general: 
$\left[  \mathbb{L}_{l},\mathbb{L}_{m}\right]  =i\hbar \varepsilon_{lmn}\mathbb{L}^{n}$. 

Dado que:
\[
\left[  \mathbb{L}_{1},\mathbb{L}_{2}\right]  \left|  \psi\right> 
=\left[ \mathbb{L}_{x},\mathbb{L}_{y}\right]  \left|  \psi\right>
=\left( \mathbb{L}_{x}\mathbb{L}_{y}-\mathbb{L}_{y}\mathbb{L}_{x}\right)\left|  \psi\right> \,,
\]
entonces:
\begin{eqnarray*}
\left[ \mathbb{L}_{x},\mathbb{L}_{y}\right]  \left|  \psi\right>&=&
\left[  \left(  y\dfrac{\partial}{\partial z}-z\dfrac{\partial
}{\partial y}\right)  \left(  z\dfrac{\partial}{\partial x}-x\dfrac{\partial
}{\partial z}\right)  -\left(  z\dfrac{\partial}{\partial x}-x\dfrac{\partial
}{\partial z}\right)  \left(  y\dfrac{\partial}{\partial z}-z\dfrac{\partial
}{\partial y}\right)  \right)  \psi(\mathbf{r}] \\
& =& 
\left[y\dfrac{\partial}{\partial z}\left(  z\dfrac{\partial}{\partial x}-x\dfrac{\partial}{\partial z}\right) -z\dfrac{\partial}{\partial y}\left(z\dfrac{\partial}{\partial x}-x\dfrac{\partial}{\partial z}\right)\right. \\
& -& 
\left.
 z\dfrac{\partial}{\partial x}\left( y\dfrac{\partial}{\partial z}-z\dfrac{\partial}{\partial y}\right)+x\dfrac{\partial}{\partial z}\left(  y\dfrac{\partial}{\partial z}-z\dfrac{\partial}{\partial y}\right)    \right] 
  \psi\left(\mathbf{r}\right) \,.
\end{eqnarray*}

Con lo cual:
\begin{eqnarray*}
\left[ \mathbb{L}_{x},\mathbb{L}_{y}\right]  \left|  \psi\right>&=&
\left[  \left(  \left(  yz\dfrac{\partial}{\partial z\partial
x}+y\dfrac{\partial}{\partial x}\right)  -xy\dfrac{\partial^{2}}{\partial
z^{2}}\right)  -\left(  z^{2}\dfrac{\partial}{\partial y\partial x}-zx\dfrac{\partial}{\partial y\partial z}\right)  \right.  \\
&-&  \left.  \left(  \left(  zy\dfrac{\partial}{\partial x\partial
z}-z^{2}\dfrac{\partial}{\partial y\partial x}\right)  -\left(  yx\dfrac
{\partial}{\partial z^{2}}-zx\dfrac{\partial}{\partial z\partial y}
-x\dfrac{\partial}{\partial y}\right)  \right)  \right]  \psi(\mathbf{r}) 
=\left(  y\dfrac{\partial}{\partial x}-x\dfrac{\partial}{\partial
y}\right)  \psi(\mathbf{r}) \,.
\end{eqnarray*}

\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}}
\begin{enumerate}
\item Si tenemos la siguiente transformación, $\left|x^{\prime}\right>=\mathbb{T}\left| x \right> $: 
\[
\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^4  \,, \,\,
\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(2x+y,x-z,x+y+z,y+z\right)\,,
\]
entonces, para que sea una transformación lineal debe satisfacer lo siguiente:
\begin{itemize}
\item $\mathbb{T}\left[ \left|  {v}_{1}\right> +\left| {v}_{2}\right> \right]=
\mathbb{T}\left[ \left|  {v}_{1}\right>\right]+\mathbb{T}\left[ \left|  {v}_{2}\right>\right]$.
\item $\mathbb{T}\left[\alpha \left| {v}\right>\right]= \alpha
\mathbb{T}\left[ \left|  {v}\right>\right]$.
\end{itemize}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f(x,y,z):=[2*x+y,x-z,x+y+z,y+z];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
f(x,y,z):=[2x+y,x-z,x+y+z,y+z]
\end{math}
\newline

El lado derecho de la igualdad es:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ld:f(x1+x2,y1+y2,z1+z2),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\left[ { y_2}+{ y_1}+2\,{ x_2}+2\,{ x_1} , -\left(
 { z_2}+{ z_1}-{ x_2}-{ x_1}\right) , { z_2}+{ z_1}
 +{ y_2}+{ y_1}+{ x_2}+{ x_1} , { z_2}+{ z_1}+
 { y_2}+{ y_1} \right] 
\end{math}
\newline

El lado izquierdo,

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
li:f(x1,y1,z1)+f(x2,y2,z2),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ {  y_2}+{  y_1}+2\,{ x_2}+2\,{ x_1} , -\left(
 { z_2}+{ z_1}-{ x_2}-{ x_1}\right) , { z_2}+{ z_1}
 +{ y_2}+{ y_1}+{ x_2}+{ x_1} , { z_2}+{ z_1}+
 { y_2}+{ y_1} \right] 
\end{math}
\newline

Por lo tanto:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ld-li,ratsimp;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\left[ 0 , 0 , 0 , 0 \right] 
\end{math}
\newline

Para la segunda condición

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f(alpha*x,alpha*y,alpha*z)-alpha*f(x,y,z);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ -\alpha\,\left(y+2\,x\right)+\alpha\,y+2\,\alpha\,x , -
 \alpha\,z-\alpha\,\left(x-z\right)+\alpha\,x , -\alpha\,\left(z+y+x
 \right)+\alpha\,z+\alpha\,y+\alpha\,x , \right. \\
 \left. 
 \qquad \qquad -\alpha\,\left(z+y\right)+  \alpha\,z+\alpha\,y \right]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
factor(%);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\left[ 0 , 0 , 0 , 0 \right] 
\end{math}
\newline

\item Consideremos ahora la siguiente transformación, $\left|x^{\prime}\right>=\mathbb{T}\left| x \right> $: 
\[
\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^3  \,, \,\,
\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(x^2,y+z,z^2\right) \,.
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
f(x,y,z):=[x^2,y+z,z^2];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
f(x,y,z):=[x^2,y+z,z^2];
\end{math}
\newline

El lado derecho:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ld:f(x1+x2,y1+y2,z1+z2),factor;
\end{verbatim}}
\end{minipage}
 
%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\left[ \left({  x_2}+{ x_1}\right)^2 , { z_2}+{ z_1}+
 { y_2}+{ y_1} , \left({ z_2}+{ z_1}\right)^2 \right] 
\end{math}
\newline

El lado izquierdo:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
li:f(x1,y1,z1)+f(x2,y2,z2),factor;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\left[ { x_2}^2+{ x_1}^2 , { z_2}+{ z_1}+{ y_2}+
 { y_1} , { z_2}^2+{ z_1}^2 \right] 
\end{math}
\newline

Por lo tanto: 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ld-li,ratsimp;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\left[ 2\,{ x_1}\,{ x_2} , 0 , 2\,{ z_1}\,{ z_2}
  \right] 
\end{math}
\newline

No es una transformación lineal. 

\item Dadas las siguientes transformaciones de $\mathds{R}^3 \rightarrow \mathds{R}^3$:
\[
\mathbb{A}\left[ \left(x,y,z\right) \right]=\left(2x-z,x+z,x\right)\,, \,\,
\mathbb{B}\left[ \left(x,y,z\right) \right]=\left(z,x,y\right)\,.
\]

Veamos si conmutan, es decir, $[\mathbb{A},\mathbb{B}]=\mathbb{A}\mathbb{B}-\mathbb{B}\mathbb{A}=0$. Podemos ver que $\mathbb{A}\mathbb{B}=\mathbb{A}[\mathbb{B}\left| x \right>]=\left(2z-y,z+y,z\right)$ y $\mathbb{B}\mathbb{A}=\mathbb{B}[\mathbb{A}\left| x \right>]=\left(x,2x-z,x+z\right)$. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A(x,y,z):=[2x-z,x+z,x];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
A(x,y,z):=[2x-z,x+z,x]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
B(x,y,z):=[z,x,y];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
B(x,y,z):=[z,x,y]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[x,y,z]: [z, x, y];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
[x,y,z]: [z, x, y]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
AB:A(x,y,z);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
[2z-y,z+y,z]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(x,y,z);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
\mbox{done}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[x,y,z]:[2*x-z, x+z,x];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
[2x-z,z+x,x]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
BA:B(x,y,z);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
[x,2x-z,z+x]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
AB-BA;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
\left[ 2\,z-y-x , 2\,z+y-2\,x , -x \right]
\end{math}
\newline

Por lo tanto no conmutan.

\end{enumerate}

\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}



\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Diga si las siguientes transformaciones, $\left|x^{\prime}\right>=\mathbb{T}\left| x \right>$ son lineales
\begin{enumerate}
\item $\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^2$, 
$\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(x+y,x+z\right) $.

\item $\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^3$, 
$\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(x,y,y+z\right) $.

\item $\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^3$, 
$\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(x,x+y,x-y\right) $. 

\item $\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^4$, 
$\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(x+y,x+z,2x+y+z,y-z\right) $.

\item $\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^3$, 
$\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(\mbox{sen}(x),\cos(y),0 \right) $.

\end{enumerate}

\item Cuál de las siguientes transformaciones definidas sobre 
$\textbf{\em V}^3$ son lineales
\begin{enumerate}
\item $\mathbb{T}\left|  {x}\right> = \left|  {x}\right> +
\left|  {a}\right>$ donde $\left|  {a}\right>$ es un vector constante diferente de cero. 
\item $\mathbb{T}\left|  {x}\right> =\left|  {a}\right>$.
\item $\mathbb{T}\left|  {x}\right> =\left< {a}\right.  \left|  {x}\right>  \left|  {a}\right>$.
\item $\mathbb{T}\left|  {x}\right> =\left< {a}\right.  \left|  {x}\right>  \left|  {x}\right>$.
\end{enumerate}
\item Considere las siguientes operaciones en el espacio de los polinomios en $x$ y diga si corresponden a transformaciones lineales:

$a) $ La multiplicación por $x$. $\quad$ 
$b) $ La multiplicación por $x^2$. $\quad$ 
$b) $ La diferenciación.

\item Suponga que $\mathbb{A}\mathbb{B}=\mathbb{B}\mathbb{A}$. Demuestre que:
\begin{enumerate}
\item $\left( \mathbb{A}+\mathbb{B} \right)^2=\mathbb{A}^2+2\mathbb{A}\mathbb{B}+\mathbb{B}^2$.
\item $\left(\mathbb{A}+\mathbb{B}\right)^3=\mathbb{A}^3+3\mathbb{A}^2\mathbb{B}+3\mathbb{A}\mathbb{B}^2+\mathbb{B}^3$.
\end{enumerate}
¿Cómo cambian las fórmulas anteriores si $\mathbb{A}\mathbb{B}\neq\mathbb{B}\mathbb{A}$?


\item Suponga que un operador $\mathbb{L}$ puede ser escrito como la composición de otros dos operadores $\mathbb{L} = \mathbb{L}_{-} \mathbb{L}_{+}$ con $[\mathbb{L}_{-}, \mathbb{L}_{+}] = \mathbb{I}$. Demostrar que:
\[
\mathrm{Si} \quad \mathbb{L} \left| x\right>  = \lambda \left| x\right>  \quad \mathrm{y} \quad  \left| y\right> =  \mathbb{L}_{+} \left| x\right>  \quad \mathrm{entonces} \quad  \mathbb{L} \left| y\right> = (\lambda + 1) \left| y\right> 
\]
y, del mismo modo, demuestre que:
\[
\mathrm{Si} \quad \mathbb{L} \left| x\right> = \lambda \left| x\right>  \quad \mathrm{y} \quad \left| z\right>=  \mathbb{L}_{-} \left| x\right>  \quad \mathrm{entonces} \quad  \mathbb{L} \left| z\right> = (\lambda - 1) \left| z\right> \,.
\]

Por ello es costumbre denominar a $\mathbb{L}_{+}$ y $\mathbb{L}_{-}$ los operadores de ``subidas'' y de ``bajada'' respectivamente, ya que ellos construyen otros vectores con autovalores mayores (menores) en una unidad al vector operado.

\item  Resuelva los problemas anteriores utilizando {\bf Maxima}.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%
\section{Tipos de operadores}
\label{Tiposdeoperadores}

Discutiremos en esta sección algunas de las propiedades más resaltantes que caracterizan a los operadores lineales. Además, tal y como están definidas las transformaciones lineales tiene sentido estudiar si ellas poseen la característica de ser: inyectivas, sobreyectivas y biyectivas. Las transformaciones lineales tendrán nombres particulares para cada uno de estos casos. 

Como ya fue señalado, una transformación lineal es una función, aplicación, operador o mapeo cuyos dominios y rangos son subconjuntos de espacios vectoriales y que hemos simbolizado como:  
\[
\mathbb{A}: \textbf{\em V}_1 \rightarrow \textbf{\em V}_2  \,\, \Rightarrow \,\, \mathbb{A}\left|{v}\right> =\left|  {v'}\right> \,,
\mbox{con } \,\,\, \left|{v}\right> \in \textbf{\em V}_1 \,\, \mbox{ y } \,\, 
\left|  {v'}\right> \in \textbf{\em V}_2 \,.
\] 

En en lenguaje de la teoría de conjuntos a los elementos  $\left|{v'}\right>\in \textbf{\em V}_2 $ se les denomina la imagen de $\left|{v}\right>$  debido a $\mathbb{A}$. Si $\textbf{\em S}$ es un subconjunto de $\textbf{\em V}_1 $, entonces al conjunto de todas la imágenes, que denotaremos por $\mathbb{A}(\textbf{\em S})$, se le denomina la imagen de $\textbf{\em S}$ debido a la aplicación de $\mathbb{A}$ en $\textbf{\em V}_1 $. A la imagen del dominio de $\textbf{\em V}_1$, $\mathbb{A}\{\textbf{\em V}_1\}$, se le llama el rango de $\mathbb{A}$ y es un subespacio de $\textbf{\em V}_2 $.


%%%%%%%%%%%%%%%%%%%
\subsection{Espacio nulo e imagen de un operador}
\label{EspacioNulo}
\index{Espacio!Nulo}
\index{Espacio!Imagen}
\index{Nulo!Espacio}


El conjunto de todos los $\left|  {v}\right> \in\textbf{\em V}_{1} \, / \, \mathbb{A}\left|  {v}\right> =\left|  {0}\right>$, se denomina espacio nulo, núcleo o \textit{kernel} (núcleo en alemán) de la transformación $\mathbb{A}$ y lo denotaremos como $\aleph\left(\mathbb{A}\right)$. En símbolos diremos que:
\[
\aleph \left(  \mathbb{A}\right)  =\left\{  \left|  {v}\right> \in\textbf{\em V}_{1} \,\, \wedge \,\, \mathbb{A}\left|  {v}\right> =\left|  {0} \right> \right\} \,.
\]

Adicionalmente, $\aleph\left(\mathbb{A}\right)  \subset\textbf{\em V}_{1}$, es decir, será un subespacio de $\textbf{\em V}_{1}$. La prueba de esta afirmación es inmediata. Dados $\left|  {v}_{1}\right> ,\left| {v}_{2}\right> \in\aleph\left(  \mathbb{A}\right)$, con $\mathbb{A}$ un operador lineal, es claro que:
\[
\left.
\begin{array}
[c]{c}
\mathbb{A}\left|  {v}_{1}\right> =\left|  {0}\right>
\\
\\
\mathbb{A}\left|  {v}_{2}\right> =\left|  {0}
\right>
\end{array}
\right\} \,\, \Rightarrow \,\, \alpha_{1}\ \mathbb{A}\left|  {v}_{1}\right> +\alpha_{2}\ \mathbb{A}\left|  {v}_{2}\right>=
\left|  {0}\right> =\ \mathbb{A}\left(  \alpha_{1}\ \left| {v}_{1}\right> +\alpha_{2}\ \left|  {v}_{2}\right>
\right)\,,
\]
por la misma razón se tiene que el elemento neutro está contenido en $\aleph\left(  \mathbb{A}\right)$, esto es:
\[
\mathbb{A}\left|  \alpha\ {v}\right> =\left|  {0} \right> \,\, \forall \,\, \left|  {v}\right> \in\textbf{\em V}_{1} \,\, \wedge \,\,  \,\, \forall \,\, \alpha 
\,\, \therefore \,\, \mathbb{A}\left|{0}\right> =\left|  {0}\right> \quad\text{si } \quad\alpha=0\,,
\]
por lo tanto, queda demostrado que $\aleph \left( \mathbb{A}\right) $ es un subespacio de $\textbf{\em V}_{1}$.
 
Por otra parte, definiremos la imagen (rango o recorrido) de $\mathbb{A}$, y la denotaremos como:
\[
\mathbb{A}\left\{\textbf{\em V}\right\}=
\left\{ \left|  {v}^{\prime}\right> \in\textbf{\em V}_{2} \quad \wedge \quad \mathbb{A}\left|  {v}\right> =
\left| {v}^{\prime}\right> \right\}\,,
\]
igualmente $\Im\left( \mathbb{A}\right)  \subset\textbf{\em V}_{2}$ también será un subespacio de $\textbf{\em V}_{2}$ ya que si 
$\left|  {v} \right> =\alpha_{1}\ \left|  {v}_{1}\right> +\alpha
_{2}\ \left|  {v}_{2}\right> $ y dado que $\mathbb{A}$ es un
operador lineal, se cumple que:
\[
\mathbb{A}\left(  \underset{\left|  {v}\right> }{\underbrace
{\alpha_{1}\ \left|  {v}_{1}\right> +\alpha_{2}\ \left|
{v}_{2}\right> }}\right)  =\alpha_{1}\ \underset{\left|
{v}_{1}^{\prime}\right> }{\underbrace{\mathbb{A}\left|
{v}_{1}\right> }}+\alpha_{2}\ \underset{\left|  {v}
_{2}^{\prime}\right> }{\underbrace{\mathbb{A}\left|  {v}
_{2}\right> }}=\underset{\left|  {v}^{\prime}\right>
}{\underbrace{\alpha_{1}\ \left|  {v}_{1}^{\prime}\right>
+\alpha_{2}\ \left|  {v}_{2}^{\prime}\right> }} \,.
\]

Es claro que si $\textbf{\em V}$ es de dimensión finita $n$, el rango $\mathbb{A}\left\{\textbf{\em V}\right\} $ también será de dimensión finita $n$ y tendremos:
\[
\dim\left[  \aleph\left(  \mathbb{A}\right)  \right]  +
\dim\left[ \mathbb{A}\left\{\textbf{\em V}\right\} \right]  =
\dim\left[  \textbf{\em V}\right]\,,
\]
vale decir, que la dimensión del núcleo más la dimensión del
recorrido o imagen de una transformación lineal es igual a la
dimensión del dominio.

Para demostrar esta afirmación supongamos que $\dim\left[  \textbf{\em V}  \right]  =n$ y que  $\left\{  \left|  \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right>, \left|  \mathrm{e}_{3}\right> \cdots\left|  \mathrm{e}_{k}\right> \right\}  \in\textbf{\em V}$ es una base de $\aleph\left(  \mathbb{A}\right) $, donde $k=\dim\left[ \aleph \left(  \mathbb{A}\right)  \right]  \leq n$. 

Como el conjunto: 
$\left\{\left|  \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right>, \left| \mathrm{e}_{3}\right> \cdots\left|  \mathrm{e}_{k} \right> \right\}  \in\textbf{\em V}$, entonces  estos elementos forman una base y  por
lo tanto, son linealmente independientes. Ademas, necesariamente estos vectores formarán parte de una base mayor de $\textbf{\em V}$, esto es: $\left\{  \left| \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right>, \left| \mathrm{e}_{3}\right>, \cdots,\left|\mathrm{e}_{k}\right>, \left|  \mathrm{e}_{k+1}\right>, \cdots, \left|  \mathrm{e}_{k+r-1}\right>,\left|  \mathrm{e}_{k+r}\right> \right\} \in \textbf{\em V}$ será una base de $\textbf{\em V}$, donde $k+r=n$.

El esquema de la demostración será el siguiente:
\begin{itemize}
\item  primero probaremos que los $r$ elementos $\left\{  \mathbb{A}\left\{  \left|
\mathrm{e}_{k+1}\right> \right\}, \mathbb{A}\left\{  \left|
\mathrm{e}_{k+2}\right> \right\}, \cdots,\mathbb{A}\left\{  \left|
\mathrm{e}_{k+r-1}\right> \right\}, \mathbb{A}\left\{  \left|\mathrm{e}_{k+r}\right> \right\}  \right\}$ forman una base para
$\mathbb{A}\left\{  \textbf{\em V}\right\}$. Esto significa que estaríamos probando que: 
dim$\{\mathbb{A}\left\{  \textbf{\em V}\right\}\}=r$,  $k+r=n$, y por lo tanto (\ref{dimensionnucleo}).

\item  luego hay que demostrar que los elementos $\left\{\mathbb{A}\left\{\left|  \mathrm{e}_{k+1}\right> \right\}  ,\mathbb{A}\left\{  \left|  \mathrm{e}_{k+2}\right> \right\}  ,\cdots,\mathbb{A}\left\{  \left| \mathrm{e}_{k+r-1}\right> \right\}  ,\mathbb{A}\left\{  \left| \mathrm{e}_{k+r}\right> \right\}  \right\}$ son linealmente independientes.
\end{itemize}

Si los $r$ elementos $\left\{\mathbb{A}\left\{\left|  \mathrm{e}_{k+1}\right> \right\}  ,\mathbb{A}\left\{  \left|  \mathrm{e}_{k+2}\right> \right\}  ,\cdots,\mathbb{A}\left\{  \left| \mathrm{e}_{k+r-1}\right> \right\}  ,\mathbb{A}\left\{  \left| \mathrm{e}_{k+r}\right> \right\}  \right\}  $ expanden $\mathbb{A}
\left\{  \textbf{\em V}\right\} $ entonces cualquier elemento
\[
\left|  {w}\right> \in\mathbb{A}\left\{  \textbf{\em V}\right\}
\,\, / \,\, \left|  {w}\right> =\mathbb{A}\left|  {v}\right>
=C^{i}\left|  \mathbb{A}\mathrm{e}_{i}\right>\,, \quad\text{con }\left|
\mathbb{A}\mathrm{e}_{i}\right> =\mathbb{A}\left|  \mathrm{e}_{i}\right>\,.
\]
Ahora bien, analicemos con cuidado los límites de la suma implícita
del índice $i=1,2,\cdots,k+r$
\[
\left|  {w}\right> =C^{i}\left|  \mathbb{A}\mathrm{e}_{i}\right>
=\underset{=\left|  {0}\right> \quad\text{ya que }
\mathbb{A}\left|  \mathrm{e}_{1}\right> =\mathbb{A}\left|
\mathrm{e}_{2}\right> =\mathbb{A}\left|  \mathrm{e}_{3}\right>
\cdots=\mathbb{A}\left|  \mathrm{e}_{k}\right> =\left|  {0}
\right> }{\underbrace{C^{1}\left|  \mathbb{A}\mathrm{e}_{1}\right>
+C^{2}\left|  \mathbb{A}\mathrm{e}_{2}\right> +\cdots+C^{k}\left|  \mathbb{A}\mathrm{e}
_{k}\right> }}+C^{k+1}\left|  \mathbb{A}\mathrm{e}_{k+1}\right>
+\cdots+C^{k+r}\left|  \mathbb{A}\mathrm{e}_{k+r}\right> \,.
\]

Por lo tanto $\left\{  \mathbb{A}\left\{  \left|  \mathrm{e}_{k+1}
\right> \right\}  ,\mathbb{A}\left\{  \left|  \mathrm{e}_{k+2}
\right> \right\}  ,\cdots,\mathbb{A}\left\{  \left|  \mathrm{e}
_{k+r-1}\right> \right\}  ,\mathbb{A}\left\{  \left|  \mathrm{e}
_{k+r}\right> \right\}  \right\}  $ expanden $\mathbb{A}\left\{
\textbf{\em V}\right\}$. 

Ahora bien, para demostrar que son base, demostraremos
que son linealmente independientes. Para ello supondremos que:
\[
\exists\quad\left\{C^{k+1},C^{k+2},\cdots,C^{k+r}\right\}  \,\, /  \,\, C^{i}\left|\mathbb{A}\mathrm{e}_{i}\right> =0\,, \quad\text{con } i=k+1,k+2,\cdots,k+r \,,
\]
y tenemos que demostrar que $C^{k+1}=C^{k+2}=\cdots=C^{k+r}=0$. 

Entonces
\[
C^{i}\left|  \mathbb{A}\mathrm{e}_{i}\right> =C^{i}\mathbb{A}\left| \mathrm{e}_{i}\right> =\mathbb{A}\left(  C^{i}\left|  \mathrm{e}_{i}\right> \right)  =0\,, \quad\text{con }i=k+1,k+2,\cdots,k+r \,,
\]
esto significa que el elemento $\left|  {v}\right> =C^{i}\left|
\mathrm{e}_{i}\right> \in\aleph\left(  \mathbb{A}\right)$,  con $i=k+1,k+2,\cdots,k+r$. 

Con lo cual, dado que:
\[
 \,\, \forall \,\, \left|{v}\right> \in\aleph\left(\mathbb{A}\right)  ,\left|{v}\right> =C^{i}\left|  \mathrm{e}_{i}\right> \,,  \mbox{con } i=1,2,\cdots, r\,, 
 \]
entonces se puede hacer la siguiente resta:
\[
\left|  {v}\right> -\left|  {v}\right> =\sum_{i=1}^{k}C^{i}\left|  \mathrm{e}_{i}\right> -\sum_{i=k+1}^{k+r}
C^{i}\left|  \mathrm{e}_{i}\right>=\left|  0 \right>\,,
\]
y como los $\left\{  \left| \mathrm{e}_{1}\right>, \left|\mathrm{e}_{2}\right>, 
\left|  \mathrm{e}_{3}\right>, \cdots,\left|  \mathrm{e}_{k}\right>, \left|  \mathrm{e}_{k+1}
\right>, \cdots, \left|  \mathrm{e}_{k+r-1}\right>, \left|\mathrm{e}_{k+r}\right> \right\}$ son una base de $\textbf{\em V}$ entonces resulta que los coeficientes: $C^{k+1}=C^{k+2}=\cdots=C^{k+r}=0$.

Si el espacio vectorial $\textbf{\em V}$ es de dimensión infinita, entonces al menos uno de los dos subespacios $\aleph\left(\mathbb{A}\right)$ o $\mathbb{A}\left\{
\textbf{\em V}\right\}$ será de dimensión infinita.


A continuación ejemplificaremos algunos casos que ilustran lo que representa un espacio nulo.

\begin{enumerate}
\item \textbf{Transformación identidad}: Sea $\mathbb{I}:\textbf{\em V}_{1}
{\rightarrow}\textbf{\em V}_{2}$, la transformación identidad, entonces
\[
\quad \forall\quad \left|  {v}\right> \in\textbf{\em V}_{1} \,\, /  \,\,\mathbb{I}\left| {v}\right> =\left|  {v}\right> \,\, \Rightarrow \,\,
\aleph\left(  \mathbb{I}\right)  =\left\{\left|  {0}\right> \right\}
\subset\textbf{\em V}_{1}\,\, \wedge\,\, 
\mathbb{A}\left\{\textbf{\em V}\right\}\equiv\textbf{\em V}_{1}\,.
\]

\item \textbf{Sistemas de ecuaciones lineales}: En $\textbf{\em V}^{n}$ las soluciones a los
sistemas de ecuaciones lineales representan el espacio nulo, 
$\aleph\left(\mathbb{A}\right)$, para vectores de $\textbf{\em V}^{n}$
\[
\mathbb{A}\left|  {x}\right> =\left|  {0}\right>
\quad\leftrightarrows\quad\left(
\begin{array}
[c]{cccc}
A_{11} & A_{12} & \cdots &  A_{1n}\\
A_{21} & A_{22} &  & \cdots\\
\vdots &  & \ddots & \\
A_{n1} & A_{n2} &  & A_{nn}
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x_{1}\\
x_{2}\\
\vdots\\
x_{n}
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
0\\
0\\
\vdots\\
0
\end{array}
\right)  \leftrightarrows  A_{j}^{i}x_{i}=0 \,,
\]
con $j$ ecuaciones ($j=1,2,\cdots,n$). Recordemos que estamos utilizando la convención de Einstein para suma de índices. 

\item \textbf{Ecuaciones diferenciales ordinarias}: Sea $\mathcal{C}_{\left[ -\infty,\infty\right]  }^{2}$ el espacio vectorial de todas las funciones continuas doblemente diferenciables. Definimos $\mathbb{A}:\mathcal{C}
_{\left[  -\infty,\infty\right]}^{2} \rightarrow 
\mathcal{C}_{\left[ -\infty,\infty\right]}$ como la transformación lineal 
$\left( \mathbb{D}^{2}-{1}\right)  $ tal que para todas las $y(x)$ $\in \mathcal{C}_{\left[  -\infty,\infty\right]  }^{2}$ se cumple:
\[
\mathbb{A}\left|  {x}\right> =\left|  {0}\right> \quad \leftrightarrows \quad \left(  \mathbb{D}^{2}-{1}\right)
y(x)=0 \quad\leftrightarrows\quad \left( \frac{\mathrm{d}^{2}}{\mathrm{d}x^{2} }-1\right)  y(x)  =y^{\prime\prime}-y=0\,.
\]
El núcleo o espacio nulo de $\mathbb{A}$, $\aleph\left( \mathbb{A}\right)$, lo constituye el conjunto de soluciones para la mencionada ecuación diferencial. Por lo tanto, el problema de encontrar las soluciones de la ecuación diferencial es equivalente a encontrar los elementos del núcleo de $\mathbb{A}$.
\end{enumerate}

%%%%%%%%%%%%%%%
\subsection{Operadores biyectivos e inversos}
\label{OperadoresBiyectivos}
\index{Operador!Biyectivo}
\index{Biyectivo!Operador}
\index{Operador!Inverso}
\index{Index!Operador}
Se dice que $\mathbb{A}:\textbf{\em V}_{1}{\rightarrow}\textbf{\em V}_{2}$ es biyectiva (uno a uno o biunívoco) si dados $\left|  {v}_{1}\right> ,\left|  {v}_{2}\right> \in\textbf{\em V}_{1},\,\, \wedge \,\,
\left| {v}^{\prime}\right> \in\textbf{\em V}_{2},$ se tiene que:
\[
\mathbb{A}\left|  {v}_{1}\right> =\left|  {v}^{\prime}\right> \,\, \wedge \,\, \mathbb{A}\left|  {v}_{2}\right>=
\left|  {v}^{\prime}\right> \,\, \Rightarrow \,\, \left| {v}_{1}\right> =\left|  {v}_{2}\right>\,,
\]
es decir, será biyectiva si $\mathbb{A}$ transforma vectores distintos de $\textbf{\em V}_{1}$ en vectores distintos de  $\textbf{\em V}_{2}$. 

Más aún, se puede afirmar que una transformación lineal $\mathbb{A}$, será biyectiva si y sólo si $\aleph\left(  \mathbb{A}\right)  =\left\{\left|{0}\right>\right\}$. Vale decir, si el subespacio nulo está constituido, únicamente por el elemento neutro del espacio vectorial. 

La demostración es sencilla. Supongamos que $\mathbb{A}$ es biyectiva y que
$\mathbb{A}\left|  {v}\right> =\left|  {0}\right> $, entonces $\left|  {v}\right> =\left|  {0}\right>
$, es decir, $\mathbb{A}\left|  {0}\right> =\left|  {0} \right> $, por consiguiente $\aleph\left(  \mathbb{A}\right) = \left\{\left|{0}\right>\right\}$. Recíprocamente, si 
\[
\left.
\begin{array}
[c]{c}
\aleph\left(  \mathbb{A}\right)  =\left\{\left|{0}\right>\right\} \\
\wedge\\
\mathbb{A}\left|  {v}_{1}\right> =\mathbb{A}\left|
{v}_{2}\right>
\end{array}
\right\}  \,\, \Rightarrow \,\,  
\mathbb{A}\left|  {v}_{1}\right>-\mathbb{A}\left|  {v}_{2}\right> =\left|  {0}
\right> =\mathbb{A}\left(  \underset{\left|  {v}_{1}\right>
-\left|  {v}_{2}\right> =0}{\underbrace{\left|  {v}_{1}\right> -\left|  {v}_{2}\right> }}\right) \,\,  \Rightarrow \,\,  \left|  {v}_{1}\right> =\left|{v}_{2}\right>\,.
\]

La importancia de las transformaciones lineales biyectivas reside en la posibilidad de definir inversa, debido a que siempre existe en
$\textbf{\em V}_{2}$ un vector $\left|  {v}^{\prime}\right> $
asociado a través de $\mathbb{A}$ con un vector $\left|  {v}
\right> \in\textbf{\em V}_{1}.$ Diremos que $\mathbb{A}^{-1}\mathbf{:V}
_{2} {\rightarrow} \textbf{\em V}_{1}$ es el inverso de $\mathbb{A}$, si
$\mathbb{A}^{-1}\mathbb{A}=\mathbb{I}=\mathbb{AA}^{-1}.$

Habría que hacer un par de comentarios al respecto. El primero es que, tal y como hemos enfatizado arriba, en general, los operadores no conmutan entre si, y los inversos no son una excepción. Es decir, debería existir (y de hecho existen) inversas por la izquierda $\mathbb{A}^{-1}\mathbb{A}$ e inversas por la derecha $\mathbb{AA}^{-1}$. Por simplicidad e importancia en Física obviaremos esta dicotomía y supondremos que $\mathbb{A}^{-1}\mathbb{A}=\mathbb{I}=\mathbb{AA}^{-1}$. El segundo comentario tiene que ver con la existencia y unicidad del inverso de un operador lineal. Algunos operadores tienen inverso, otros no, pero aquellos que tienen inverso, ese inverso será único. 

Supongamos:
\[
\left.
\begin{array}
[c]{c}
\mathbb{A}_{1}^{-1}\mathbb{A}\left|  {v}\right> =\left|{v}\right> \\
\wedge\\
\mathbb{A}_{2}^{-1}\mathbb{A}\left|  {v}\right> =\left| {v}\right>
\end{array}
\right\}  \,\, \Rightarrow \,\,  \mathbb{A}_{1}^{-1}\mathbb{A}\left|  {v}\right> -\mathbb{A}_{2}^{-1}\mathbb{A}\left|  {v}\right>=
\left|  {0}\right> =\underset{\mathbb{A}_{1}^{-1}=\mathbb{A}_{2}^{-1}}{\underbrace{\left(  \mathbb{A}_{1}^{-1}-\mathbb{A}_{2}^{-1}\right)}}\mathbb{A}\left|  {v}\right> \,\, \Rightarrow \,\,  \mathbb{A}_{1}^{-1}=\mathbb{A}_{2}^{-1}\,.
\]

Ahora bien, un operador lineal $\mathbb{A}$  tendrá inverso si y sólo si para cada vector $\left|  {v}^{\prime}\right> \in \textbf{\em V}_{2}\,\, \exists \,\, $ $\left|  {v}\right> \in \textbf{\em V}_{1}\,\, /  \,\, \mathbb{A}\left|  {v}\right> =\left| {v}^{\prime}\right>$.  Es decir, cada vector $\left| {v}^{\prime}\right> $ está asociado con uno y sólo un vector $\left|  {v}\right> $ a través de la
transformación lineal $\mathbb{A}$.  Dejaremos sin demostración
esta afirmación pero lo importante es recalcar que para que exista
inverso la transformación lineal $\mathbb{A}$, tiene que ser biyectiva y esto implica que se asocia uno y sólo un vector de $\textbf{\em V}_{1}$ con otro de $\textbf{\em V}_{2}$.

Diremos que dos espacios vectoriales $\textbf{\em V}_{1}, \textbf{\em V}_{2}$ son {\it isomorfos} si existe una transformación 
\[
\mathbb{A}: \textbf{\em V}_{1} \rightarrow \textbf{\em V}_{2} \quad \Rightarrow \quad
\left|  {v}^{\prime}\right> =\mathbb{A}\left|  {v}\right> \,,
\]
denominada {\it isomorfismo}\footnote{Se dice que una transformación lineal es un {\it monomorfismo} si $\mathbb{A}$ es inyectiva, un {\it epimorfismo}  si $\mathbb{A}$ es sobreyectiva y un  {\it isomorfismo} si es biyectiva.}, tal que:
\begin{enumerate}
\item $\mathbb{A}$ es biyectiva.
\item $\mathbb{A}$ es lineal: $ \mathbb{A}\left[  \alpha\ \left|  {v}
_{{1}}\right> +\beta\ \left|  {v}_{2}\right>
\right]  =\alpha\ \mathbb{A}\left|  {v}_{{1}}\right>
+\beta\ \mathbb{A}\left|  {v}_{2}\right> \qquad\forall
\mathbf{\quad} \left|  {v}_{{1}
}\right> \text{ y }\left|  {v}_{2}\right> \in\textbf{\em V}_{1}$.
\end{enumerate}

El que $\mathbb{A}$ sea biyectiva nos garantiza que existe la transformación inversa $\mathbb{A}^{-1}$, que también será biyectiva. Es decir, podríamos hablar de manera equivalente de una transformación $\mathbb{F}: \textbf{\em V}_{2} \rightarrow \textbf{\em V}_{1} $. Volveremos a este tema más adelante.

Todavía podemos añadir algunas demostraciones que resultan ser consecuencias de las afirmaciones anteriores. 

Sea la transformación lineal $\mathbb{A} :\textbf{\em V}_{1}\rightarrow \textbf{\em V}_{2}$ y supongamos además que $\mathbb{A}\in\ \mathcal{L}\left(  \textbf{\em V}_{1},\textbf{\em V}_{2}\right)$.
Entonces las siguientes afirmaciones son válidas y equivalentes:

\begin{enumerate}
\item $\mathbb{A}$ es biyectiva en $\textbf{\em V}_{1}$.

\item $\mathbb{A}$ es invertible y su inversa $\mathbb{A}^{-1}:\mathbb{A} \left\{  \textbf{\em V}_{1}\right\}  \rightarrow \textbf{\em V}_{1}$ es lineal.

\item $\forall\, \left|  v\right> \in\textbf{\em V}_{1},\ \mathbb{A}\left\{
\left|  v\right> \right\}  =\left|  0\right> \,\, \Rightarrow \,\,
\left|  v\right> =\left|  0\right> $.  Esto es, el espacio nulo
$\aleph\left(  \mathbb{A}\right)$ únicamente contiene al elemento neutro de $\textbf{\em V}_{1}$.
\end{enumerate}

Si ahora suponemos que $\textbf{\em V}_{1}$ tiene dimensión finita, digamos $\dim\left[  \textbf{\em V}_{1}\right]  =n$, las siguientes afirmaciones serán válidas y equivalentes:

\begin{enumerate}
\item $\mathbb{A}$ es biyectiva en $\textbf{\em V}_{1}$.

\item  Si $\left\{  \left|  {u}_{1}\right>, \left|{u}_{2}\right>, \left|  {u}_{3}\right>, \cdots , \left|  {u}_{n}\right> \right\}  \in\textbf{\em V}_{1}$ son
linealmente independientes, entonces, el conjunto de transformaciones lineales:  $\left\{  \mathbb{A}\left\{  \left|
{u}_{1}\right> \right\}, \mathbb{A}\left\{  \left|
{u}_{2}\right> \right\}, \mathbb{A}\left\{  \left|
{u}_{3}\right> \right\}, \cdots , \mathbb{A}\left\{  \left|
{u}_{n}\right> \right\}  \right\}  \in\mathbb{A}\left\{
\textbf{\em V}_{1}\right\} $ también será linealmente independiente.

\item $\dim\left[  \mathbb{A}\left\{  \textbf{\em V}_{1}\right\}  \right]  =n$.

\item  Si $\left\{  \left|  \mathrm{e}_{1}\right>, \left|\mathrm{e}_{2}\right>, \left|  \mathrm{e}_{3}\right>, 
\cdots , \left|  \mathrm{e}_{n}\right> \right\}  \in\textbf{\em V}_{1}$ es una
base de $\textbf{\em V}_{1}$, entonces $\left\{  \mathbb{A}\left\{  \left|
\mathrm{e}_{1}\right> \right\}, \mathbb{A}\left\{  \left|
\mathrm{e}_{2}\right> \right\}, \mathbb{A}\left\{  \left|
\mathrm{e}_{3}\right> \right\},  \cdots , \mathbb{A}\left\{  \left|
\mathrm{e}_{n}\right> \right\}  \right\}  \in\mathbb{A}\left\{
\textbf{\em V}_{1}\right\}$ es una base de $\mathbb{A}\left\{  \textbf{\em V}
_{1}\right\} $.
\end{enumerate}


\subsection{Operadores adjuntos}
\label{OpAdjunto}
En la sección \ref{OperadoresLineales}, definimos la acción  de un operador lineal $\mathbb{A}: \textbf{\em V} \rightarrow \textbf{\em W}$ de tal forma que  $\mathbb{A}\left|  v \right> = \left|  v^{\prime} \right>$. En esta sección definiremos los operadores  $\mathbb{A}^{\dagger}:  \textbf{\em V}^{\ast} \rightarrow \textbf{\em W}^{\ast}$, de tal forma que $\left< {v}^{\prime}\right|  =\left< {v}\right|  \mathbb{A}^{\dagger}$, donde $\textbf{\em V}^{\ast}$ y $\textbf{\em W}^{\ast}$ son los duales de $\textbf{\em V}$ y  $\textbf{\em W}$, respectivamente. Diremos que el operador $\mathbb{A}^{\dagger}$ es el adjunto de $\mathbb{A}$\footnote{En la literatura también encontrarán que $\mathbb{A}^{\dagger}$ es el hermítico conjugado de $\mathbb{A}$, pero hemos creído conveniente llamarlo únicamente el adjunto de $\mathbb{A}$ para evitar confusiones con operadores hermíticos.}.

Entonces, discutimos en la sección \ref{EspacioVectorialDual}, a cada vector (\textit{ket}) $\left|  {v}\right> $ le está asociado una forma lineal (\textit{bra}) $\left<{v}\right| $, a cada \textit{ket} transformado $\mathbb{A}\left|
{v}\right> =\left|  {v}^{\prime}\right> $ le corresponderá un \textit{bra} transformado $\left< {v}^{\prime}\right|  =\left< {v}\right|  \mathbb{A}^{\dagger}$.  En pocas palabras:
\[
\left|  {v}\right> \,\, \Longleftrightarrow \,\, \left<{v}\right| 
\quad \Rightarrow \quad
\left|  {v}^{\prime}\right> =\mathbb{A}\left|  {v}
\right> \,\,  \Longleftrightarrow \,\, \left< {v}^{\prime}\right|  =\left< {v}\right|  \mathbb{A}^{\dagger}\,.
\]
Si $\mathbb{A}$ es lineal, $\mathbb{A}^{\dagger}$ también lo será, dado que a un vector $\left|  {w}\right>
=\lambda_{1} \left|  {z}_{{1}}\right> +\lambda_{2} \left|  {z}_{2}\right> $ le corresponde un \textit{bra}
$\left< {w}\right|  =\lambda_{1}^{*}\left<{z}_{1}\right|  +\lambda_{2}^{\ast}\left< {z}_{2}\right|$.\footnote{La correspondencia es antilineal, note la conjugación de los números $\lambda_{1}$ y $\lambda_{2}$.} 
Por lo tanto, 
$\left|  {w}^{\prime}\right> =\mathbb{A}\left|  {w}\right> =\lambda_{1}\ \mathbb{A}\left|  {z}_{{1}}\right> +\lambda_{2}\ \mathbb{A}\left|  {z}_{2}\right> ,$ por ser $\mathbb{A}$ lineal, entonces
\[
\left|  {w}^{\prime}\right> \Longleftrightarrow \left<{w}^{\prime}\right|  \equiv\left< {w}\right|\mathbb{A}^{\dagger}=\left(  \lambda_{1}^{*}\left<{z}_{1}\right|  +\lambda_{2}^{\ast}\left< {z}_{2}\right|\right)  \mathbb{A}^{\dagger}\equiv\lambda_{1}^{\ast}\left<{z}_{1}^{\prime}\right|  +\lambda_{2}^{\ast}\left< {z}_{2}^{\prime}\right|  =\lambda_{1}^{\ast}\left< {z}_{1}\right|\mathbb{A}^{\dagger}+\lambda_{2}^{\ast}\left< {z}_{2}\right|  \mathbb{A}^{\dagger}\,.
\]

Es claro que a partir de la definición de producto interno tendremos:
\[
\left< \tilde{x}\right|  \left.  {y}\right> =\left< {y}\right|  \left.  \tilde{x}\right>^{\ast}\quad \forall\quad \left|  \tilde{x}\right>=
\mathbb{A}\left|  {x}\right> ,\left|  {y}\right> \in\textbf{\em V} \,\, \Rightarrow \,\, \left< {x}\right|\mathbb{A}^{\dagger}\left|  {y}\right> =
\left< {y}\right|  \mathbb{A}\left|  {x}\right> ^{\ast}  \,\, \forall\,\, \left|  {x}\right> ,\left| {y}\right> \in\textbf{\em V}\,.
\]
Esta última relación 
\begin{equation}
\label{DefOperadConjugado}
\left< {x}\right|\mathbb{A}^{\dagger}\left|  {y}\right> = \left< {y}\right|  \mathbb{A}\left|  {x}\right> ^{\ast} \quad \,\,
 \forall \,\, \left|  {x}\right> ,\left| {y}\right> \in\textbf{\em V}\,,
\end{equation}
nos permite asociar $\mathbb{A}^{\dagger}$ con $\mathbb{A}$. Esto es, conociendo las propiedades de $\mathbb{A}$ las extendemos a $\mathbb{A}^{\dagger}$ y es a partir de esta relación que podemos deducir las propiedades de los operadores adjuntos:
\[
\left(  \mathbb{A}^{\dagger}\right)^{\dagger}= \mathbb{A}\,, \quad 
\left(  \lambda\mathbb{A}\right)^{\dagger}= \lambda^{\ast} \mathbb{A}^{\dagger}\,, \quad 
\left(\mathbb{A+B}\right)^{\dagger}= \mathbb{A}^{\dagger}+\mathbb{B}^{\dagger}\,, \quad 
\left(\mathbb{AB}\right)^{\dagger}= \mathbb{B}^{\dagger}\mathbb{A}^{\dagger}\,.
\]

Esta última propiedad es fácilmente demostrable y es educativa su demostración. Dado que $\left|  {v}^{\prime}\right> =\mathbb{AB}\left|  {v}\right> $, y además se tiene que:
\[
\left.
\begin{array}
[c]{c}
\left|  {\bar{v}}\right> =\mathbb{B}\left|  {v}\right> \\
\\
\left|  {v}^{\prime}\right> =\mathbb{A}\left|  {\bar{v}}\right>
\end{array}
\right\}  \,\, 
\Rightarrow \,\, \left< {v}^{\prime}\right| =  \left< {\bar{v}}\right|  \mathbb{A}^{\dagger}=\left< {v}\right| \mathbb{B}^{\dagger}\mathbb{A}^{\dagger}= \left< {v}\right|  \left(  \mathbb{AB}\right)^{\dagger}\,.
\]

A partir de las propiedades anteriores se deriva una más útil relacionada con el conmutador,
\[
\left[  \mathbb{A,B}\right]  ^{\dagger}=-\left[  \mathbb{A}^{\dagger} ,\mathbb{B}^{\dagger}\right]  =\left[  \mathbb{B^{\dagger},A^{\dagger}}\right]\,.
\]

Las conclusiones a las que llegamos resultan ser que, para obtener el adjunto de una expresión, se debe proceder de la siguiente manera:
\begin{itemize}
\item  Cambiar constantes por sus complejas conjugadas $\lambda\leftrightarrows \lambda^{\ast}$.

\item  Cambiar los \textit{kets} por sus \textit{bras} asociados y viceversa
(\textit{bras} por \textit{kets}): $\left|  {v}\right>
\leftrightarrows\left< {v}\right|$.

\item  Cambiar operadores lineales por sus adjuntos
$\mathbb{A}^{\dagger}\leftrightarrows\mathbb{A}$.

\item  Invertir el orden de los factores.
\end{itemize}

De este modo
$
\left(  \left|  {v}\right> \left< {w}\right| \right)^{\dagger}=\left|  {w}\right> \left< {v}\right|\,,
$
que se deduce fácilmente de la consecuencia de la definición de producto interno
\[
\left< {x}\right|  \left(  \underline{\left|  {v} \right> \left< {w}\right|  }\right)^{\dagger} \left|  {y}\right> =
\left< {y}\right|  \left( \left|  {v}\right> \left< {w}\right|  \right)\left|  {x}\right>^{\ast}=
\left< {y}\right| \left|  {v}\right> ^{\ast}\left< {w}\right|  \left| {x}\right> ^{\ast}=
\left< {x}\right|  \underline {\left|  {w}\right> \left< {v}\right|  }\left| {y}\right> \,.
\]

\subsection{Operadores hermíticos}
\label{OperadoresHermiticos}
\index{Hermíticos!Operadores}
\index{Operadores Hermíticos}
Existe un conjunto de operadores que se denominan hermíticos o autoadjuntos. Un operador hermítico (o autoadjunto) será aquel para el cual su adjunto es el mismo operador: $\mathbb{A}^{\dagger}=\mathbb{A}$\footnote{Si $\mathbb{A}^{\dagger}=-\mathbb{A}$ el operador lineal se denomina antihermítico.}. Entonces, a partir de (\ref{DefOperadConjugado}) tendremos:
\[
\left< {x}\right|  \mathbb{A}^{\dagger}\left|{y}\right> \equiv
\left< {x}\right|  \mathbb{A}\left|{y}\right> = \left< {y}\right|  \mathbb{A}\left|{x}\right>^{\ast}\,.
\]
Estos operadores juegan el rol de los números reales en el sentido de que son ``iguales a su propio complejo conjugado'' y se utilizan como los observables en la Mecánica Cuántica.  Claramente los proyectores son operadores autoadjuntos por construcción: 
$
\mathbb{P}_{\left|  {v}\right> }^{\dagger} \equiv \left(\left|  {v}\right> \left< {v}\right|  \right)^{\dagger}=
\left|  {v}\right> \left<{v}\right| \,.
$

\subsection{Operadores unitarios}
\label{OperadoresUnitarios}
\index{Operadores Unitarios}
\index{Unitarios!Operadores}
Por definición, un operador será unitario si su inversa es igual a su adjunto:
$
\mathbb{U}^{-1} =\mathbb{U}^{\dagger} \; \Rightarrow \mathbb{U}^{\dagger}\mathbb{U}=\mathbb{U}\mathbb{U}^{\dagger}=\mathbb{I} 
$. 
 
De estos operadores podemos decir varias cosas:
\begin{itemize}
\item  Las transformaciones unitarias dejan invariante al producto interno y consecuentemente la norma de vectores y esto se demuestra fácilmente. Dados dos vectores $\left|  {x}\right> ,\left|  {y}\right>$  sobre los cuales actúa un operador unitario
\[
\left.
\begin{array}
[c]{c}
\left|  {\tilde{x}}\right> =\mathbb{U}\left|  {x} \right> \\
\\
\left|  {\tilde{y}}\right> =\mathbb{U}\left|  {y} \right>
\end{array}
\right\} \,\, \Rightarrow 
\, \left< {\tilde{y}}\right.  \left| {\tilde{x}}\right> = \left< {y}\right| \mathbb{U}^{\dagger}\mathbb{U}\left|  {x}\right>=
\left< {y}\right.  \left|  {x}\right> \,.
\]

Es claro que si $\mathbb{A}$ es hermítico, $\mathbb{A}^{\dagger}=\mathbb{A}$, el operador $\mathbb{U}=\mathrm{e}^{i\mathbb{A}}$ es unitario.
\[
\mathbb{U}=\mathrm{e}^{i\mathbb{A}}\,\, \Rightarrow \,\, \mathbb{U}^{\dagger} =
\mathrm{e}^{-i\mathbb{A^{\dagger}}}=\mathrm{e}^{-i\mathbb{A}}\,\, \Rightarrow  
\mathbb{U}\mathbb{U}^{\dagger} = \mathrm{e}^{i\mathbb{A}}\mathrm{e}^{-i\mathbb{A}}=\mathbb{I}=\mathbb{U}^{\dagger}\mathbb{U}=\mathrm{e}^{-i\mathbb{A}}\mathrm{e}^{i\mathbb{A}}\,.
\]

\item  El producto de dos operadores unitarios también es unitario. Esto es, si $\mathbb{U}$ y $\mathbb{V}$ son unitarios entonces:
\[
\left(  \mathbb{U} \mathbb{V} \right)^{\dagger} \left(  \mathbb{U}\mathbb{V} \right) = 
\mathbb{V}^{\dagger} \underbrace{\mathbb{U}^{\dagger}  \mathbb{U}}_{\mathbb{I}} \mathbb{V}  = 
\mathbb{V}^{\dagger} \mathbb{V} = \mathbb{I} \quad \Leftrightarrow \quad 
\left(  \mathbb{U}\mathbb{V} \right) \left(  \mathbb{U} \mathbb{V} \right)^{\dagger} = 
\mathbb{U}^{\dagger} \underbrace{\mathbb{V}^{\dagger}  \mathbb{V}}_{\mathbb{I}} \mathbb{U}  = 
\mathbb{U}^{\dagger} \mathbb{U} = \mathbb{I}  
\]

\item La invariancia del producto interno implica que los operadores unitarios aplican una base ortogonal en otra: 
$
\left\{  \left| \mathrm{e}_{i}\right> \right\} \xrightarrow{\mathbb{U}} \left\{  \left| \tilde{\mathrm{e}}_{i}\right> \right\}$. Veamos,  si $\left\{  \left| \mathrm{e}_{i} \right> \right\}$ es una base para \textbf{{\em V}} entonces:
\[
\left|  \tilde{\mathrm{e}}_{j} \right> = \mathbb{U}\left|  {\mathrm{ e}}_{j}\right> \quad \Rightarrow 
\left< \tilde{\mathrm{e}}^{i}\right. \left|  \tilde{\mathrm{e}}_{j}\right> =
\left< \tilde{\mathrm{e}}^{i}\right| \mathbb{U} \left|  \mathrm{ e}_{j}\right> =
\left< \mathrm{ e}^{i}\right|  \mathbb{U}^{\dagger}\mathbb{U}\left| \mathrm{ e}_{j} \right> =
\left< \mathrm{e}^{i}\right. \left|  \mathrm{e}_{j} \right> = \delta_{j}^{i}\,.
\]
\end{itemize}

\subsection{Funciones de operadores} 
\label{FuncionesOperadores}
\index{Operador!Funciones de}
\index{Funciones de Operadores}
Para construir funciones de operadores lineales, procedemos por analogía, basándonos en el primero de los ejemplos de la sección \ref{ComposicionOperLineales}. Vale decir que se puede construir un ``polinomio'' en potencias de los operadores a partir de la idea:
\[
P_{n}(x)  =a_{0}+a_{1}x+\cdots+a_{n}x^{n}=a_{i}x^{i}  \leftrightarrows 
P_{n}(\mathbb{A})\left|  {v}\right>    =\left[  a_{0} +a_{1}\mathbb{A} +\cdots +a_{n}\mathbb{A}^{n}\right]  \left|  {v}\right> =\left[  a_{i}\mathbb{A}^{i}\right]  \left|  {v}\right>\,, \,\, \forall \,\, \left|  {v}\right> \in\textbf{\em V}_{1}\,.
\] 

Si nos saltamos todos los detalles de convergencia de la serie anterior --los cuales dependerán de los autovalores de $\mathbb{A}$ y de su radio de convergencia-- y nos inspiramos en el desarrollo de una función $F(z)$ como una serie de potencias de $z$ en un cierto dominio, es posible expresar la función de un operador, $F\left(  \mathbb{A}\right)$, como una serie de potencias del operador $\mathbb{A}$, esto es:
\[
F(z)  =a_{i}z^{i}\quad \leftrightarrows \quad F\left( \mathbb{A}\right)  \left|  {v}\right> =\left[  a_{i} \mathbb{A}^{i}\right]  \left|  {v}\right>\,.
\]
Tal y como se hace en el caso de funciones, ``desarrollamos por Taylor'' la función como una serie de potencias del operador:  
\begin{equation}
\label{FuncOperador}
F(z)= \sum_{n=0}^{\infty}f^{(n)}(0)\dfrac{z^{n}}{n!} \quad \leftrightarrows \quad F\left(  \mathbb{A}\right) \left|  {v}\right> = \left[ \sum_{n=0}^{\infty}f^{(n)}(0)\dfrac{\mathbb{A}^{n}}{n!}\right]  \left|  {v}\right> \, ,
\end{equation}
de esta manera podemos expresar la exponencial de un operador ${\mathbb{A}}$, como
\[
\mathrm{e}^{\mathbb{A}}\left|  {v}\right> =\left[  \sum_{n=0}^{\infty}\dfrac{\mathbb{A}^{n}}{n!}\right]  \left|  {v}
\right> =\left[  \mathbb{I}+\mathbb{A}+\dfrac{\mathbb{A}^2}{2!}
+\cdots+\dfrac{\mathbb{A}^{n}}{n!}\cdots\right]  \left|  {v}
\right>\,.
\]
En este caso hay que hacer una acotación, dado que, en general, 
$\left[ \mathbb{A,B} \right]  \neq 0 \,\, \Rightarrow \,\,{ e}^{\mathbb{A}}{ e}^{\mathbb{B}} \neq { e}^{\mathbb{B}}{ e}^{\mathbb{A}} \neq { e}^{\mathbb{A+B}}$. 
Esta afirmación se corrobora de manera inmediata al desarrollar las exponenciales:
\begin{align*}
{ e}^{\mathbb{A}}{ e}^{\mathbb{B}}\left|  {v}\right>
&  =\left[  \sum_{n=0}^{\infty}\dfrac{\mathbb{A}^{n}}{n!}\right]  \left[\sum_{m=0}^{\infty}\dfrac{\mathbb{B}^{m}}{m!}\right]  \left|  {v} \right> =\left[  \sum_{n=0}^{\infty}\sum_{m=0}^{\infty}\dfrac{\mathbb{A}^{n}}{n!}\dfrac{\mathbb{B}^{m}}{m!}\right]  \left|  {v}\right>\,, \\
& \\
{ e}^{\mathbb{B}}{ e}^{\mathbb{A}}\left|  {v}\right>
&  =\left[  \sum_{n=0}^{\infty}\dfrac{\mathbb{B}^{n}}{n!}\right]  \left[\sum_{m=0}^{\infty}\dfrac{\mathbb{A}^{m}}{m!}\right]  \left|  {v}\right> =\left[  \sum_{n=0}^{\infty}\sum_{m=0}^{\infty} \dfrac{\mathbb{B}^{n}}{n!}\dfrac{\mathbb{A}^{m}}{m!}\right]  \left|  {v}
\right> \,,\\
\text{y sólo en el caso en que}\left[  \mathbb{A,B}\right]  =0&\quad \text{se tiene} \\
{ e}^{\mathbb{A+B}}\left|  {v}\right>  &  =\left[ \sum_{n=0}^{\infty}\dfrac{\left(  \mathbb{A+B}\right)  ^{n}}{n!}\right]
\left|  {v}\right>\,,
\end{align*}
es decir, si $\left[  \mathbb{A,B}\right]  =0
\,\, \Rightarrow \,\,
{ e}^{\mathbb{A}}{ e}^{\mathbb{B}}={ e}^{\mathbb{B}}{ e}^{\mathbb{A}}={ e}^{\mathbb{A+B}}$. La demostración no es inmediata y la haremos al final de la próxima sección en la cual desarrollaremos el concepto de derivada de operadores.

\subsection{Diferenciación de operadores}
\label{DiferenciacionOperadores}
\index{Diferenciación de Operadores}
Distinguiremos dos casos en la diferenciación de operadores: uno cuando el operador depende de una variable y diferenciamos respecto esa variable, $\frac{\mathrm{d}{ e}^{\mathbb{A}(t)} }{ \mathrm{d}t}\left|{v}\right>$  y otro cuando diferenciamos respecto al operador mismo: $\frac{\mathrm{d}{F}\left(  \mathbb{B}\right)}{\mathrm{d}\mathbb{B}}$. 

Empecemos por considerar operadores $\mathbb{A}(t)  $, es decir que pueden depender de una variable arbitraria $t$. Podremos entonces definir la derivada como:
\[
\frac{\mathrm{d}\mathbb{A}(t)  }{\mathrm{d}t} = \lim_{\Delta t \rightarrow 0} \frac{\mathbb{A}\left(  t+\Delta t\right)  -\mathbb{A}(t)  }{\Delta t} \,.
\]
Como era de esperarse, con esta definición se cumplirán todas las propiedades de las derivadas de funciones\footnote{Más adelante, en la sección \ref{DiferenciacionOperadoresMatrices} consideraremos la expresión matricial de los operadores y serán evidentes estas propiedades que aquí presentamos sin mayores justificaciones.}. 

Empecemos por considerar el caso más simple: $\mathbb{A}(t) = \mathbb{A}t$, el operador dependen linealmente de la variable $t$. Si queremos conocer la expresión para $\frac{\mathrm{d}{ e}^{\mathbb{A}t}}{\mathrm{d}t}$, para este caso elemental  recordemos que
\begin{equation}
\label{DesarrolloOpExpPotencia}
{ e}^{\mathbb{A}t}\left|  {v}\right> =\left[  \sum_{n=0}^{\infty}\dfrac{\left( \mathbb{A}t\right)  ^{n}}{n!}\right]  \left|
{v}\right> =
\left[  \mathbb{I}+\mathbb{A}t+\dfrac{\left(\mathbb{A}t\right)  ^{2}}{2!}+\cdots+\dfrac{\left(  \mathbb{A} t\right)  ^{n}}{n!}\cdots\right]  \left|  {v}\right>\,,
\end{equation}
por lo tanto, tendremos:
\[
\frac{\mathrm{d}{ e}^{\mathbb{A}t}}{\mathrm{d}t}\left|  {v}\right>   =\frac{\mathrm{d}}{\mathrm{d}t}\left[  \sum_{n=0}^{\infty}\dfrac{\left(  \mathbb{A}t\right)  ^{n}}{n!}\right]  \left|  {v}\right> =
\left[  \sum_{n=0}^{\infty}\frac{\mathrm{d}}{\mathrm{d}t}\left(  \dfrac{\left(  \mathbb{A}t\right)  ^{n}}{n!}\right)  \right]
\left|  {v}\right> =
\left[  \sum_{n=0}^{\infty}\dfrac{nt^{n-1}\mathbb{A}^{n}}{n!}\right]\left|  {v}\right> =\underset{{ e}^{\mathbb{A}t}
}{\underbrace{\left[  \sum_{n=0}^{\infty}\dfrac{t^{n-1}\mathbb{A}^{n-1}}{\left(  n-1\right)  !}\right]  }}\mathbb{A}\left|  {v}\right>\,.
\]
Nótese que la suma es hasta infinito, por lo tanto, al cambiar de índice $p=n-1$, $p$ sigue variando hasta infinito y la serie es la misma que la anterior. Finalmente, obtendremos\footnote{Es inmediato comprobarlo si consideramos la expansión \ref{DesarrolloOpExpPotencia}}:
\[
\frac{\mathrm{d}{ e}^{\mathbb{A}t}}{\mathrm{d}t}\left|  {v}\right> =
{ e}^{\mathbb{A}t}\mathbb{A}\left|  {v}\right> \equiv \mathbb{A}{ e}^{\mathbb{A}t}\left|  {v}\right> \quad 
\Rightarrow \left[  { e}^{\mathbb{A}t},\mathbb{A}\right]  =0 \, .
\]
En general también es fácil demostrar que $\left[  F(\mathbb{A}),\mathbb{A} \right]  = 0$ ya que a partir del desarrollo (\ref{FuncOperador}) se hace evidente 
\[
\left(  \sum_{n=0}^{\infty}f^{(n)}(0)\dfrac{\mathbb{A}^{n}}{n!} \right)\mathbb{A} \equiv
\mathbb{A} \left(  \sum_{n=0}^{\infty}f^{(n)}(0)\dfrac{\mathbb{A}^{n}}{n!} \right)\, ,
\]
Ahora bien, cuando se presenta la siguiente situación:
\[
\frac{\mathrm{d}\left(  { e}^{\mathbb{A}t}{ e}^{\mathbb{B}t}\right)  }{\mathrm{d}t}\left|  {v}\right> =
\frac{\mathrm{d}{ e}^{\mathbb{A}t}}{\mathrm{d}t}{ e}^{\mathbb{B}t}\left|  {v}\right> +{ e}^{\mathbb{A}t}\frac
{\mathrm{d}{ e}^{\mathbb{B}t}}{\mathrm{d}t}\left|  {v}\right> =
\mathbb{A}{ e}^{\mathbb{A}t}{ e}^{\mathbb{B}t}\left|  {v}\right> +{ e}^{\mathbb{A}t}\mathbb{B}
{ e}^{\mathbb{B}t}\left|  {v}\right>  = 
{ e}^{\mathbb{A}t}\mathbb{A}{ e}^{\mathbb{B}t}\left|{v}\right> +{ e}^{\mathbb{A}t}{ e}^{\mathbb{B}t}\mathbb{B}\left|  {v}\right>\,,
\] 
hay que cuidar el orden en el cual se presentan lo operadores. En particular en la expresión anterior hemos utilizado que siempre se cumple $\left[  { e}^{\mathbb{B}t},\mathbb{B}\right]  =0$, pero no hemos supuesto (ni conocemos) nada de $\left[  \mathbb{A},\mathbb{B}\right] $.  Si, adicionalmente, $\left[  \mathbb{A},\mathbb{B}\right]  =0$ podremos factorizar ${e}^{\mathbb{A}t}{ e}^{\mathbb{B}t}$ y tendremos
\[
\frac{\mathrm{d}\left(  { e}^{\mathbb{A}t}{ e}^{\mathbb{B}t}\right)  }{\mathrm{d}t}\left|  {v}\right> =
\left(\mathbb{A}+\mathbb{B}\right)  { e}^{\mathbb{A}t}{ e}^{\mathbb{B}t}\left|  {v}\right>\,,
\]
pero si $\left[  \mathbb{A},\mathbb{B}\right]  \neq 0$, el orden de aparición de los operadores es MUY importante.

Para construir la expresión de la derivada de una función de operador respecto a su argumento, probaremos la siguiente afirmación:
\begin{equation}
\label{DFA1}
\text{Si} \quad \left[  \mathbb{A,}\left[  \mathbb{A,B}\right]  \right]  =
\left[\mathbb{B,}\left[  \mathbb{A,B}\right]  \right]  =0
\,\, \Rightarrow \,\, 
\left[  \mathbb{A},F\left(  \mathbb{B}\right)  \right]  =
\left[ \mathbb{A,B}\right]  \frac{\mathrm{d}{F}\left(  \mathbb{B}\right)}{\mathrm{d}\mathbb{B}}\,.
\end{equation}

Esta relación es fácilmente demostrable si suponemos  $\left[  \mathbb{A,B}\right]=\mathbb{I}$, el operador identidad. Obviamente aquí se cumple que:
$\left[  \mathbb{A,B}\right]  =\mathbb{I}\,\, \Rightarrow \,\, 
\left[ \mathbb{A,}\left[  \mathbb{A,B}\right]  \right]  =\left[  \mathbb{B,}\left[ \mathbb{A,B}\right]  \right]  =0 \,.$

En ese caso es facil demostrar que $\mathbb{AB}^{n}-\mathbb{B}^{n}\mathbb{A}=n\mathbb{B}^{n-1}$:
\begin{align*}
\mathbb{AB}^{n}-\mathbb{B}^{n}\mathbb{A}  &  
=\mathbb{A} \underset{n}{\underbrace{\mathbb{BB\cdots B}}}\mathbf{-}\underset{n}{\underbrace
{\mathbb{BB\cdots B}}}\mathbb{A} 
  =\left(  \mathbb{I}+\mathbb{BA}\right)  \underset{n-1}{\underbrace
{\mathbb{BB\cdots B}}}-\underset{n}{\underbrace{\mathbb{BB\cdots B}}}\mathbb{A}\\
&  =\mathbb{I \ B}^{n-1}+\mathbb{B\left(  \mathbb{I}+\mathbb{BA}\right)}\underset{n-2}{\underbrace{\mathbb{BB\cdots B}}}-\underset{n}{\underbrace{\mathbb{BB\cdots B}}}\mathbb{A}\\
&  =2\mathbb{B}^{n-1}+\mathbb{B}^{2}\mathbb{\left(  I+\mathbb{BA}\right)}\underset{n-3}{\underbrace{\mathbb{BB\cdots B}}}-\underset{n}{\underbrace{\mathbb{BB\cdots B}}}\mathbb{A}  = \cdots = n \mathbb{B}^{n-1}  \,.
\end{align*}
Para demostrar la relación (\ref{DFA1}) ``desarrollamos en serie de Taylor'' la función ${F}\left(\mathbb{B}\right)$ en el conmutador
\begin{align*}
\left[  \mathbb{A},F \left(  \mathbb{B}\right)  \right]   &  =\left[ \mathbb{A}, \sum_{n=0}^{\infty}f_{n}\dfrac{\mathbb{B}^{n}}{n!}\right] =\sum_{n=0}^{\infty}f_{n}\dfrac{\left[  \mathbb{A,B}^{n}\right]  }{n!}=\left[\mathbb{A,B}\right]  \sum_{n=0}^{\infty}f_{n}\dfrac{n\mathbb{B}^{n-1}}{n!}=\left[  \mathbb{A,B}\right]  \sum_{n=0}^{\infty}f_{n}\dfrac
{\mathbb{B}^{n-1}}{\left(  n-1\right)  !}\\
&  =\left[  \mathbb{A,B}\right]  \frac{\mathrm{d}{F}\left(\mathbb{B}\right)  }{\mathrm{d}\mathbb{B}}\,.
\end{align*}

Para el caso más general: si 
$
\left[  \mathbb{A,C}\right]  =\left[  \mathbb{B,C}\right]=0$ con 
$\mathbb{C}=\left[  \mathbb{A,B}\right]  
\,\, \Rightarrow \,\, \left[  \mathbb{A},F\left(  \mathbb{B}\right)  \right]  \overset{?}{=}\left[
\mathbb{A,B}\right]  \frac{\mathrm{d}{F}\left(  \mathbb{B}\right)}{\mathrm{d} \mathbb{B}} \,, $ se procede del mismo modo. Probamos primero que:
\[
\text{si }\left[  \mathbb{A,C}\right]  =\left[  \mathbb{B,C}\right]=0\,, \quad \text{con } \mathbb{C}= \left[  \mathbb{A,B}\right]  \,\, \Rightarrow \,\,
\left[  \mathbb{A,B}^{n}\right]  = \mathbb{AB}^{n}-\mathbb{B}^{n} \mathbb{A}=n\left[  \mathbb{A,B}\right]  \mathbb{B}^{n-1}\,.
\]
Tendremos:
\begin{align*}
\mathbb{AB}^{n}-\mathbb{B}^{n}\mathbb{A}  &  =
\mathbb{A}\underset{n}{\underbrace{\mathbb{BB\cdots B}}}\mathbf{-}\underset{n}{\underbrace
{\mathbb{BB\cdots B}}}\mathbb{A}  =
\left(  \mathbb{C}+\mathbb{BA}\right)  \underset{n-1}{\underbrace{\mathbb{BB\cdots B}}}-\underset{n}{\underbrace{\mathbb{BB\cdots B}}}\mathbb{A}\\ 
                                                                       &  =
\mathbb{CB}^{n-1}+\mathbb{B\left(  \mathbb{C}+\mathbb{BA}\right)}\underset{n-2}{\underbrace{\mathbb{BB\cdots B}}}-\underset{n}{\underbrace{\mathbb{BB\cdots B}}}\mathbb{A}\\
                                                                       &  =
2\mathbb{CB}^{n-1}+\mathbb{B}^{2}\mathbb{\left(  \mathbb{C}+\mathbb{BA}\right)  }\underset{n-3}{\underbrace{\mathbb{BB\cdots B}}}-\underset{n}{\underbrace{\mathbb{BB\cdots B}}}\mathbb{A} = \cdots = 
n\mathbb{CB}^{n-1}=n\mathbb{\left[  \mathbb{A,B}\right]  B}^{n-1}\,,
\end{align*}
con lo cual es inmediato demostrar que:
\begin{align*}
\left[  \mathbb{A},F\left(  \mathbb{B}\right)  \right]   &  =
\left[ \mathbb{A,}\sum_{n=0}^{\infty}f_{n}\dfrac{\mathbb{B}^{n}}{n!}\right] =
\sum_{n=0}^{\infty}f_{n}\dfrac{\left[  \mathbb{A,B}^{n}\right]  }{n!} =
\left[ \mathbb{A,B}\right]  \sum_{n=0}^{\infty}f_{n}\dfrac{n\mathbb{B}^{n-1}}{n!}=
\left[  \mathbb{A,B}\right]  \sum_{n=0}^{\infty}f_{n}\dfrac{\mathbb{B}^{n-1}}{\left(  n-1\right)  !}\\
&  =\left[  \mathbb{A,B}\right]  \frac{\mathrm{d}{F}\left(\mathbb{B}\right)  }{\mathrm{d}\mathbb{B}}\,.
\end{align*}

\subsubsection{La fórmula de Glauber}
\label{Glauber}
\index{Glauber!Fórmula de}
\index{Fórmula!de Glauber}
Ahora estamos en capacidad de demostrar limpiamente la fórmula de Glauber:
\[
\mathrm{e}^{\mathbb{A}}\mathrm{e}^{\mathbb{B}}=\mathrm{e}^{\mathbb{A+B}}\mathrm{e}^{\frac{1}{2}\left[\mathbb{A,B}\right] }\,.
\]

Para demostrarla, procedemos a considerar un operador $\mathbb{F}(t) =\mathrm{e}^{\mathbb{A}t}\mathrm{e}^{\mathbb{B}t}$,  por lo tanto:
\begin{align*}
\frac{\mathrm{d}\mathbb{F}(t)}{\mathrm{d}t}\left|{v}\right>&  =\frac{\mathrm{d}\mathrm{e}^{\mathbb{A}t}\mathrm{e}^{\mathbb{B}t}}{\mathrm{d}t}\left|  {v}\right>=\mathbb{A}\mathrm{e}^{\mathbb{A}t}\mathrm{e}^{\mathbb{B}t}\left|{v}\right> +\mathrm{e}^{\mathbb{A}t}\ \mathbb{B}\mathrm{e}^{\mathbb{B}t}\left|  {v}\right> =\left(\mathbb{A}
+\mathrm{e}^{\mathbb{A}t}\ \mathbb{B}\mathrm{e}^{\mathbb{-A}t}\right)\mathrm{e}^{\mathbb{A}t}\mathrm{e}^{\mathbb{B}t}\left|{v}
\right> \\
&  =\left(  \mathbb{A}+\mathrm{e}^{\mathbb{A}t}\mathbb{B}\mathrm{e}
^{\mathbb{-A}t}\right)  \mathbb{F}(t)  \left|  {v}\right>\,.
\end{align*}
Ahora bien, dado que:
$\left[  \mathbb{A,}\left[  \mathbb{A,B}\right]  \right]  =
\left[\mathbb{B,}\left[  \mathbb{A,B}\right]  \right]  =0
\,\, \Rightarrow \,\,\left[  \mathbb{A},F\left(\mathbb{B}\right)  \right]  =
\left[\mathbb{A,B}\right]  \frac{\mathrm{d}{F}\left(\mathbb{B}\right)}{\mathrm{d}\mathbb{B}}\,,
$ \\ \\
entonces:
$
\left[  \mathrm{e}^{\mathbb{A}t},\mathbb{B}\right]  =t\left[  \mathbb{A,B}\right]  \mathrm{e}^{\mathbb{A}t}
\,\, \Rightarrow \,\,
\mathrm{e}^{\mathbb{A}t}\mathbb{B=B}\mathrm{e}^{\mathbb{A}t}+t\left[  \mathbb{A,B}\right]\mathrm{e}^{\mathbb{A}t}\,,
$ \\ \\
es decir:
$
\frac{\mathrm{d}\mathbb{F}(t)  }{\mathrm{d}t}\left|{v}\right> =
\left(  \mathbb{A}+\mathrm{e}^{\mathbb{A}t} \mathbb{B}\mathrm{e}^{\mathbb{-A}t}\right)  \mathbb{F}(t)
\left|  {v}\right> =
\left(  \mathbb{A}+\mathbb{B}+t\left[ \mathbb{A,B}\right]  \right)  \mathbb{F}\left(t\right)  \left|  {v}\right>\,,
$ \\ \\
por tanteo uno puede darse cuenta que:
$
\mathbb{F}(t)  =\mathrm{e}^{\left\{  \left( \mathbb{A+B}\right)
t+\frac{t^{2}}{2}\left[  \mathbb{A,B}\right]  \right\}  }\,,
$ \\ \\
cumple con la ecuación anterior, por lo tanto absorbiendo $t$ en los operadores correspondientes llegamos a la fórmula de Glauber:
$
\mathrm{e}^{\mathbb{A}}\mathrm{e}^{\mathbb{B}}=\mathrm{e}^{\mathbb{A+B}}\mathrm{e}^{\frac{1}{2}\left[  \mathbb{A,B}\right]  } \,.
$


\subsection{{\color{Fuchsia}Ejemplos}}
\begin{enumerate}
\item Sean $\mathbb{A}$ y $\mathbb{B}$ dos operadores hermíticos y un operador unitario definido como: $\mathbb{U} = \mathbb{A} + i\mathbb{B}$. \\ Mostraremos que $[\mathbb{A},\mathbb{B}] = 0$ y $\mathbb{A}^{2} + \mathbb{B}^{2} =\mathbb{I}$.

Comenzamos con $[\mathbb{A},\mathbb{B}] = 0$: 
\[
 \mathbb{U} \mathbb{U}^{\dag} =  
 \mathbb{U}^{\dag} \mathbb{U}= \left(\mathbb{A} + i\mathbb{B} \right)\left(\mathbb{A} + i\mathbb{B} \right)^{\dag} = 
 \left(\mathbb{A} + i\mathbb{B} \right)^{\dag}  \left(\mathbb{A} + i\mathbb{B} \right) \Rightarrow  \left(\mathbb{A} + i\mathbb{B} \right)\left(\mathbb{A} - i\mathbb{B} \right) = 
 \left(\mathbb{A} - i\mathbb{B} \right)  \left(\mathbb{A} + i\mathbb{B} \right)  
 \]
 \[
\mathbb{B}\mathbb{A} - \mathbb{A}\mathbb{B} = -\mathbb{B}\mathbb{A} + \mathbb{A}\mathbb{B}  \,\, \Rightarrow \,\, 
[\mathbb{B}, \mathbb{A} ] = -[\mathbb{B}, \mathbb{A} ]  
\,\, \Rightarrow \,\, [\mathbb{B}, \mathbb{A} ] = 0\,.
 \] 

Continuamos con la segunda de las afirmaciones, que se puede demostrar, a partir de:
 \[
 \mathbb{U} \mathbb{U}^{\dag}= \mathbb{I} \,\, \Rightarrow \,\, \left(\mathbb{A} + i\mathbb{B} \right)\left(\mathbb{A} + i\mathbb{B} \right)^{\dag} =    \left(\mathbb{A} + i\mathbb{B} \right) \left(\mathbb{A} - i\mathbb{B} \right) =
\left(\mathbb{A}^{2} + \mathbb{B}^{2} + i\left(\mathbb{B}\mathbb{A} -\mathbb{A}\mathbb{B}\right)  \right) \,\, \Rightarrow \,\,  \mathbb{I} = \mathbb{A}^{2} + \mathbb{B}^{2} \,.
 \]
 
 
\item  Un operador cantidad de movimiento generalizado se define como aquel conjunto de operadores hermíticos que cumplen con: 
\[
\left[  \mathbb{J}_{x},\mathbb{J}_{y}\right]  =i\hbar\mathbb{J}_{z} \quad 
\left[  \mathbb{J}_{y},\mathbb{J}_{z}\right]  =i\hbar\mathbb{J}_{x} \quad 
\left[  \mathbb{J}_{z},\mathbb{J}_{x}\right]  =i\hbar\mathbb{J}_{y}\,, \quad \text{es decir} \quad 
\left[  \mathbb{J}_{i},\mathbb{J}_{j}\right] =  i\hbar\epsilon_{ijk}\mathbb{J}_{k} \,,
\]
con $\epsilon_{ijk}$ el símbolo de Levy-Civita (Aquí los índices repetidos NO indican suma). 

Adicionalmente, definimos los siguientes operadores:
\[
\mathbb{J}^{2}=\mathbb{J}_{x}^{2}+\mathbb{J}_{y}^{2}+\mathbb{J}_{z}^{2}; \quad\mathbb{J}_{+}=\mathbb{J}_{x}+i\mathbb{J}_{y}\quad\mathbb{J}_{-}=\mathbb{J}_{x}-i\mathbb{J}_{y} \,.
\]

Demostremos que:
$
\left[  \mathbb{J}^{2},\mathbb{J}_{+}\right]     =\left[  \mathbb{J}^{2},\mathbb{J}_{-}\right]  =\left[  \mathbb{J}^{2},\mathbb{J}_{z}\right]  =0 \,.
$

Para probar esta propiedad se puede demostrar de forma genérica que $\left[  \mathbb{J}^{2}_{k},\mathbb{J}_{m}\right] =0$, donde los índices $k,m = 1,2,3\equiv x,y,z$, esto es
  \[
  \left[  \mathbb{J}^{2}_{k},\mathbb{J}_{m}\right] = \left[  \mathbb{J}_{k}\mathbb{J}_{k},\mathbb{J}_{m}\right] =  \mathbb{J}_{k}\mathbb{J}_{k} \mathbb{J}_{m} - \mathbb{J}_{m} \mathbb{J}_{k}  \mathbb{J}_{k} =
   \mathbb{J}_{k}\mathbb{J}_{k} \mathbb{J}_{m}  - \left(  i\hbar\epsilon_{mkl}\mathbb{J}_{l} + \mathbb{J}_{k} \mathbb{J}_{m} \right) \mathbb{J}_{k} \,. 
  \]
  
Notemos que 
$
\left[  \mathbb{J}_{i},\mathbb{J}_{j}\right] =  i\hbar\epsilon_{ijk}\mathbb{J}_{k} \,\, \Rightarrow \,\,
 \mathbb{J}_{m}\mathbb{J}_{k} = i\hbar\epsilon_{mkl}\mathbb{J}_{l} + \mathbb{J}_{k} \mathbb{J}_{m}
$,  y que hemos realizado esa sustitución en el segundo de los términos de arriba. Desarrollando y realizando una sustitución equivalente obtenemos:
  \[
 \left[  \mathbb{J}^{2}_{k},\mathbb{J}_{m}\right] = \mathbb{J}_{k}\mathbb{J}_{k} \mathbb{J}_{m}  -  i\hbar\epsilon_{mkl}\mathbb{J}_{l}\mathbb{J}_{k} - \mathbb{J}_{k} \left(  i\hbar\epsilon_{mkn}\mathbb{J}_{n} + \mathbb{J}_{k} \mathbb{J}_{m} \right) \,,     
 \] 
y claramente $ \left[  \mathbb{J}^{2}_{k},\mathbb{J}_{m}\right] = 0$, por cuanto los índices no suman pero si son mudos, y $\epsilon_{mkl} = - \epsilon_{mlk}$. 
\[
\left[  \mathbb{J}^{2}_{k},\mathbb{J}_{m}\right]=
\mathbb{J}_{k}\mathbb{J}_{k} \mathbb{J}_{m}  -  i\hbar\epsilon_{mkl}\mathbb{J}_{l}\mathbb{J}_{k} -  i\hbar\epsilon_{mkn}\mathbb{J}_{k}  \mathbb{J}_{n} -\mathbb{J}_{k}  \mathbb{J}_{k} \mathbb{J}_{m}\,,
\]
al conmutar los cuadrados de las componentes con cualquiera de las componentes, y dado que los conmutadores son lineales, entonces queda demostrado que: 
\[
 \left[  \mathbb{J}^{2},\mathbb{J}_{\pm}\right] =   \left[ \mathbb{J}_{x}^{2}+\mathbb{J}_{y}^{2}+\mathbb{J}_{z}^{2},\mathbb{J}_{x} \pm i\mathbb{J}_{y} \right] = 
  \left[ \mathbb{J}_{y}^{2},\mathbb{J}_{x} \right] + \left[ \mathbb{J}_{z}^{2},\mathbb{J}_{x}  \right] \pm i \left[ \mathbb{J}_{x}^{2}, \mathbb{J}_{y} \right]  \pm i \left[ \mathbb{J}_{z}^{2},\mathbb{J}_{y} \right] =0\,.
\]


\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}}

Consideremos la siguiente transformación lineal: 
\[
\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^4  \,, \,\,
\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(2x+y,x-z,x+y+z,y+z\right)\,.
\]

Podemos preguntarnos si es una transformación biyectiva, para averiguarlo calculemos el  núcleo de la transformación. Hagamos entonces lo siguiente:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve([2*x+y=0,x-z=0,x+y+z=0,y+z=0],[x,y,z]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\left[ \left[ x=0 , y=0 , z=0 \right]  \right] 
\end{math}
\newline

De manera que $\aleph \left(\mathbb{T}\right) =\left\{ (0,0,0)  \in \mathds{R}^3 \right\}$ y la transformación es biyectiva. Consideremos ahora la transformación lineal:
\[
\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^4  \,, \,\,
\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(x+y,x+z,2x+y+z,y-z\right)
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve([x+y=0,x+z=0,2*x+y+z=0,y-z=0],[x,y,z]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\left[ \left[ x=-{\tt \%r_1} , y={\tt \%r_1} , z={\tt \%r_1} \right]  \right]
\end{math}
\newline.

Por lo tanto $\aleph \left(\mathbb{T}\right) =\left\{ (-\lambda,\lambda,\lambda) \,, \lambda \in \mathds{R} \right\}$. En este caso, la transformación no es biyectiva. Notemos que el vector $\left| e\right>=(-1,1,1)$ resulta ser el vector base para subespacio vectorial  núcleo, por lo tanto, la dimensión de este espacio vectorial es: $\dim\left[  \aleph\left(  \mathbb{T}\right)  \right]=1$. Como veremos más adelante una base en el espacio imagen o rango es el conjunto: $\{\mathbb{T}\left[\left(1,0,0\right) \right], \mathbb{T}\left[\left(0,1,0\right) \right], \mathbb{T}\left[\left(0,0,1\right) \right]\}$.

Por lo tanto, podemos hacer los siguientes cálculos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T(x,y,z):=[x+y,x+z,2*x+y+z,y-z];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
T(x,y,z):=[x+y,x+z,2x+y+z,y-z]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M:matrix(T(1,0,0),T(0,1,0),T(0,0,1))$
\end{verbatim}}
\end{minipage}


%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\begin{pmatrix}1 & 1 & 2 & 0 \\ 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & -1 \\ 
 \end{pmatrix}
\end{math}
\newline

La función {\bf triangularize} devolverá una matriz equivalente, pero en la forma triangular superior de la matriz M, obtenida por eliminación gaussiana. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
triangularize(M);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\begin{pmatrix}1 & 1 & 2 & 0 \\ 0 & -1 & -1 & 1 \\ 0 & 0 & 0 & 0  \\ \end{pmatrix}
\end{math}
\newline

Eso significa que una base para el rango $\mathbb{T}\{\mathds{R}^4\}$ será: $\{\left(1,1,2,0\right), \left(0,-1, -1, 1 \right) \}$, es decir, la dimensión se éste subespacio vectorial es $\dim\left[ \mathbb{T}\left\{\mathds{R}^4 \right\} \right]=2$.

Por lo tanto:
\[
\dim\left[  \aleph\left(  \mathbb{T}\right)  \right]  +
\dim\left[ \mathbb{T}\left\{\mathds{R}^4 \right\} \right]  =
\dim\left[  \mathds{R}^3 \right] \,\, \Rightarrow \,\,
1+2=3 \,.
\]


Consideremos ahora el siguiente espacio vectorial $\textbf{\em V}^3 =\{(x,y,z): x+y+z=0\} $ en $\mathds{R}^3$, y la  siguiente transformación lineal de $ \textbf{\em V}^3 \,\, \rightarrow \,\,\textbf{\em V}^3$
\[
F(x,y,z) = (x + y + z, x + y-z, 2z)\,, 
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
F(x,y,z):=[x+y+z,x+y-z,2*z];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
F(x,y,z):=[x+y+z,x+y-z,2z]
\end{math}
\newline

Ahora bien, para obtener el núcleo de la transformación resolvemos 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([x+y+z=0,x+y-z=0,2*z=0],[x,y,z]);
\end{verbatim}}
{\tt solve: dependent equations eliminated: (1)}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\left[ x={\tt \%r_1} , y=-{\tt \%r_1} , z=0 \right] 
\end{math}
\newline

La solución es: $x=C$, $y=-C$ y $z=0$, por lo tanto, una  base para el espacio $\aleph\left[{F}\right]$ es: $\{(1, -1, 0)\}$. 

Si ahora evaluamos la transformación $F$ para la base canónica resulta lo siguiente:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Img:matrix(F(1,0,0),F(0,1,0),F(0,0,1));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\begin{pmatrix}1 & 1 & 0 \\ 1 & 1 & 0 \\ 1 & -1 & 2 \\ 
 \end{pmatrix}
\end{math}
\newline

Reduciremos la matriz anterior pero esta vez con la función {\bf echelon}, que al igual que {\bf triangularize} devuelve la forma escalonada de la matriz $M$, obtenida por eliminación gaussiana, pero normalizando el primer elemento no nulo de cada fila.


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
echelon(Img);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\begin{pmatrix}1 & 1 & 0 \\ 0 & 1 & -1 \\ 0 & 0 & 0 \\ 
 \end{pmatrix}
\end{math}
\newline

Es decir, una base para el espacio imagen o rango es el conjunto: $\{(1,1, 0), (0,1,-1) \}$. 

Podemos también calcular una base para $\textbf{\em V}^3 =\{(x,y,z): x+y+z=0\} $

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve(x+y+z=0,[x,y,z]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\left[ x=-{\tt \%r_3}-{\tt \%r_2} , y={\tt \%r_3} , z={\tt \%r_2} \right] 
\end{math}
\newline
Es una solución del tipo: $x=-{\it C_3}-{\it C_2}$ , $y={\it C_3}$ y  $z={\it C_2} $. Una base vendrá dada por: $\{(-1, 1, 0), (-1, 0, 1) \}$.
Si evaluamos la transformación  dada en esta base, resulta:


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
matrix(F(-1,1,0),F(-1,0,1));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\begin{pmatrix}0 & 0 & 0 \\ 0 & -2 & 2 \\ 
\end{pmatrix}
\end{math}
\newline

El espacio vectorial tiene como base $\{(0, -2, 2) \}$.

\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}


\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}

\item Considere los siguientes operadores: $\mathbb{A}=\mathbb{A}^{\dag}$ hermítico, $\mathbb{K} = -\mathbb{K}^{\dag}$ antihermítico; $\mathbb{U}^{-1}=\mathbb{U}^{\dag}$ unitario, $\mathbb{P}$ y $\mathbb{Q}$ dos operadores genéricos. Pruebe las siguientes afirmaciones:
\begin{enumerate}
  \item En general: 
  \begin{enumerate}
  \item $\left(\mathbb{P}^{\dag}\right)^{-1} = \left(\mathbb{P}^{-1}\right)^{\dag} $.
  \item $\left(\mathbb{P} \mathbb{Q}\right)^{-1} = \mathbb{Q}^{-1}\mathbb{P}^{-1} $
  \item Si $[\mathbb{P}, \mathbb{Q}] = 0$, entonces $\mathbb{P} (\mathbb{Q})^{-1} = (\mathbb{Q})^{-1}\mathbb{P}$ 
\end{enumerate}  
  \item Si $ \mathbb{A} $ es hermítico entonces $\tilde{ \mathbb{A}} = \mathbb{U}^{-1} \mathbb{A} \mathbb{U}$  también será un operador hermítico.
  \item Si $ \mathbb{K} $ es antihermítico entonces $\tilde{ \mathbb{K}} = \mathbb{U}^{-1} \mathbb{K} \mathbb{U}$ es también lo será. En particular eso se cumple para  $\tilde{ \mathbb{K}} = i \mathbb{A}$. Es decir, podemos construir un operador antihermítico a partir de uno hermítico.
  \item Dados dos operadores $\mathbb{A}$ y $\mathbb{B}$, hermíticos, su composición  $\mathbb{A}\mathbb{B}$, será hermítica \textit{si y sólo si} $\mathbb{A}$ y $\mathbb{B}$ conmuntan. 
  \item Si $\mathbb{S}$ es un operador real y antisimétrico\footnote{Esto es $\mathbb{S}^{\dag} \equiv \mathbb{S}^{T} = -\mathbb{S}$ con $\mathbb{S}^{T}$ el traspuesto de $\mathbb{S}$.} y $\mathbb{I}$ el operador unidad, pruebe:
  \begin{enumerate}
  \item Los operadores $ \left(\mathbb{I} -\mathbb{S} \right)$ y $\left(\mathbb{I} +\mathbb{S} \right)$ conmutan.
  \item El operador $\left(\mathbb{I} -\mathbb{S} \right) \left(\mathbb{I} +\mathbb{S} \right)$ es simétrico, mientras que $\left(\mathbb{I} -\mathbb{S} \right) \left(\mathbb{I} +\mathbb{S} \right)^{-1}$ es ortogonal.\footnote{Esto es $\mathbb{A}^{T} = \mathbb{A}^{-1}$ con $\mathbb{A}^{T}$ el traspuesto de $\mathbb{A}$.}
\end{enumerate}

\item Considere una matriz ortogonal de la forma 
 $ \mathbb{R} =\left(
  \begin{array}{cc}
  \cos(\theta)& \mathrm{sen}(\theta) \\
  -\mathrm{sen}(\theta) & \cos(\theta)
  \end{array}\right)\,, 
$  encuentre la expresión para  $\mathbb{S}$ que reproduce $\mathbb{R} = \left(\mathbb{I} -\mathbb{S} \right) \left(\mathbb{I} +\mathbb{S} \right)^{-1}$.
\end{enumerate}
\item  Si un operador lineal $\mathbb{C}$ genérico, entonces pruebe que 
 $\left(  \mathbb{C}+\mathbb{C}^{\dagger}\right)$ y $i \left( \mathbb{C}-\mathbb{C}^{\dagger}\right) $, con $i=\sqrt{-1},$ serán hermíticos.  
Esto, obviamente implica que siempre podremos separar un operador lineal como:
\[
\mathbb{C=}\frac{1}{2}\left(  \mathbb{C}+\mathbb{C}^{\dagger}\right) +\frac{1}{2}\left(  \mathbb{C} -\mathbb{C}^{\dagger}\right) \,.
\] donde $\left(  \mathbb{C}+\mathbb{C}^{\dagger}\right)$ representa su parte hermítica y $\left(  \mathbb{C}-\mathbb{C}^{\dagger}\right)$ su parte antihermítica.

\end{enumerate}

\section{Representación matricial de operadores}
\label{RepresentacionMatricialOpe}
Dados dos vectores $\left|  {v}_{{1}}\right> $ y $\left| {v}_{2}\right> $ definiremos como el elemento de matriz del
operador $\mathbb{A}$ al producto interno de dos vectores
\begin{equation}
\label{OperElementoMatriz}
\left< {v}_{2}\right|  \left(  \mathbb{A}\left|  {v}_{{1}}\right> \right)  \equiv 
A_{\left(  \left|  {v} _{{1}}\right> ,\left|  {v}_{2}\right> \right)  }\,,
\end{equation}
es claro que $A_{\left(  \left|  {v}_{{1}}\right> ,\left| {v}_{2}\right> \right)  }$ será en general un número complejo, pero además el valor de ese número dependerá de los vectores $\left|  {v}_{{1}}\right> $ y $\left| {v}_{2}\right> $ con los cuales se haga la operación (\ref{OperElementoMatriz}). 

El paso más importante para asociar un conjunto de números a un operador lo constituye realizar la operación (\ref{OperElementoMatriz}) con los elementos de una base del espacio vectorial donde opera $\mathbb{A}$. 

Supongamos un operador lineal $\mathbb{A}$ en el espacio vectorial de transformaciones lineales $\mathcal{L}\left( \textbf {\em V,W}\right)  $ donde $\dim\left(\textbf{\em V}\right)  =n$ y $\dim\left( \textbf{\em W}\right) =m$, y sean $\left\{  \left| \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right>, \left|\mathrm{e}_{3}\right>, \cdots , \left|  \mathrm{e}_{n}\right>\right\}  $ y 
$\left\{  \left|  \tilde{\mathrm{e}}_{1}\right>, \left|\tilde{\mathrm{e}}_{2}\right>, \left|  \tilde{\mathrm{e}}_{3}\right>, \cdots , \left|  \tilde{\mathrm{e}}_{m}\right> \right\}$ bases ortonormales\footnote{Como hemos mencionado con anterioridad, las bases no necesariamente deben ser ortogonales (o mejor ortonormales), pero uno siempre puede ortogonalizarlas (y ortonormalizarlas)} para $\textbf{\em V}$ y $\textbf{\em W}$ respectivamente. 

Entonces en la ecuación (\ref{OperElementoMatriz}) cada uno de los vectores $\mathbb{A}\left|\mathrm{e}_{i}\right> \in \textbf{\em W}$ nos conduce a:
\begin{equation}
\label{RepresentacionMatricial}
\left< \tilde{\mathrm{e}}^{\beta}\right| \mathbb{A}\left|  \mathrm{e}_{i}\right> =
A_{i}^{\alpha}\left< \tilde{\mathrm{e}}^{\beta}\right.\left|\tilde{\mathrm{e}}_{\alpha}\right> = A_{i}^{\beta} \quad \text{con }i=1,2,..,n\,\, \text{y }\,\, \alpha \, , \beta =1,2,..,m\,.
\end{equation}

Las cantidades $A_{j}^{\beta}$ son la representación del operador $\mathbb{A}$ respecto a las bases $\left\{\left| \mathrm{e}_{n}\right> \right\}$ y $\left\{ \left|\tilde{\mathrm{e}}_{m}\right> \right\} $ de $\textbf{\em V}$ y $\textbf{\em W}$ respectivamente. Es decir, definiremos una matriz $A_{j}^{\beta}$ como un arreglo de números donde el superíndice, $\beta$, indica fila y el subíndice, $j$, columna:
\[
A_{j}^{\beta}=\left(
\begin{array}
[c]{cccc}
A_{1}^{1} & A_{2}^{1} & \cdots &  A_{n}^{1}\\
A_{1}^{2} & A_{2}^{2} &  & A_{n}^{2}\\
\vdots &  & \ddots & \\
A_{1}^{n} & A_{2}^{n} &  & A_{n}^{n}
\end{array}
\right)\,,\quad 
\left(
\begin{array}
[c]{c}
A_{1}^{1}\\
A_{1}^{2}\\
\vdots\\
A_{1}^{n}
\end{array}
\right)\,, \quad
\left(
\begin{array}
[c]{cccc}
A_{1}^{1} & A_{2}^{1} & \cdots &  A_{n}^{1}
\end{array}
\right)\,.
\]

Es importante señalar que cambiando el orden de los vectores dentro de la base cambia la representación matricial del operador. Esto significa que la organización de los número $A_{j}^{\beta}$ dependerá del orden que le demos a los vectores en las bases $\left\{\left|\mathrm{e}_{i}\right> \right\}$ y $\left\{\left|\tilde{\mathrm{e}}_{i}\right> \right\}$. 

Definitivamente, las matrices son uno de los objetos más útiles de las matemáticas que permiten ``aterrizar'' conceptos y calcular cantidades. La palabra matriz fue introducida en 1850 por James Joseph
Sylvester\footnote{{James Joseph Sylvester} (1814-1897 Londres, Inglaterra) Además de sus aportes con Cayley a la teoría de las matrices, descubrió la solución a la ecuación cúbica y fue el
primero en utilizar el término discriminante para categorizar cada una de las raíces de la ecuación. Para vivir tuvo que ejercer de abogado durante una década. Por fortuna, otro matemático de la época (Arthur Cayley) frecuentaba los mismos juzgados y tribunales y pudieron interactuar. Por ser judío tuvo cantidad de dificultades para conseguir trabajo en la Academia.} y su teoría desarrollada por
Hamilton\footnote{Sir William Rowan Hamilton (1805 - 1865, Dublin, Irlanda) Sus contribuciones en el campo de la óptica, dinámica del cuerpo rígido, teoría de ecuaciones algebraicas y teoría de
operadores lineales.} y Cayley\footnote{Arthur Cayley (1821, Richmond, 1895, Cambridge, Inglaterra) En sus cerca de 900 trabajos cubrió casi la totalidad de las áreas de las matemáticas de  aquel entonces. Sus mayores contribuciones se centran el la teoría de matrices y la geometría no euclidiana. No consiguió empleo como matemático y tuvo que graduarse de abogado para ejercer durante más de 15 años, durante los cuales publicó más de 250 trabajos en matemáticas.}.
Si bien los físicos las consideramos indispensables, no fueron utilizadas de manera intensiva hasta el aparición de la Mecánica Cuántica alrededor de 1925.
 
\subsection{Álgebra elemental de matrices}
\label{AlgebraMatricial}
\index{Algebra de Matrices}
\index{Matrices!Algebra de}
El álgebra de operadores que definimos en la sección \ref{EspacioVectorialOpLineales}, puede ser traducida al lenguaje de matrices.  Por comodidad supondremos un único espacio vectorial,  $ \textbf{\em V} \equiv  \textbf{\em W} $ y, por lo tanto nos basta una base ortogonal $\left\{ \left|  \mathrm{e}_{n}\right> \right\}$. De este modo, es claro que se obtienen nuevamente  las conocidas relaciones para matrices cuadradas
\[
\left< \mathrm{e}^{i}\right|  \mathbb{A}+\mathbb{B}\left|\mathrm{e}_{j}\right> =\left<\mathrm{e}^{i}\right|  \mathbb{A}\left|  \mathrm{e}_{j}\right>+\left< \mathrm{e}^{i}\right|  \mathbb{B}\left|  \mathrm{e}_{j}\right>=A_{j}^{i}+B_{j}^{i} \,.
\]

Con lo cual tenemos la suma de matrices que todos hemos visto en los cursos básicos
\begin{gather*}
\left(
\begin{array}
[c]{cccc}
A_{1}^{1} & A_{2}^{1} & \cdots &  A_{n}^{1}\\
A_{1}^{2} & A_{2}^{2} &  & A_{n}^{2}\\
\vdots &  & \ddots & \\
A_{1}^{n} & A_{2}^{n} &  & A_{n}^{n}
\end{array}
\right)  +\left(
\begin{array}
[c]{cccc}
B_{1}^{1} & B_{2}^{1} & \cdots &  B_{n}^{1}\\
B_{1}^{2} & B_{2}^{2} &  & B_{n}^{2}\\
\vdots &  & \vdots & \\
B_{1}^{n} & B_{2}^{n} &  & A_{n}^{n}
\end{array}
\right) = 
\left(
\begin{array}
[c]{cccc}
A_{1}^{1}+B_{1}^{1} & A_{2}^{1}+B_{2}^{1} & \cdots &  A_{n}^{1}+B_{n}^{1}\\
A_{1}^{2}+B_{1}^{2} & A_{2}^{2}+B_{2}^{2} &  & A_{n}^{2}+B_{n}^{2}\\
\vdots & \vdots & \ddots & \\
A_{1}^{n}+B_{1}^{n} &  &  & A_{n}^{n}+B_{n}^{n}
\end{array}
\right)\, .
\end{gather*}


De igual modo, para la representación de composición de operadores que consideramos en la sección \ref{ComposicionOperLineales} tendremos:
\[
\left< \mathrm{e}^{i}\right|  \mathbb{AB}\left|  \mathrm{e}_{j}\right> =
\left< \mathrm{e}^{i}\right|  \mathbb{A \ I \ B}\left|  \mathrm{e}_{j}\right> =
\left< \mathrm{e}^{i}\right|\mathbb{A} \left(  \left|  \mathrm{e}_{k}\right> \left<\mathrm{e}^{k}\right|  \right)  \mathbb{B}\left|  \mathrm{e}_{j}\right>=
\left< \mathrm{e}^{i}\right|  \mathbb{A}\left|  \mathrm{e}_{k}\right> \left< \mathrm{e}^{k}\right|  \mathbb{B}\left|
\mathrm{e}_{j}\right> =A_{k}^{i}B_{j}^{k}\,,
\]
que se traduce en la tradicional multiplicación de matrices:
\begin{gather*}
\left(
\begin{array}
[c]{cccc}
A_{1}^{1} & A_{2}^{1} & \cdots &  A_{n}^{1}\\
A_{1}^{2} & A_{2}^{2} &  & A_{n}^{2}\\
\vdots &  & \ddots & \\
A_{1}^{n} & A_{2}^{n} &  & A_{n}^{n}
\end{array}
\right)  \times\left(
\begin{array}
[c]{cccc}
B_{1}^{1} & B_{2}^{1} & \cdots &  B_{n}^{1}\\
B_{1}^{2} & B_{2}^{2} &  & B_{n}^{2}\\
\vdots &  & \vdots & \\
B_{1}^{n} & B_{2}^{n} &  & A_{n}^{n}
\end{array}
\right) =
\left(
\begin{array}
[c]{cccc}
A_{k}^{1}B_{1}^{k} & A_{k}^{1}B_{2}^{k} & \cdots &  A_{k}^{1}B_{n}^{k}\\
A_{k}^{2}B_{1}^{k} & A_{k}^{2}B_{2}^{k} &  & A_{k}^{2}B_{n}^{k}\\
\vdots & \vdots & \ddots & \\
A_{k}^{n}B_{1}^{k} &  &  & A_{k}^{n}B_{n}^{k}
\end{array}
\right)\,,
\end{gather*}
como ya sabíamos $\mathbb{AB} \neq \mathbb{BA} \rightarrow A_{k}^{i}B_{j}^{k}\mathbb{\neq}B_{k}^{i}A_{j}^{k}$.

Finalmente, es claro que la multiplicación de un número por una matriz es la multiplicación de todos sus elementos por ese número
\[
\left< \mathrm{e}^{i}\right|  \alpha\mathbb{A}\left|  \mathrm{e}_{j}\right> =\alpha\left< \mathrm{e}^{i}\right|  \mathbb{A}\left|
\mathrm{e}_{j}\right> =\alpha A_{j}^{i} \,.
\]
 
 
\subsection{Bases y la representación matricial de operadores}
Como es claro de la ecuación (\ref{RepresentacionMatricial}) la representación matricial de un operador depende de las bases de $\textbf{\em V}$ y $\textbf{\em W}$, respectivamente.  A continuación discutiremos algunos ejemplos particulares.

\subsubsection{Representación diagonal}
\label{RepresentacionDiagonal}
Dado un operador lineal $\mathbb{A} \in \mathcal{L}\left(\textbf{\em V,W}\right)$, donde $\dim\left( \textbf{\em V}\right)  =\dim\left(\textbf{\em W}\right) =n$,  y sea $\left\{  \left|  {\mathrm{e}}_{i}\right>  \right\} $ una base ortonormal para $\textbf{\em V}$ y $\textbf{\em W}$. Entonces, si adicionalmente se da el caso que  
$
\mathbb{A}\left| {\mathrm{e}}_{i}\right> = \lambda_{i} \left|  {\mathrm{e}}_{i}\right> \,,
$
la representación matricial será diagonal
\[
\left< {\mathrm{e}}^{j}\right|  \mathbb{A}\left|  {\mathrm{e}}_{i}\right> =A_{i}^{j}=\left< {\mathrm{e}}^{j}\right.  \left|
{\mathrm{e}}_{i}\right> =\lambda_{i}\delta_{i}^{j}\,.
\]

Es importante señalar que $\lambda_{i}\delta_{i}^{j}$, en la ecuación anterior,  no se puede interpretar como suma, sino sencillamente que se repite para cada índice $i$. Adicionalmente, que esta afirmación también es válida para $\dim\left(  \textbf{\em V}\right) \neq \dim\left(  \textbf{\em W}\right) $ pero por simplicidad seguimos trabajando con matrices cuadradas. 

\subsubsection{Representación matricial de operadores adjuntos y hermíticos}
\label{MatricesdeOperadoresHermiticos}
\index{Operadores hermíticos!Representación matricial}
\index{Operadores adjuntos!Representación matricial}
\index{Representación matricial de operadores adjuntos y hermíticos}
Consideremos la representación matricial de un operador adjunto, tal y como lo desarrollamos en la sección \ref{OpAdjunto},
\[
\left(  A ^{\dagger} \right)  _{j}^{i}=\left< \mathrm{e}^{i}\right|  \mathbb{A}^{\dagger}\left|  \mathrm{e}_{j}\right>
=\left< \mathrm{e}^{j}\right|  \mathbb{A}\left|  \mathrm{e}
_{i}\right> ^{\ast}=\left(  A_{i}^{j}\right)  ^{\ast}\,,
\]
vale decir: la matriz que representa el operador adjunto $\mathbb{A}^{\dagger}$, es la traspuesta conjugada de la matriz que representa al operador $\mathbb{A}$.

Si el operador es hermítico: $\mathbb{A}^{\dagger}=\mathbb{A} \,\, \Rightarrow \,\, \left(A^{\dagger}\right)_{j}^{i}=A_{j}^{i}\,.$
Por lo tanto, las matrices hermíticas son simétricas respecto a la diagonal y los elementos de la diagonal son números reales. 

Aquí vale la pena reexpresar, con matrices, algunas de las propiedades que arriba expusimos:
\[
\left(  \mathbb{A}^{\dagger}\right)^{\dagger} \rightarrow \left(\left< \mathrm{e}^{i}\right|  \mathbb{A}^{\dagger}\left|\mathrm{e}_{j}\right> \right)  ^{\dagger} =
\left(  \left(A^{\dagger}\right)  _{j}^{i}\right)^{\dagger} = 
\left(  \left(A_{i}^{j}\right)  ^{\ast}\right)  ^{\dagger}=A_{j}^{i}\quad \text{y} 
\]
\[
\left(  \lambda\mathbb{A}\right)^{\dagger}\rightarrow\left<\mathrm{e}^{i}\right|  \lambda\mathbb{A}^{\dagger}\left|\mathrm{e}_{j}\right> =
\left< \mathrm{e}^{j}\right| \lambda\mathbb{A}\left|  \mathrm{e}_{i}\right> ^{\ast}=
\lambda^{\ast}\left< \mathrm{e}^{j}\right|  \mathbb{A}\left|  \mathrm{e}_{i}\right>^{\ast}=
\lambda^{\ast}\left< \mathrm{e}^{i}\right| \mathbb{A}^{\dagger}\left|  \mathrm{e}_{j}\right>=
\lambda^{\ast}\mathbb{A}^{\dagger}\,,
\]
pero más interesante es:
\[
\left(  \mathbb{AB}\right)^{\dagger} \rightarrow \left<\mathrm{e}^{i}\right|  \left(  \mathbb{AB}\right)^{\dagger}\left|\mathrm{e}_{j}\right> =
\left(  A_{k}^{i}B_{j}^{k}\right)^{\dagger} = (A_{k}^{j})^{\ast}(B_{i}^{k})^{\ast}=
(A^{\dagger})_{j}^{k}(B^{\dagger})_{k}^{i}= (B^{\dagger})_{k}^{i}(A^{\dagger})_{j}^{k} = \rightarrow \mathbb{B}^{\dagger}\mathbb{A}^{\dagger}\,.
\]


\subsection{Representación matricial y transformaciones}
\label{RepresentacionCambioBase}
\index{Representación matricial de operadores!Cambios de Base}
\index{Cambios de Base!Representación matricial de operadores}
Hemos dicho que dada una una base particular en el espacio de acción de un operador, éste quedará representado por una matriz adaptada a esa base. Por lo tanto, si cambiamos la base que genera la representación, ese mismo  operador tendrá otra matriz como representación. 

Por comodidad volvamos a suponer un único espacio vectorial,  $ \textbf{\em V} \equiv  \textbf{\em W} $, pero ahora supondremos que este  espacio vectorial $\textbf{\em V}$ tiene dos bases discretas ortonormales $\left\{  \left| {\mathrm{e}}_{i}\right> \right\}  $ y  $\left\{  \left|  \tilde{\mathrm{e}}_{i} \right> \right\}$. Entonces las representaciones matriciales de $\mathbb{A}$: 
$\tilde{A}_{j}^{i} = \left< \tilde{\mathrm{e}}^{i}\right| \mathbb{A}\left|  \tilde{\mathrm{e}}_{j}\right> $ y 
$A_{j}^{i} = \left< \mathrm{e}^{k}\right| \mathbb{A} \left| \mathrm{e}_{m}\right>$, están relacionadas por:
\begin{equation}
\label{TransformaMatricesOperadores}
\left< \tilde{\mathrm{e}}^{i}\right| \mathbb{A}\left|  \tilde{\mathrm{e}}_{j}\right> =
\left< \tilde{\mathrm{e}}^{i}\right|  {\left(  \left| {\mathrm{e}}_{k}\right> \left< {\mathrm{e}}^{k}\right|  \right)
\mathbb{A}\left(  \left|  {\mathrm{e}}_{m}\right> \left< {\mathrm{e}}^{m}\right|  \right)  }\left|  \tilde{\mathrm{e}}_{j}\right> =
\underset{S_{k}^{i}}{\underbrace{\left< \tilde{\mathrm{e}}^{i}\right.  \left| {\mathrm{e}}_{k}\right> }} \left< {\mathrm{e}}^{k}\right| \mathbb{A}\left|  {\mathrm{e}}_{m}\right> 
\underset{\tilde{S}_{j}^{m}}{\underbrace{\left< {\mathrm{e}}^{m}\right.  \left|\tilde{\mathrm{e}}_{j}\right> }} \; \Leftrightarrow \; \tilde{A}_{j}^{i}=S_{k}^{i}\ A_{m}^{k}\ \tilde{S}_{j}^{m} \, ,
\end{equation}
donde $\tilde{A}_{j}^{i}$ es la representación del operador $\mathbb{A}$ en la base $\left\{\left| \tilde{\mathrm{e}}_{j}\right> \right\}  $ y $A_{m}^{k}$  en la base $\left\{  \left| {\mathrm{e}}_{m}\right> \right\} $. 

Más aún, siempre podremos expresar unos vectores base en términos de los otros de tal forma que:  
\begin{equation}
\label{TildeInversa}
\left| \tilde{\mathrm{e}}_{j}\right> = \tilde{S}^{m}_{j}\left| \mathrm{e}_{m}\right>  =
\tilde{S}^{m}_{j} \left( S^{n}_{m} \left| \tilde{\mathrm{e}}_{n}\right> \right)  \quad \Rightarrow 
\left< \tilde{\mathrm{e}}^{n}\right.  \left| \tilde{\mathrm{e}}_{j}\right> = \delta^{n}_{j} = \tilde{S}^{m}_{j} S^{n}_{m} \equiv S^{n}_{m} \tilde{S}^{m}_{j} \quad \Rightarrow  \tilde{S}^{i}_{j} = \left( S^{i}_{j} \right)^{-1} \,,
\end{equation}
con lo cual la relación  (\ref{TransformaMatricesOperadores}), $\tilde{A}^{i}_{j}=S^{i}_{k}\ A^{k}_{m}\ \tilde{S}^{m}_{j} $, puede ser reescrita como 
\begin{equation}
\label{TransformacionSimilaridad}
\tilde{A}^{i}_{j}=S^{i}_{k}\ A^{k}_{m}\ \left(S^{m}_{j}\right)^{-1} \quad \Leftrightarrow \quad
\tilde{\mathbb{A}} = \mathbb{S} \mathbb{A} \mathbb{S}^{-1} \,
\quad \Rightarrow \quad 
\mathbb{A} = \mathbb{S}^{-1} \tilde{\mathbb{A}} \mathbb{S} .
\end{equation}

Diremos que dos representaciones matriciales $A^{i}_{j}$ y $\tilde{A}^{k}_{m}$, de un mismo operador $\mathbb{A}$, son similares si están  relacionadas entre si por (\ref{TransformacionSimilaridad}), donde la matriz de transformación $S^{i}_{k}$ y su inversa se construyen a partir de los productos internos de las bases.  Esa misma afirmación se puede refrasear por el hecho que dos operadores $\tilde{\mathbb{A}}$ y $\mathbb{A}$ están relacionados por una transformación de similaridad (\ref{TransformacionSimilaridad}) constituyen distintas representaciones de un mismo operador.
\index{Similares!Matrices}
\index{Matrices similares}

Adicionalmente, por la definición de producto interno, $\left< {\mathrm{e}}^{k}\right.  \left| \tilde{\mathrm{e}}_{m}\right>= \left< \tilde{\mathrm{e}}^{m}\right. \left|{\mathrm{e}}_{k}\right>^{\ast}$ y tomando en cuenta (\ref{TildeInversa}), tendremos: 
\[
\tilde{S}_{m}^{k}=\left({S}_{k}^{m}\right)^\ast \equiv \left(S^\dagger \right)^{k}_{m} = \left(S^{k}_{m}\right)^{-1}\,,
\] 
es decir, las matrices de productos internos que transforman las representaciones matriciales de los operadores, son unitarias y  la relación  (\ref{TransformaMatricesOperadores}) puede escribirse  también como:
\begin{equation}
\label{TransformaMatricesOperadores2}
\tilde{A}^{i}_{j}=S^{i}_{k}\ A^{k}_{m}\ \tilde{S}^{m}_{j} \quad \Leftrightarrow \quad 
\tilde{A}^{i}_{j}=S^{i}_{k}\ A^{k}_{m}\ \left(S^\dagger \right)^{m}_{j}  \,.
\end{equation}

En este caso, diremos que las representaciones matriciales de $\mathbb{A}$ están relacionadas a una transformación unitaria del tipo (\ref{TransformaMatricesOperadores2}). 
\index{Transformaciones Unitarias}
\index{Unitarias!Transformaciones}

\subsection{Traza de operadores}

La traza, $\mathrm{Tr}\left(  \mathbb{A}\right)$,  de un operador
$\mathbb{A}$ es la suma de los elementos diagonales de su representación
matricial $ \mathbb{A}$. Esto es, dado un operador $\mathbb{A}$ y una base ortogonal
$\left\{  \left|  \mathrm{e}_{i}\right> \right\} $ para $\textbf{\em V}^{n}$, entonces: 
\[
\mathrm{Tr}\left(  \mathbb{A}\right)  =\left< \mathrm{e}^{k}\right|  \mathbb{A}\left|  \mathrm{e}_{k}\right> =A_{k}^{k}\,.
\]

La traza de la representación matricial de un operador es, por ejemplo:  
\[
A_{j}^{i}=\left(
\begin{array}
[c]{ccc}
1 & 2 & 3\\
4 & 5 & 6\\
7 & 8 & 9
\end{array}
\right)  \,\, \Rightarrow \,\, 
\mathrm{Tr}\left( \mathbb{A}\right)
=A_{i}^{i}=15 \,.
\]

\subsubsection{Propiedades de la Traza}

Claramente la traza es lineal: 
$\mathrm{Tr}\left(  \mathbb{A+}\lambda\mathbb{B}\right)=\mathrm{Tr}\left(  \mathbb{A}\right)  +
\lambda\mathrm{Tr}\left(  \mathbb{B}\right)$,  ya que 
\[
\mathrm{Tr}\left(  \mathbb{A}+\lambda \mathbb{B}\right)  =
\left<{\mathrm{e}}^{k}\right|  \mathbb{A}+\lambda \mathbb{B}\left|  {\mathrm{e}}_{k}\right>
=
\left< {\mathrm{e}}^{k}\right|  \mathbb{A}\left|  {\mathrm{e}}_{k}\right> +{\lambda}\left< {\mathrm{e}}^{k}\right|
\mathbb{B}\left|  {\mathrm{e}}_{k}\right> =\mathrm{Tr}\left(\mathbb{A}\right)  +{\lambda}\mathrm{Tr}\left(  \mathbb{B} \right)\,.
\]

La traza de un producto conmuta, esto es, 
$\mathrm{Tr}\left(  \mathbb{AB}\right)  =\mathrm{Tr}\left( \mathbb{BA}\right)$,  y es fácilmente demostrable:
\[
\mathrm{Tr}\left(  \mathbb{AB}\right)  =\left< {\mathrm{e}}^{k}\right|  \mathbb{AB}\left|  {\mathrm{e}}_{k}\right> =
\left<{\mathrm{e}}^{k}\right|  \mathbb{A}\underset{\mathbb{I}}{\underbrace{\left|{\mathrm{e}}_{m}\right> \left< {\mathrm{e}}^{m}\right|  }} \mathbb{B}\left|  {\mathrm{e}}_{k}\right> =
\left< {\mathrm{e}}^{k}\right|  \mathbb{B}\underset{\mathbb{I}}{\underbrace{{\left|{\mathrm{e}}_{m}\right> \left< {\mathrm{e}}^{m}\right|}}} \mathbb{A}\left|  {\mathrm{e}}_{k}\right> =
\mathrm{Tr}\left(\mathbb{BA}\right)\,.
\]

Recuerde que $\left< {\mathrm{e}}^{k}\right|  \mathbb{B}\left|
{\mathrm{e}}_{m}\right>$ y $\left< {\mathrm{e}}^{k}\right|\mathbb{A}\left| {\mathrm{e}}_{k}\right> $ son números que pueden ser reordenados. Donde una vez más hemos utilizado las dos relaciones de cierre 
$\left|\tilde{\mathrm{e}}_{m}\right>  \left< \tilde{\mathrm{e}}^{m} \right|  = \left|{\mathrm{e}}_{k}\right>  \left< {\mathrm{e}}^{k} \right| = \mathbb{I}$.

Del mismo modo es fácil demostrar que la traza de un triple producto de matrices respeta la ciclicidad del orden de la matrices en el producto: 
$
\mathrm{Tr}\left(  \mathbb{ABC}\right)  =\mathrm{Tr}\left(\mathbb{BCA}\right)  =\mathrm{Tr}\left(  \mathbb{CAB}\right)$. 

\subsubsection{Invariancia de la Traza}
La traza de una matriz no depende de la base que seleccionemos, es un invariante que caracteriza al operador independientemente de la base en la cual se represente. Entonces, a partir de (\ref{TransformaMatricesOperadores}) tendremos:
\[
A_{k}^{k}=  \left<{\mathrm{e}}^{k} \right|   \mathbb{A}  \left|{\mathrm{e}}_{k}\right>  =
\left< {\mathrm{e}}^{k} \right.  \underbrace{ \left| \tilde{\mathrm{e}}_{m}\right>  \left<\tilde{\mathrm{e}}^{m} \right|}_{\mathbb{I}}  \mathbb{A}  \left|{\mathrm{e}}_{k}\right>  =
\left< \tilde{\mathrm{e}}^{m}\right| \mathbb{A} 
\underset{\mathbb{I}}{\underbrace{ \left|{\mathrm{e}}_{k}\right>   \left< {\mathrm{e}}^{k} \right|  }}  \left. \tilde{\mathrm{e}}_{m}\right>  =
\left<\tilde{\mathrm{e}}^{m} \right|  \mathbb{A} \left|\tilde{\mathrm{e}}_{m}\right>  =\tilde{A}_{m}^{m}\,.
\]
Nótese que primero hemos introducido $\left| \tilde{\mathrm{e}}_{m}\right>  \left<\tilde{\mathrm{e}}^{m} \right|$, luego hemos reubicado el término $\left< {\mathrm{e}}^{k} \right. \left| \tilde{\mathrm{e}}_{m}\right> $, para el lado derecho y, finalmente, hemos identificado el término $\left| \mathrm{e}_{k}\right>  \left< \mathrm{e}^{k} \right|$ como el operador unidad. Es claro que la traza caracteriza al operador independiente de su representación matricial.


\subsection{Un paréntesis determinante}
\label{Determinante}
\index{Determinante}
\index{Operador!Determinante de un}
El determinante de un operador, $\mathbb{A}$, se define como una aplicación de su representación matricial, 
$\left< \mathrm{e}^{i}\right|\mathbb{A}\left| \mathrm{e}_{j}\right>$ en $\mathds{R}$, es decir, asocia un número real con la representación matricial del operador, y se define como:
\[ 
\det|\mathbb{A}| =\varepsilon^{ijk\cdots}A_{i}^{1}A_{j}^{2}A_{k}^{3}\cdots \equiv \left|
\begin{array}
[c]{cccc}
A_{1}^{1} & A_{2}^{1} & \cdots &  A_{n}^{1}\\
A_{1}^{2} & A_{2}^{2} &  & A_{n}^{2}\\
\vdots &  & \ddots & \\
A_{1}^{n} & A_{2}^{n} &  & A_{n}^{n}
\end{array}
\right| \,,
\]
donde los elementos de la representación matricial se expresan como $A^{i}_{j} =\left< \mathrm{e}^{i}\right|\mathbb{A}\left| \mathrm{e}_{j}\right>$ y hemos generalizado el tensor de Levi-Civita que presentamos en la sección \ref{ConvencionEinstein}, de tal forma que:
\[
\varepsilon^{ijk\cdots}=\varepsilon_{ijk\cdots}=
\left\{
\begin{array}
[c]{l}
\ \ 0,\text{ si cualesquiera dos índices son iguales}\\
\ \ 1,\text{ si los índices }i,j,k\cdots\text{ constituyen una permutación cíclica de }1,2,3\cdots n\\
-1,\text{ si los índices }i,j,k\cdots\text{ constituyen una permutación anticíclica de }2,1,3\cdots n
\end{array}
\right.
\]
\index{Levi-Civita!Tensor generalizado}
\index{Tensor!Levi-Civita} 

Un ejemplo simplificado de esta asociación para el caso de representaciones matriciales $3 \times 3$ es el siguiente:
\[
\left< \mathrm{e}^{i}\right|\mathbb{A}\left| \mathrm{e}_{j}\right> = \left(
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{2} & A_{2}^{2} & A_{3}^{2}\\
A_{1}^{3} & A_{2}^{3} & A_{3}^{3}
\end{array}
\right) \,\, \Rightarrow \,\,  
\det|\mathbb{A}|=\varepsilon^{ijk}A_{i}^{1}
A_{j}^{2}A_{k}^{3}=\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{2} & A_{2}^{2} & A_{3}^{2}\\
A_{1}^{3} & A_{2}^{3} & A_{3}^{3}
\end{array}
\right|\,,
\]
con lo cual
\begin{align*}
\det|\mathbb{A}|  &  =
\varepsilon^{123}A_{1}^{1}A_{2}^{2}A_{3}^{3}
+\varepsilon^{312}A_{3}^{1}A_{1}^{2}A_{2}^{3}+\varepsilon^{231}A_{2}^{1}A_{3}^{2}A_{1}^{3}+\varepsilon^{132}A_{1}^{1}A_{3}^{2}A_{2}^{3}+\varepsilon^{321}A_{3}^{1}A_{2}^{2}A_{1}^{3}+\varepsilon^{213}A_{2}^{1}A_{1}^{2}A_{3}^{3}\\
&  =A_{1}^{1}A_{2}^{2}A_{3}^{3}+A_{3}^{1}A_{1}^{2}A_{2}^{3}+A_{2}^{1}A_{3}^{2}A_{1}^{3}-A_{1}^{1}A_{3}^{2}A_{2}^{3}-A_{3}^{1}A_{2}^{2}A_{1}^{3}-A_{2}^{1}A_{1}^{2}A_{3}^{3} \,.
\end{align*}

\subsubsection{Propiedades de los determinantes}

\begin{enumerate}
\item $\det|\mathbb{A}| = \det|\mathbb{A}^{T}|$, donde $\mathbb{A}^{T}$ es el operador traspuesto de $\mathbb{A}$, esto es claro para las representaciones matriciales $\left<{\mathrm{e}}^{i} \right|   \mathbb{A}  \left|{\mathrm{e}}_{j}\right> = A^{i}_{j} = \left(\left<{\mathrm{e}}^{j} \right|   \mathbb{A}  \left|{\mathrm{e}}_{i}\right> \right)^{T} $.  

Esta propiedad proviene de la definición del índice de Levi-Civita: 
$
\det|\mathbb{A}|=\varepsilon^{ijk\cdots}A_{i}^{1}A_{j}^{2}A_{k}^{3} \cdots=
\varepsilon_{ijk\cdots}A_{1}^{i}A_{2}^{j}A_{3}^{k}\cdots=\det|\mathbb{A}^{T}|\,,
$ que se traduce en que si se  intercambian filas por columnas el determinante no se altera. 
\[
\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{2} & A_{2}^{2} & A_{3}^{2}\\
A_{1}^{3} & A_{2}^{3} & A_{3}^{3}
\end{array}
\right|  =\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{1}^{2} & A_{1}^{3}\\
A_{2}^{1} & A_{2}^{2} & A_{2}^{3}\\
A_{3}^{1} & A_{3}^{2} & A_{3}^{3}
\end{array}
\right|\,.
\]

\item  Si dos filas o dos columnas son idénticas el determinante se anula
\[
\varepsilon^{iik\cdots}A_{i}^{1}A_{i}^{2}A_{k}^{3}\cdots=\varepsilon_{iik\cdots}A_{1}^{i}A_{2}^{i}A_{3}^{k}\cdots=0 \,,
\quad \Leftrightarrow \quad
\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{3} & A_{2}^{3} & A_{3}^{3}
\end{array}
\right|  =0 \,.
\]

\item  Si multiplicamos una fila o una columna por un número, el determinante queda multiplicado por el número
\begin{align*}
\varepsilon^{ijk\cdots}A_{i}^{1}\left(  \lambda A_{j}^{2}\right)  A_{k}^{3}\cdots &  =
\lambda \varepsilon^{ijk\cdots}A_{i}^{1}A_{j}^{2}A_{k}^{3}\cdots= \lambda\det|\mathbb{A}|\\
\varepsilon_{ijk\cdots}A_{1}^{i}A_{2}^{j}\left( \lambda A_{3}^{k}\right) \cdots &  =\lambda\varepsilon_{ijk\cdots}A_{1}^{i}A_{2}^{j}A_{3}^{k}
\cdots=\lambda\det|\mathbb{A}|\,,
\end{align*}
de aquí claramente se desprende que si una fila o una columna es cero ($\lambda=0$) el determinante se anula. Más aún, si dos filas o dos columnas son proporcionales $A_{i}^{1}=\lambda A_{j}^{2}$ el determinante se anula, por cuanto se cumple la propiedad anterior:
\[
\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & \lambda A_{2}^{1} & A_{3}^{1}\\
A_{1}^{2} & \lambda A_{2}^{2} & A_{3}^{2}\\
A_{1}^{3} & \lambda A_{2}^{3} & A_{3}^{3}
\end{array}
\right|  =\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{2} & A_{2}^{2} & A_{3}^{2}\\
\lambda A_{1}^{3} & \lambda A_{2}^{3} & \lambda A_{3}^{3}
\end{array}
\right|  =\lambda\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{2} & A_{2}^{2} & A_{3}^{2}\\
A_{1}^{3} & A_{2}^{3} & A_{3}^{3}
\end{array}
\right| \,.
\]

Resulta obvio que:
\[
\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & 0 & A_{3}^{1}\\
A_{1}^{2} & 0 & A_{3}^{2}\\
A_{1}^{3} & 0 & A_{3}^{3}
\end{array}
\right|  =\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{2} & A_{2}^{2} & A_{3}^{2}\\
0 & 0 & 0
\end{array}
\right|  =0 \,,
\]
al igual que:
\[
\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & \lambda A_{1}^{1} & A_{3}^{1}\\
A_{1}^{2} & \lambda A_{1}^{2} & A_{3}^{2}\\
A_{1}^{3} & \lambda A_{1}^{3} & A_{3}^{3}
\end{array}
\right|  =\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
\lambda A_{1}^{1} & \lambda A_{2}^{1} & \lambda A_{3}^{1}\\
A_{1}^{3} & A_{2}^{3} & A_{3}^{3}
\end{array}
\right|  =\lambda\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{1}^{1} & A_{3}^{1}\\
A_{1}^{2} & A_{1}^{2} & A_{3}^{2}\\
A_{1}^{3} & A_{1}^{3} & A_{3}^{3}
\end{array}
\right|  =\lambda\left|
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{3} & A_{2}^{3} & A_{3}^{3}
\end{array}
\right|  =0 \,.
\]

\item  Si se intercambian dos filas o dos columnas cambia de signo el
determinante.
\[
\det|\mathbb{A}|=\varepsilon^{ijk\cdots}A_{i}^{1}A_{j}^{2}A_{k}^{3} \cdots=
\left|
\begin{array}
[c]{cccc}
A_{1}^{1} & A_{2}^{1} & \cdots &  A_{n}^{1}\\
A_{1}^{2} & A_{2}^{2} &  & A_{n}^{2}\\
\vdots &  & \ddots & \\
A_{1}^{n} & A_{2}^{n} &  & A_{n}^{n}
\end{array}
\right| \,\, \Rightarrow \,\, 
\varepsilon^{ijk\cdots}A_{j}^{1}A_{i}^{2}A_{k}^{3}\cdots=- \det|\left<{\mathrm{e}}^{i} \right|   \mathbb{\bar{A}}  \left|{\mathrm{e}}_{j}\right>|\,,
\]
donde en la representación matricial $\left<{\mathrm{e}}^{i} \right|   \mathbb{\bar{A}}  \left|{\mathrm{e}}_{j}\right>$  se han intercambiando un par de columnas. Claramente las propiedades del índice de Levi-Civita, obliga al cambio de signo 
$\det|\mathbb{\tilde{A}}|=-\det|\mathbb{A}|\,.$

Nótese que las propiedades anteriores nos lleva a reescribir el determinante de un operador de la siguiente manera:
\[
\det|\mathbb{A}|=\varepsilon^{\alpha\beta\gamma\cdots}
\det|\mathbb{A}|=
\varepsilon_{ijk\cdots}A_{\alpha}^{i}A_{\beta}^{j}A_{\gamma}^{k}\cdots \Longleftrightarrow 
\det|\mathbb{A}| =
\varepsilon_{\alpha\beta\gamma\cdots}  \det|\mathbb{A}|=\varepsilon^{ijk\cdots}A_{i}^{\alpha}A_{j}^{\beta}A_{k}^{\gamma}\cdots
\]
claramente, si $\alpha\beta\gamma \cdots \Longleftrightarrow 123 \cdots$  obtenemos nuevamente la definición anterior. Si se intercambian dos filas o dos columnas, el determinante cambia de signo debido al intercambio de dos índices griegos. Si dos filas o dos columnas son iguales el determinante se anula debido a la propiedad de símbolo de Levi-Civita con índices griegos.

\item  El determinante de la composición de operadores es el producto de los determinantes
\[
\det|\mathbb{AB}| = \det|\mathbb{A}|  \det|\mathbb{B}|\,.
\]
Antes de proceder a la demostración de esta importante propiedad jugaremos un poco más con las propiedades de las matrices.  Si $ \mathbb{A}$ es un operador con una representación matricial $m\times n$ y $ \mathbb{B}$ es uno con una representación matricial $n\times p$, entonces tendremos
$\left(  \mathbb{AB}\right)  ^{\alpha}= \mathbb{A}^{\alpha} \mathbb{B}$, 
esto es, la $\alpha$-ésima fila de $\mathbb{AB}$ es igual a la multiplicación de la $\alpha$-ésima fila de $ \mathbb{A}$, por toda la matriz $ \mathbb{B}$.

Veamos: $C_{j}^{i}=\left(  \mathbb{AB}\right)  _{j}^{i}=A_{l}^{i}B_{j}^{l}$,  por lo tanto la $\alpha$-ésima fila:
\[
C_{j}^{\alpha}=A_{l}^{\alpha}B_{j}^{l} \,\, \Rightarrow \,\,  C_{j}^{\alpha}=\left(  A_{1}^{\alpha},A_{2}^{\alpha},A_{3}^{\alpha},\cdots A_{n}^{\alpha},\right)  
\left(
\begin{array}
[c]{cccc}
B_{1}^{1} & B_{2}^{1} & \cdots &  B_{n}^{1}\\
B_{1}^{2} & B_{2}^{2} &  & B_{n}^{2}\\
\vdots &  & \ddots & \\
B_{1}^{n} & B_{2}^{n} &  & B_{n}^{n}
\end{array}
\right)\,.
\]

Tendremos entonces:
\[
\det|\mathbb{A}|  \det| \mathbb{B}|  =
\det| \mathbb{A}|  \left(  \varepsilon_{ijk\cdots}B_{1}^{i}B_{2}^{j}B_{3}^{k}\cdots\right)  =
\left(  \varepsilon_{ijk\cdots}A_{\alpha}^{i}A_{\beta}^{j}A_{\gamma}^{k}\cdots\right)  \left(  \varepsilon_{abc\cdots}B_{1}^{a}B_{2}^{b}B_{3}^{c}\cdots\right)\,,
\]
que siempre puede ser rearreglado como:
\[
\left(  \varepsilon^{ijk\cdots}A_{i}^{\alpha}A_{j}^{\beta}A_{k}^{\gamma} \cdots\right)  \left(  \varepsilon_{ijk\cdots}B_{1}^{i}B_{2}^{j}B_{3}^{k}\cdots\right)  =
A_{i}^{\alpha}B_{1}^{i}A_{j}^{\beta}B_{2}^{j}A_{k}^{\gamma}B_{3}^{k}\cdots=\det|\mathbb{AB}|\,.
\]

Veamos este desarrollo para el caso de matrices $3 \times 3$:
\begin{eqnarray*}
\det| \mathbb{A}|  \det| \mathbb{B}| &=&  
\left(  \varepsilon^{123}A_{1}^{1}A_{2}^{2}A_{3}^{3}+\varepsilon^{312}A_{3}^{1}A_{1}^{2}A_{2}^{3}+\varepsilon^{231}A_{2}^{1}A_{3}^{2}A_{1}^{3}+\varepsilon^{132}A_{1}^{1}A_{3}^{2}A_{2}^{3}\right. \\
&+& \left. \varepsilon^{321}A_{3}^{1}A_{2}^{2}A_{1}^{3}+\varepsilon^{213}A_{2}^{1}A_{1}^{2}A_{3}^{3}\right) . \left(  \varepsilon^{123}B_{1}^{1}B_{2}^{2}B_{3}^{3}+\varepsilon
^{312}B_{3}^{1}B_{1}^{2}B_{2}^{3}+\varepsilon^{231}B_{2}^{1}B_{3}^{2}B_{1}^{3} \right. \\
&+& \left. 
\varepsilon^{132}B_{1}^{1}B_{3}^{2}B_{2}^{3}+
\varepsilon^{321}B_{3}^{1}B_{2}^{2}B_{1}^{3}+\varepsilon^{213}B_{2}^{1}B_{1}^{2}B_{3}^{3}\right)\,,
\end{eqnarray*}
con lo cual:
\begin{align*}
&  =A_{1}^{1}A_{2}^{2}A_{3}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{3}^{1}B_{1}^{2}B_{2}^{3}+B_{2}^{1}B_{3}^{2}B_{1}^{3}-B_{1}^{1}B_{3}^{2}B_{2}^{3}-B_{3}^{1}B_{2}^{2}B_{1}^{3}-B_{2}^{1}B_{1}^{2}B_{3}^{3}\right)  \\
&  +A_{3}^{1}A_{1}^{2}A_{2}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{3}^{1}B_{1}^{2}B_{2}^{3}+B_{2}^{1}B_{3}^{2}B_{1}^{3}+B_{1}^{1}B_{3}^{2}B_{2}^{3}+B_{3}^{1}B_{2}^{2}B_{1}^{3}+B_{2}^{1}B_{1}^{2}B_{3}^{3}\right)  \\
&  +A_{2}^{1}A_{3}^{2}A_{1}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{3}^{1}B_{1}^{2}B_{2}^{3}+B_{2}^{1}B_{3}^{2}B_{1}^{3}+B_{1}^{1}B_{3}^{2}B_{2}^{3}+B_{3}^{1}B_{2}^{2}B_{1}^{3}+B_{2}^{1}B_{1}^{2}B_{3}^{3}\right)  \\
&  -A_{1}^{1}A_{3}^{2}A_{2}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{3}^{1}B_{1}^{2}B_{2}^{3}+B_{2}^{1}B_{3}^{2}B_{1}^{3}+B_{1}^{1}B_{3}^{2}B_{2}^{3}+B_{3}^{1}B_{2}^{2}B_{1}^{3}+B_{2}^{1}B_{1}^{2}B_{3}^{3}\right)  \\
&  -A_{3}^{1}A_{2}^{2}A_{1}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{3}^{1}B_{1}^{2}B_{2}^{3}+B_{2}^{1}B_{3}^{2}B_{1}^{3}+B_{1}^{1}B_{3}^{2}B_{2}^{3}+B_{3}^{1}B_{2}^{2}B_{1}^{3}+B_{2}^{1}B_{1}^{2}B_{3}^{3}\right)  \\
&  -A_{2}^{1}A_{1}^{2}A_{3}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{3}^{1}B_{1}^{2}B_{2}^{3}+B_{2}^{1}B_{3}^{2}B_{1}^{3}+B_{1}^{1}B_{3}^{2}B_{2}^{3}+B_{3}^{1}B_{2}^{2}B_{1}^{3}+B_{2}^{1}B_{1}^{2}B_{3}^{3}\right) \,.
\end{align*}

Como son números se pueden arreglar de la siguiente forma:
\begin{align*}
&  =A_{1}^{1}A_{2}^{2}A_{3}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{1}^{2}B_{2}^{3}B_{3}^{1}+B_{1}^{3}B_{2}^{1}B_{3}^{2}-B_{1}^{1}B_{2}^{3}B_{3}^{2}-B_{1}^{3}B_{2}^{2}B_{3}^{1}-B_{1}^{2}B_{2}^{1}B_{3}^{3}\right)  \\
&  +A_{3}^{1}A_{1}^{2}A_{2}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{1}^{2}B_{2}^{3}B_{3}^{1}+B_{1}^{3}B_{2}^{1}B_{3}^{2}-B_{1}^{1}B_{2}^{3}B_{3}^{2}-B_{1}^{3}B_{2}^{2}B_{3}^{1}-B_{1}^{2}B_{2}^{1}B_{3}^{3}\right)  \\
&  +A_{2}^{1}A_{3}^{2}A_{1}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{1}^{2}B_{2}^{3}B_{3}^{1}+B_{1}^{3}B_{2}^{1}B_{3}^{2}-B_{1}^{1}B_{2}^{3}B_{3}^{2}-B_{1}^{3}B_{2}^{2}B_{3}^{1}-B_{1}^{2}B_{2}^{1}B_{3}^{3}\right)  \\
&  -A_{1}^{1}A_{3}^{2}A_{2}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{1}^{2}B_{2}^{3}B_{3}^{1}+B_{1}^{3}B_{2}^{1}B_{3}^{2}-B_{1}^{1}B_{2}^{3}B_{3}
^{2}-B_{1}^{3}B_{2}^{2}B_{3}^{1}-B_{1}^{2}B_{2}^{1}B_{3}^{3}\right)  \\
&  -A_{3}^{1}A_{2}^{2}A_{1}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{1}^{2}B_{2}^{3}B_{3}^{1}+B_{1}^{3}B_{2}^{1}B_{3}^{2}-B_{1}^{1}B_{2}^{3}B_{3}^{2}-B_{1}^{3}B_{2}^{2}B_{3}^{1}-B_{1}^{2}B_{2}^{1}B_{3}^{3}\right)  \\
&  -A_{2}^{1}A_{1}^{2}A_{3}^{3}\left(  B_{1}^{1}B_{2}^{2}B_{3}^{3}+B_{1}^{2}B_{2}^{3}B_{3}^{1}+B_{1}^{3}B_{2}^{1}B_{3}^{2}-B_{1}^{1}B_{2}^{3}B_{3}^{2}-B_{1}^{3}B_{2}^{2}B_{3}^{1}-B_{1}^{2}B_{2}^{1}B_{3}^{3}\right) \\
& = \varepsilon_{ijk}A_{\alpha}^{i}B_{1}^{\alpha}A_{\lambda}^{j}
B_{2}^{\lambda}A_{\mu}^{k}B_{3}^{\mu}\,.
\end{align*}

Por lo tanto:
\begin{equation}
\label{MultiplicacionDeterminante}
\varepsilon_{ijk}A_{\alpha}^{i}B_{1}^{\alpha}A_{\lambda}^{j}
B_{2}^{\lambda}A_{\mu}^{k}B_{3}^{\mu} = \det|\mathbb{A}|  \det| \mathbb{B}| = \det| \mathbb{AB}|.
\end{equation}

\end{enumerate}

\subsubsection{La invariancia del determinante}
\label{InvariaciaDeterminante}
El resultado de expresado en la ecuación (\ref{MultiplicacionDeterminante}) tiene varias consecuencias importantes:
\begin{enumerate}
  \item El determinante del operador inverso es el inverso del determinante, es decir: 
 \[
 \det|\mathbb{A}^{-1}| = \frac{1}{\det|\mathbb{A}|} = \det|\mathbb{A}|^{-1} \, . 
 \]
 Esta afirmación es fácilmente demostrable
 \[
 \mathbb{I} = \mathbb{A}^{-1} \mathbb{A} \,\, \Rightarrow \,\, \det|\mathbb{I}| =  
 \det| \mathbb{A} \mathbb{A}^{-1}| =  \det| \mathbb{A}|  \det|\mathbb{A}^{-1}| = 1
 \,\, \Rightarrow  \,\, \det|\mathbb{A}^{-1}| = \frac{1}{\det|\mathbb{A}|}\,.
 \]

 \item La segunda consecuencia es todavía más importante: el determinante de un operador no depende de su representación matricial. Es decir, el determinante, al igual que la traza, asocia un número real al operador, independientemente de su representación matricial: 
\[
\det|\mathbb{A}| \equiv \det|\left< \mathrm{e}^{i}\right| \mathbb{A}\left|  \mathrm{e}_{j}\right>| =
\det| \left< \tilde{\mathrm{e}}^{i}\right| \mathbb{A}\left|  \tilde{\mathrm{e}}_{j}\right>| \equiv 
\det| \tilde{\mathbb{A}}| \, .
\]
Es inmediato darse que cuenta que al aplicar la función determinante a la ecuación (\ref{TransformacionSimilaridad}) se tiene:
\[
\det|\tilde{A}_{j}^{i}|  = \det|S^{i}_{k}\ A^{k}_{m}\ \left(S^{m}_{j}\right)^{-1} | \equiv 
\det|S_{k}^{i}| \det|A_{m}^{k}|\ \det|\left(S^{m}_{j}\right)^{-1}| = \det|S_{k}^{i}| \det|A_{m}^{k}| \frac{1}{\det|S_{j}^{m}|} =  \det|A_{m}^{k}| \,. 
\]
Con lo cual queda demostrada la invariancia del determinante de representaciones matriciales de operadores en bases ortonormales.     
\end{enumerate}

\subsubsection{Fórmula de Laplace}
La fórmula de Laplace permite expresar el determinate de una matriz en términos de sus matrices menores o cofactores, matrices que definiremos en la siguiente sección. 
\begin{equation}
\det| \mathbb{A}| = 
\sum_{j=1}^n A^{i}_{j} (A^{c})^{i}_{j} \,,\quad 
\mbox{para cualquier } i\,.
\label{formulaLaplace}
\end{equation}



\subsection{Diferenciación de operadores y representación matricial}
\label{DiferenciacionOperadoresMatrices}
\index{Diferenciación!Operadores}
\index{Operador!Diferenciación de}
Retomemos, ahora en el lenguaje de representaciones matriciales de operadores, lo que desarrollamos en la sección \ref{DiferenciacionOperadores}. Dado un operador $\mathbb{A}(t)$, el cual supondremos dependiente de una variable arbitraria $t$, podremos definir la derivada como:
\[
\frac{\mathrm{d}\mathbb{A}(t)  }{\mathrm{d}t} = \lim_{\Delta t \rightarrow 0} \frac{\mathbb{A}\left(  t+\Delta t\right)  -\mathbb{A}(t)  }{\Delta t}\,,
\]
por lo tanto, si $\left< u^{k}\right|  \mathbb{A}\left|  u_{i} \right> =A_{i}^{k}$ entonces:
\[
\left< u^{k}\right|  \frac{\mathrm{d}\mathbb{A}(t)}{\mathrm{d}t}\left|  u_{i}\right> =
\left(  \frac{\mathrm{d} \mathbb{A}(t)  }{\mathrm{d}t}\right)  _{i}^{k}=\frac{\mathrm{d}}{\mathrm{d}t}\left< u^{k}\right|  \mathbb{A}(t)  \left| u_{i}\right> =\frac{\mathrm{d}A_{i}^{k}}{\mathrm{d}t} =
\left(
\begin{array}
[c]{cccc}
\frac{\mathrm{d}A_{1}^{1}}{\mathrm{d}t} & \frac{\mathrm{d}A_{2}^{1}}{\mathrm{d}t} & \cdots & \frac{\mathrm{d}A_{n}^{1}}{\mathrm{d}t} \\
\frac{\mathrm{d}A_{1}^{2}}{\mathrm{d}t} & \frac{\mathrm{d}A_{2}^{2}}{\mathrm{d}t} &  & \frac{\mathrm{d}A_{n}^{2}}{\mathrm{d}t} \\
\vdots &  & \ddots & \\
\frac{\mathrm{d}A_{1}^{n}}{\mathrm{d}t} & \frac{\mathrm{d}A_{2}^{n}}{\mathrm{d}t} &  & \frac{\mathrm{d}A_{n}^{n}}{\mathrm{d}t}
\end{array}
\right)\,.
\]
La regla es simple, la representación matricial de la derivada de un operador será la derivada de cada uno de sus elementos. Por ejemplo:
\[
\frac{\mathrm{d}}{\mathrm{d}x}
\left(
\begin{array}
[c]{ccc}
x & x^{2} & 2\\
1 & { e}^{-x} & 5x\\
3x^{3} & 3 & \cos(x)
\end{array}
\right)  =
\left(
\begin{array}
[c]{ccc}
1 & 2x & 0\\
0 & -{ e}^{-x} & 5\\
9x^{2} & 0 & -\mathrm{sen}(x)
\end{array}
\right)\,.
\]

De igual forma tendremos las reglas usuales de la diferenciación y por lo tanto se cumplirá 
\[
\frac{\mathrm{d}\left(  \mathbb{A}(t)  +\mathbb{B}(t)  \right)  }{\mathrm{d}t}=
\frac{\mathrm{d}\left(  \mathbb{A}(t)  \right)  }{\mathrm{d}t}+\frac{\mathrm{d}\left(  \mathbb{B}(t)  \right)  }{\mathrm{d}t}\,,
\]
es decir:
\begin{align*}
\left< u^{k}\right|  \frac{\mathrm{d}\left(  \mathbb{A}(t) +\mathbb{B}(t)  \right)  }{\mathrm{d}t}\left|  u_{i}\right>  &  
= \frac{\mathrm{d}}{\mathrm{d}t}\left< u^{k}\right| \left(  \mathbb{A}(t)  +\mathbb{B}(t)  \right) \left|  u_{i}\right> =\frac{\mathrm{d}}{\mathrm{d}t}\left(  \left< u^{k}\right| \mathbb{A}(t)  \left|  u_{i}\right> +\left< u^{k}\right|  \mathbb{B}(t)  \left|  u_{i}\right> \right)\\
&  = \frac{\mathrm{d}}{\mathrm{d}t} \left< u^{k} \right|  \mathbb{A}(t) \left| u_{i} \right> 
 + \frac{\mathrm{d}}{\mathrm{d}t} \left< u^{k}\right|  \mathbb{B}(t)  \left| u_{i}\right> \\
&  =\left< u^{k}\right|  \frac{\mathrm{d}\mathbb{A}(t)}{\mathrm{d}t}\left|  u_{i}\right>  +\left< u^{k}\right| \frac{\mathrm{d}\mathbb{B}(t)  }{\mathrm{d}t}\left| u_{i}\right> = \frac{\mathrm{d}\left(  \mathbb{A}(t) \right)  }{\mathrm{d}t} +\frac{\mathrm{d}\left(  \mathbb{B}(t) \right)  }{\mathrm{d}t} \,.
\end{align*}

Del mismo modo se cumplirá:
\[
\frac{\mathrm{d}\left(  \mathbb{A}(t)  \mathbb{B}(t)  \right)  }{\mathrm{d}t} = 
\frac{\mathrm{d}\mathbb{A}(t)}{\mathrm{d}t}\mathbb{B}(t)  +\mathbb{A}(t) \frac{\mathrm{d}\mathbb{B}(t)  }{\mathrm{d}t}\,,
\]
con la precaución que no se puede modificar el orden de aparición de los operadores. Es fácil ver que:
\begin{align*}
\left< u^{k}\right|  \frac{\mathrm{d}\left(  \mathbb{A}(t)\mathbb{B}(t)  \right)  }{\mathrm{d}t}\left|  u_{i}\right>
&  = \frac{\mathrm{d}}{\mathrm{d}t}\left< u^{k}\right|  \mathbb{A}(t)  \mathbb{B}(t)  \left|  u_{i}\right>
= \frac{\mathrm{d}}{\mathrm{d}t}\left< u^{k}\right|  \mathbb{A}(t)  \mathbb{I \ B}(t)  \left|  u_{i}\right> \\
&  =\left(  \left< u^{k}\right|  \mathbb{A}(t)  \left| {u}_{m}\right> \left< {u}^{m}\right|  \mathbb{B}
(t)  \left|  u_{i}\right> \right) \\
&  =\frac{\mathrm{d}\left< u^{k}\right|  \mathbb{A}(t) \left|  {u}_{m}\right> }{\mathrm{d}t}\left< {u}^{m}\right|  \mathbb{B}(t)  \left|  u_{i}\right> +\left< u^{k}\right|  \mathbb{A}(t)  \left|  {u}_{m}\right> \frac{\mathrm{d}\left< {u}^{m}\right| \mathbb{B}(t)  \left|  u_{i}\right> }{\mathrm{d}t} \\
&  =\left< u^{k}\right|  \frac{\mathrm{d}\mathbb{A}(t)}{\mathrm{d}t}\left|  {u}_{m}\right> \left< {u}^{m}\right|  \mathbb{B}(t)  \left|  u_{i}\right> + \left< u^{k}\right|  \mathbb{A}(t)  \left|  {u}_{m}\right> \left< {u}^{m}\right|  \frac{\mathrm{d}\mathbb{B}(t)  }{\mathrm{d}t} \left|  u_{i}\right>\,.
\end{align*}


\subsection{{\color{Fuchsia}Ejemplos}}
\begin{enumerate}
\item Si tenemos un matriz $\mathbb{B}$, $2\times3$, de la forma
\[
\mathbb{B}=\left(
\begin{array}
[c]{ccc}
3 & 1 & -2\\
1 & 0 & 4
\end{array}
\right) \,,
\]
y suponemos las bases canónicas para $\textbf{\em V}^{3}$ y $\textbf{\em V}^{2}:\left\{  \left|\mathrm{i}_{1}\right>,  \left|  \mathrm{i}_{2}\right>, \left| \mathrm{i}_{3}\right> \right\}  $ y $\left\{ \left| \mathrm{i}_{1}\right>, \left|  \mathrm{i}_{2}\right> \right\}$, respectivamente, entonces la matriz $ \mathbb{B}$ representa la transformación $\mathbb{B}:
\textbf{\em V}^{3}\rightarrow \textbf{\em V}^{2}$ que lleva un vector genérico $\left| x\right> =\left(  x_{1},x_{2},x_{3}\right)  $ a un vector genérico
$\left|  y\right> =\left(  y_{1},y_{2}\right)$ tal que:
\[
\mathbb{B=}\left(
\begin{array}
[c]{ccc}
3 & 1 & -2\\
1 & 0 & 4
\end{array}
\right) \,\, \Rightarrow \,\,   \mathbb{B}\left|  x\right> =\left|
y\right> \,\, \Rightarrow \,\,  \left(
\begin{array}
[c]{ccc}
3 & 1 & -2\\
1 & 0 & 4
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x_{1}\\
x_{2}\\
x_{3}
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
y_{1}\\
y_{2}
\end{array}
\right) \,,
\]
esto es:
\begin{align*}
y_{1}  &  =3x_{1}+x_{2}-2x_{3}\\
y_{2}  &  =x_{1}+0x_{2}+4x_{3}\,.
\end{align*}

Es claro que la representación matricial dependerá de la base en la cual se exprese. 

\item Si suponemos el operador diferencial $\mathbb{D}\left(  \cdot\right) =\frac{\mathrm{d}\left(  \cdot\right)  }{\mathrm{d}x}$ cuyo
dominio está conformado por el espacio vectorial de los polinomios de grado $\leq 3$, entonces tendremos que:
$\mathbb{D}\left(\cdot\right): \mathbf{P}^{3}\rightarrow \mathbf{P}^{2}$. En este caso las bases podrían ser: 
$\left| p_{i}\right>=\left\{1,x,x^{2},x^{3}\right\}$ y 
$\left| \tilde{p}_{j}\right>=\left\{1,x,x^{2}\right\}$ de
$\mathbf{P}^{3}$ y $\mathbf{P}^{2}$ respectivamente. 

Al operar (diferenciar) sobre $\left|p_{j} \right> \in \mathbf{P}^{3}$ y expresar ese resultado en la base  de $\mathbf{P}^{2}$ tenemos:
\[
\mathbb{D}\left| p_{j}\right> \,\, \Rightarrow \,\, \left\{
\begin{array}
[c]{ccccc}
\frac{\mathrm{d}\left(  1\right)  }{\mathrm{d}x} & = & 0 & = & 0\cdot1+0\cdot
x+0\cdot x^{2}\\ \\
\frac{\mathrm{d}(x)  }{\mathrm{d}x} & = & 1 & = & 1\cdot1+0\cdot
x+0\cdot x^{2}\\ \\
\frac{\mathrm{d}\left(  x^{2}\right)  }{\mathrm{d}x} & = & 2x & = &
0\cdot1+2\cdot x+0\cdot x^{2}\\ \\
\frac{\mathrm{d}\left(  x^{3}\right)  }{\mathrm{d}x} & = & 3x^{2} & = &
0\cdot1+0\cdot x+3\cdot x^{2}
\end{array}
\right. \,\, \Rightarrow \,\,
\left< {\tilde p}^{\alpha}\right|\mathbb{D}\left| p_{j}\right> =
\left(
\begin{array}
[c]{cccc}
0 & 1 & 0 & 0\\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 3
\end{array}
\right)\,.
\]

Los coeficientes de esa expansión serán las columnas de la matriz
que los representa. Para enfatizar, los elementos de una matriz, no sólo dependen de la base sino del orden en el cual la base se presente. 

Consideremos que la base
de $\mathbf{P}^{2}$ viene representada por $\left|\hat{p}_{j}\right>=\left\{  x^{2},x,1\right\}$, la representación matricial del operador $\mathbb{D}\left(  \cdot\right)=\frac{\mathrm{d}\left(  \cdot\right)  }{\mathrm{d}x}$ será:
\[
\mathbb{D}\left| p_{j}\right> \,\, \Rightarrow \,\, \left\{
\begin{array}
[c]{ccccc}
\frac{\mathrm{d}\left(1\right)  }{\mathrm{d}x} & = & 0 & = &
0 \cdot x^2+0\cdot x+0\cdot 1\\ \\
\frac{\mathrm{d}(x)  }{\mathrm{d}x} & = & 1 & = &
0\cdot x^2+0\cdot x+1\cdot 1\\ \\
\frac{\mathrm{d}\left(  x^{2}\right)  }{\mathrm{d}x} & = & 2x & = &
0\cdot x^2+2\cdot x+0\cdot 1\\ \\
\frac{\mathrm{d}\left(  x^{3}\right)  }{\mathrm{d}x} & = & 3x^{2} & =&
3\cdot x^2+0\cdot x+0\cdot 1
\end{array}
\right. \,\, \Rightarrow \,\,
\left< {\hat p}^{\alpha}\right|\mathbb{D}\left| p_{j}\right> =
\left(
\begin{array}
[c]{cccc}
0 & 0 & 0 & 3\\
0 & 0 & 2 & 0\\
0 & 1 & 0 & 0
\end{array}
\right)\,.
\]

Se puede ver que $\mathbb{D}\left| p_{j}\right>$ es:
\[
\left(
\begin{array}
[c]{cccc}
0 & 1 & 0 & 0\\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 3
\end{array}
\right)  \left(
\begin{array}
[c]{c}
1\\
1\\
1\\
1
\end{array}
\right)  = 
\left(
\begin{array}
[c]{c}
1\\
2\\
3
\end{array}
\right)\,\, \Rightarrow \,\, 1+2x+3x^2\,,
\]
y equivalentemente:
\[
\left(
\begin{array}
[c]{cccc}
0 & 0 & 0 & 3\\
0 & 0 & 2 & 0\\
0 & 1 & 0 & 0
\end{array}
\right)  \left(
\begin{array}
[c]{c}
1\\
1\\
1\\
1
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
3\\
2\\
1
\end{array}
\right) \,\, \Rightarrow \,\, 3x^{2}+2x+1\,,
\]

¡Es el mismo polinomio! Recuerde que las componentes del vector multiplican a los vectores bases en el mismo orden.

\item Construyamos la representación para el mismo operador 
$\mathbb{D}$ del ejemplo anterior, pero ahora en las siguientes bases:  
$\left| p_{i}\right>=\left\{1,1+x,1+x+x^{2},1+x+x^{2}+x^{3}\right\}$ y
$\left| {\tilde p}_{i}\right>=\left\{  1,x,x^{2}\right\}  $ de $\mathbf{P}^{3}$ y $\mathbf{P}^{2}$, respectivamente. Entonces, nuevamente  vemos que $\mathbb{D}\left|p_{j}\right>$ implica:
\[
\left\{
\begin{array}
[c]{lllll}
\frac{\mathrm{d}\left(  1\right)  }{\mathrm{d}x} & = & 0 & = & 0\cdot1+0\cdot
x+0\cdot x^{2}\\ \\
\frac{\mathrm{d}\left(  1+x\right)  }{\mathrm{d}x} & = & 1 & = &
1\cdot1+0\cdot x+0\cdot x^{2}\\ \\
\frac{\mathrm{d}\left(  1+x+x^{2}\right)  }{\mathrm{d}x} & = & 1+2x & = &
1\cdot1+2\cdot x+0\cdot x^{2}\\ \\
\frac{\mathrm{d}\left(  1+x+x^{2}+x^{3}\right)  }{\mathrm{d}x} & = &
1+2x+3x^{2} & = & 1\cdot1+2\cdot x+3\cdot x^{2}
\end{array}
\right. \,\, \Rightarrow \,\,
\left< {{\tilde p}}^{\alpha}\right|\mathbb{D}\left| p_{j}\right> =
\left(
\begin{array}
[c]{cccc}
0 & 1 & 1 & 1\\
0 & 0 & 2 & 2\\
0 & 0 & 0 & 3
\end{array}
\right)\,.
\]

\item Dada la siguiente transformación:
\[
\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^4  \,, \,\,
\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(x+y,x-2z,x+2y+3z,y-2z\right)\,.
\]
y tomemos como bases los conjuntos:
\[
\left|\mathrm{e}_{i}\right>=\{(1,1,0), (1,0,1),(0,1,1)\} \in \mathds{R}^3\,, \quad 
\left|\mathrm{\tilde e}_{i}\right>=\{(1,1,1,1), (0,1,1,1),(0,0,1,1), (0,0,0,1)\} \in \mathds{R}^4\,.
\] 
Queremos encontrar la representación matricial del operador $\mathbb{T}$, esto es 
\[
T^{i}_{j} = \left< \mathrm{\tilde e}^{i}\right| \mathbb{T} \left|\mathrm{e}_{j}\right> =
\left< \mathrm{\tilde e}^{i}\right| \left(C^{k}_{j} \left|\mathrm{\tilde e}_{k}\right> \right) \equiv
C^{k}_{j} \left< \mathrm{\tilde e}^{i}\right|  \left.\mathrm{\tilde e}_{k}\right> \, ,
\]por lo tanto los $C^{k}_{j}$ son los coeficientes de la expansión de los vectores base de $\mathds{R}^3$ transformados. Es decir
\[
\mathbb{T} \left|\mathrm{e}_{1}\right> = C^{k}_{1}\left|\mathrm{\tilde e}_{k}\right>, \, 
\mathbb{T} \left|\mathrm{e}_{2}\right> = C^{k}_{2}\left|\mathrm{\tilde e}_{k}\right>, \, 
\mathbb{T} \left|\mathrm{e}_{3}\right> = C^{k}_{3}\left|\mathrm{\tilde e}_{k}\right>, \quad \mathrm{y} \quad
\mathbb{T} \left|\mathrm{e}_{4}\right> = C^{k}_{4}\left|\mathrm{\tilde e}_{k}\right>. 
\] 


Entonces, podemos hacer las siguientes expansiones:
\[
\mathbb{T}\left[ \left(1,1,0\right) \right]=(2,1,3,1)=
C^{1}_{1}(1,1,1,1) +C^{2}_{1}(0,1,1,1) +C^{3}_{1}(0,0,1,1) +C^{4}_{1}(0,0,0,1) \,,
\]
y al resolver para $\{C^{1}_{1},C^{2}_{1},C^{3}_{1},C^{4}_{1} \}$ obtener: $\{C^{1}_{1}=2,C^{2}_{1}=-1,C^{3}_{1}=2,C^{4}_{1}=-2 \}$.

Para la siguiente base de  $\mathds{R}^3$
\[
\mathbb{T}\left[ \left(1,0,1\right) \right]=(1,-1,4,-2)=
C^{1}_{2}(1,1,1,1) +C^{2}_{2}(0,1,1,1) +C^{3}_{2}(0,0,1,1) + C^{4}_{2}(0,0,0,1) \,,
\]
resolviendo para $\{C^{1}_{2},C^{2}_{2},C^{3}_{2},C^{4}_{2} \}$ se obtiene: $\{C^{1}_{2}=1,C^{2}_{2}=-2,C^{3}_{2}=5,C^{4}_{2}=-6 \}$.

Y finalmente, para:
\[
\mathbb{T}\left[ \left(0,1,1\right) \right]=(1,-2,5,-1)=
C^{1}_{3}(1,1,1,1) +C^{2}_{3}(0,1,1,1) +C^{3}_{3}(0,0,1,1) +C^{4}_{3}(0,0,0,1)\,,
\]
resulta: $\{C^{1}_{3}=1,C^{2}_{3}=-3,C^{3}_{3}=7,C^{4}_{3}=-6 \}$.

Por lo tanto, la representación matricial que hemos obtenido será:
\begin{eqnarray*}
T^{1}_{1} &=& C^{k}_{1} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{k}\right> =
C^{1}_{1} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{1}\right> +
C^{2}_{1} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{2}\right> +
C^{3}_{1} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{3}\right> +
C^{4}_{1} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{4}\right> \,, \\
T^{1}_{2} &=&C^{k}_{2} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{k}\right> =
C^{1}_{2} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{1}\right> +
C^{2}_{2} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{2}\right> +
C^{3}_{2} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{3}\right> +
C^{4}_{2} \left< \mathrm{\tilde e}^{1}\right|  \left.\mathrm{\tilde e}_{4}\right> \,, \cdots 
\end{eqnarray*}

Vale decir:
\begin{eqnarray*}
T^{1}_{1} &=& 2 \cdot 4 +(-1) \cdot 3 + 2 \cdot 2 +(-1) \cdot 1 = 7\,, \\
T^{1}_{2} &=& 1\cdot 4 +(-2) \cdot 3 + 5 \cdot 2 +(-6) \cdot 1 = 2 \,,  \quad \cdots
\end{eqnarray*}

Si continuamos haciendo todas las cuentas, para todas las demás componentes, tendremos la siguiente matriz:
\[
\left< \mathrm{\tilde e}_{j}\right|\mathbb{T}\left|\mathrm{e}_{i}\right>=
\left(\begin{array}{rrr}
7 & 2 & 3 \\
5 & 1 & 2 \\
4 & 2  & 4 \\
1 & -2 & -1
\end{array}\right) \,.
\]

La ecuación para la transformación de cualquier vector es entonces:
\[
\mathbb{T}\left| {\tilde x}_i \right> =T_{i}^{j}\left|x_j \right>
\,\, \Rightarrow \,\,
\left(\begin{array}{r}
 {\tilde x}^1  \\
 {\tilde x}^2 \\
 {\tilde x}^3 \\
 {\tilde x}^4
\end{array}\right)= 
\left(\begin{array}{rrr}
7 & 2 & 3 \\
5 & 1 & 2 \\
4 & 2  & 4 \\
1 & -2 & -1
\end{array}\right) 
\left(\begin{array}{r}
 {x}^1  \\
 {x}^2  \\
 {x}^3 
\end{array}\right)\,.
\]

Pedimos al lector repetir los cálculos pero considerando las bases:
\[
\left|\mathrm{e}_{i}\right>=\{(1,1,0), (1,0,1),(0,1,1)\} \in \mathds{R}^3\,, \quad 
\left|\mathrm{\tilde e}_{i}\right>=\{(1,0,0,0), (0,1,0,0),(0,0,1,0), (0,0,0, 1)\} \in \mathds{R}^4\,;
\]
y también las siguientes bases canónicas
\[
\left|\mathrm{e}_{i}\right>=\{(1,0,0), (0,1,0),(0,0,1)\} \in \mathds{R}^3\,, \quad 
\left|\mathrm{\tilde e}_{i}\right>=\{(1,0,0,0), (0,1,0,0),(0,0,1,0), (0,0,0, 1)\} \in \mathds{R}^4\,.
\] ?`Qué puede concluir de los resultados obtenidos de este último par de bases?

\item Para la matriz
\[
  \mathbb{A} = \left(
\begin{array}
[c]{rrr}
3 & -2 & 2\\
1 & 2 & -3 \\
4 & 1 & 2 
\end{array}
\right) \,,
\]
el desarrollo de Laplace (\ref{formulaLaplace}) para la primera fila ($i=1$) es:
\begin{eqnarray*}
\det|\mathbb{A}| &=& A^{1}_{1} (A^{c})^{1}_{1} + A^{1}_{2} (A^{c})^{1}_{2} +
A^{1}_{3} (A^{c})^{1}_{3}\\
&=& 3
\left|
\begin{array}
[c]{rr}
2 & -3 \\
1 & 2  
\end{array}
\right|
-(-2)
\left|
\begin{array}
[c]{rr}
1 & -3 \\
4 & 2  
\end{array}
\right|
+2
\left|
\begin{array}
[c]{rr}
1 & 2 \\
4 & 1  
\end{array}
\right| \\
&=& 3(7) + 2(14)+2(-7)= 35 \,.
\end{eqnarray*}

\item  En Mecánica Clásica la cantidad de momento angular viene definida como $\mathbf{L}=\mathbf{r}\times\mathbf{p}$. Para pasar a Mecánica Cuántica se asocia $\mathbf{r}$ y $\mathbf{p}$ con los operadores posición y cantidad de movimiento los cuales, al operar sobre la función de onda nos proveen lo siguiente:
\[
\begin{array}
[c]{ccc}
\left\langle {r}\right|  \mathbb{X}\left|  \psi\right\rangle
=x\left\langle {r}\right.  \left|  \psi\right\rangle =x\ \psi\left(
\mathbf{r}\right)   & \quad & \left\langle {r}\right|  \mathbb{P}
_{x}\left|  \psi\right\rangle =\left(  -i\hbar\ \dfrac{\partial}{\partial
x}\right)  \left\langle {r}\right.  \left|  \psi\right\rangle
=-i\hbar\ \dfrac{\partial}{\partial x}\psi\left(  \mathbf{r}\right)  \\
&  & \\
\left\langle {r}\right|  \mathbb{Y}\left|  \psi\right\rangle
=y\left\langle {r}\right.  \left|  \psi\right\rangle =y\ \psi\left(
\mathbf{r}\right)   &  & \left\langle {r}\right|  \mathbb{P}_{y}\left|
\psi\right\rangle =\left(  -i\hbar\ \dfrac{\partial}{\partial y}\right)
\left\langle {r}\right.  \left|  \psi\right\rangle =-i\hbar
\ \dfrac{\partial}{\partial y}\psi\left(  \mathbf{r}\right)  \\
&  & \\
\left\langle {r}\right|  \mathbb{Z}\left|  \psi\right\rangle
=z\left\langle {r}\right.  \left|  \psi\right\rangle =z\ \psi\left(
\mathbf{r}\right)   &  & \left\langle {r}\right|  \mathbb{P}_{z}\left|
\psi\right\rangle =\left(  -i\hbar\ \dfrac{\partial}{\partial z}\right)
\left\langle {r}\right.  \left|  \psi\right\rangle =-i\hbar
\ \dfrac{\partial}{\partial z}\psi\left(  \mathbf{r}\right)
\end{array}
\]
En coordenadas cartesianas, en la representación de
coordenadas $\left\{  \left|  {r}\right\rangle \right\}$ tendremos que
\[
\left\langle {r}\right|  \mathbb{R}\left|  \psi\right\rangle =
\mathbf{r}\ \psi\left(  \mathbf{r}\right)  \quad\text{y}\quad\left\langle
{r}\right|  \mathbb{P}_{x}\left|  \psi\right\rangle =-i\hbar
\ \boldsymbol{\nabla}\ \psi\left(  \mathbf{r}\right) \,.
\]

De forma que en Mecánica Cuántica las componentes cartesianas del operador cantidad de movimiento angular son: 
\begin{align*}
\left\langle {r}\right|  \mathbb{{L}}\left|  \psi\right\rangle  &
=-i\hbar\ \left(  \mathbf{r\times}\boldsymbol{\nabla}\right)  
\psi\left(  \mathbf{r}\right)  \\
& \\
\left\langle {r}\right|  \mathbb{{L}}\left|  \psi\right\rangle  &
=-i\hbar\ \left(  y\dfrac{\partial}{\partial z}-z\dfrac{\partial}{\partial
y}\right)  \psi\left(  \mathbf{r}\right)  \mathbf{i}-
i\hbar\ \left(z\dfrac{\partial}{\partial x}-x\dfrac{\partial}{\partial z}\right)
\psi\left(  \mathbf{r}\right)  \mathbf{j}-i\hbar\ \left(
x\dfrac{\partial}{\partial y}-y\dfrac{\partial}{\partial x}\right)
\psi\left(  \mathbf{r}\right)  \mathbf{{k}} \,.
\end{align*}

Utilizando las definiciones anteriores vamos a mostrar que el conmutador de las componentes cartesianas de la cantidad de movimiento angular cumple la siguiente relación
\[
\left[  \mathbb{L}_{x},\mathbb{L}_{y}\right]  \left|  \psi\right\rangle
=i\hbar\mathbb{L}_{z}\left|  \psi\right\rangle \,,
\]
con:  $\mathbb{L}^{1}=\mathbb{L}_{1}=\mathbb{L}_{x};$ $\mathbb{L}^{2}=\mathbb{L}_{2}=\mathbb{L}_{y};$ $\mathbb{L}^{3}=\mathbb{L}_{3}=\mathbb{L}_{z}$.  En general, 
$\left[  \mathbb{L}_{l},\mathbb{L}_{m}\right]  =i\hbar \varepsilon_{lmn}\mathbb{L}^{n}$.

Dado que:
\begin{align*}
\left[  \mathbb{L}_{1},\mathbb{L}_{2}\right]  \left|  \psi\right\rangle  &
=\left[ \mathbb{L}_{x},\mathbb{L}_{y}\right]  \left|  \psi\right\rangle
=\left( \mathbb{L}_{x}\mathbb{L}_{y}-\mathbb{L}_{y}\mathbb{L}_{x}\right)
\left|  \psi\right\rangle \\
&  =-i\hbar\ \left(  \left(  y\dfrac{\partial}{\partial z}-z\dfrac{\partial
}{\partial y}\right)  \left(  z\dfrac{\partial}{\partial x}-x\dfrac{\partial
}{\partial z}\right)  -\left(  z\dfrac{\partial}{\partial x}-x\dfrac{\partial
}{\partial z}\right)  \left(  y\dfrac{\partial}{\partial z}-z\dfrac{\partial
}{\partial y}\right)  \right)  \psi\left(  \mathbf{r}\right) \\
&  =\left[   y\dfrac{\partial}{\partial z}\left(  z\dfrac{\partial
}{\partial x}-x\dfrac{\partial}{\partial z}\right)  -z\dfrac{\partial
}{\partial y}\left(  z\dfrac{\partial}{\partial x}-x\dfrac{\partial}{\partial
z}\right)    -\left(  z\dfrac{\partial}{\partial x}\left(
y\dfrac{\partial}{\partial z}-z\dfrac{\partial}{\partial y}\right)
-x\dfrac{\partial}{\partial z}\left(  y\dfrac{\partial}{\partial z}
-z\dfrac{\partial}{\partial y}\right)  \right)  \right]  \psi\left(\mathbf{r}\right) \,.
\end{align*}

Con lo cual:
\begin{align*}
&  =\left[  \left(  \left(  yz\dfrac{\partial}{\partial z\partial
x}+y\dfrac{\partial}{\partial x}\right)  -xy\dfrac{\partial^{2}}{\partial
z^{2}}\right)  -\left(  z^{2}\dfrac{\partial}{\partial y\partial x}
-zx\dfrac{\partial}{\partial y\partial z}\right)  \right.  \\
&  \qquad\qquad\left.  -\left(  \left(  zy\dfrac{\partial}{\partial x\partial
z}-z^{2}\dfrac{\partial}{\partial y\partial x}\right)  -\left(  yx\dfrac
{\partial}{\partial z^{2}}-zx\dfrac{\partial}{\partial z\partial y}
-x\dfrac{\partial}{\partial y}\right)  \right)  \right]  \psi\left(  \mathbf{r}\right) \\
& \\
&  =\left(  y\dfrac{\partial}{\partial x}-x\dfrac{\partial}{\partial
y}\right)  \psi\left(  \mathbf{r}\right) \,.
\end{align*}

\item  Consideremos que el espacio de estados para un determinado sistema físico viene expandido por la base ortonormal $\left\{  \left|
u_{1}\right\rangle ,\left|  u_{2}\right\rangle ,\left|  u_{3}\right\rangle
\right\}$. Definamos dos operadores $\mathbb{L}_{z}$ y $\mathbb{S}$ de la siguiente manera:
\begin{align*}
\mathbb{L}_{z}\left|u_{1}\right\rangle &=\left|u_{1}\right\rangle\,, \quad 
\mathbb{L}_{z}\left|u_{2}\right\rangle =0\,, \quad 
\mathbb{L}_{z}\left|u_{3}\right\rangle =-\left|  u_{3}\right\rangle \,, \\
\mathbb{S}\left|  u_{1}\right\rangle  &  =\left| u_{3}\right\rangle\,,\quad
\mathbb{S}\left|  u_{2}\right\rangle =\left|  u_{2}\right\rangle\,, \quad
\mathbb{S}\left|  u_{3}\right\rangle =\left|  u_{1}\right\rangle \,.
\end{align*}

\begin{enumerate}
\item Calculemos la representación matricial en la base $\left\{  \left|
u_{1}\right\rangle ,\left|  u_{2}\right\rangle ,\left|  u_{3}\right\rangle
\right\}  $ del operador: $\left[  \mathbb{L}_{z},\mathbb{S}\right]$. 

La matriz será:
\[
\left(
\begin{array}
[c]{ccccc}
\left\langle u^{1}\right|  \mathbb{L}_{z}\mathbb{S-SL}_{z}\left|
u_{1}\right\rangle  &  & \left\langle u^{1}\right|  \mathbb{L}_{z}
\mathbb{S-SL}_{z}\left|  u_{2}\right\rangle  &  & \left\langle u^{1}\right|
\mathbb{L}_{z}\mathbb{S-SL}_{z}\left|  u_{3}\right\rangle \\
&  &  &  & \\
\left\langle u^{2}\right|  \mathbb{L}_{z}\mathbb{S-SL}_{z}\left|
u_{1}\right\rangle  &  & \left\langle u^{2}\right|  \mathbb{L}_{z}
\mathbb{S-SL}_{z}\left|  u_{2}\right\rangle  &  & \left\langle u^{2}\right|
\mathbb{L}_{z}\mathbb{S-SL}_{z}\left|  u_{3}\right\rangle \\
&  &  &  & \\
\left\langle u^{3}\right|  \mathbb{L}_{z}\mathbb{S-SL}_{z}\left|
u_{1}\right\rangle  &  & \left\langle u^{3}\right|  \mathbb{L}_{z}
\mathbb{S-SL}_{z}\left|  u_{2}\right\rangle  &  & \left\langle u^{3}\right|
\mathbb{L}_{z}\mathbb{S-SL}_{z}\left|  u_{3}\right\rangle
\end{array}
\right)
\]
con lo cual:
\[
\left(
\begin{array}
[c]{ccccc}
\left\langle u^{1}\right|  \mathbb{L}_{z}\mathbb{S}\left|  u_{1}\right\rangle
-\left\langle u^{1}\right|  \mathbb{SL}_{z}\left|  u_{1}\right\rangle  &  &
\left\langle u^{1}\right|  \mathbb{L}_{z}\mathbb{S}\left|  u_{2}\right\rangle
-\left\langle u^{1}\right|  \mathbb{SL}_{z}\left|  u_{2}\right\rangle  &  &
\left\langle u^{1}\right|  \mathbb{L}_{z}\mathbb{S}\left|  u_{3}\right\rangle
-\left\langle u^{1}\right|  \mathbb{SL}_{z}\left|  u_{3}\right\rangle \\
&  &  &  & \\
\left\langle u^{2}\right|  \mathbb{L}_{z}\mathbb{S}\left|  u_{1}\right\rangle
-\left\langle u^{2}\right|  \mathbb{SL}_{z}\left|  u_{1}\right\rangle  &  &
\left\langle u^{2}\right|  \mathbb{L}_{z}\mathbb{S}\left|  u_{2}\right\rangle
-\left\langle u^{2}\right|  \mathbb{SL}_{z}\left|  u_{2}\right\rangle  &  &
\left\langle u^{2}\right|  \mathbb{L}_{z}\mathbb{S}\left|  u_{3}\right\rangle
-\left\langle u^{2}\right|  \mathbb{SL}_{z}\left|  u_{3}\right\rangle \\
&  &  &  & \\
\left\langle u^{3}\right|  \mathbb{L}_{z}\mathbb{S}\left|  u_{1}\right\rangle
-\left\langle u^{3}\right|  \mathbb{SL}_{z}\left|  u_{1}\right\rangle  &  &
\left\langle u^{3}\right|  \mathbb{L}_{z}\mathbb{S}\left|  u_{2}\right\rangle
-\left\langle u^{3}\right|  \mathbb{SL}_{z}\left|  u_{2}\right\rangle  &  &
\left\langle u^{3}\right|  \mathbb{L}_{z}\mathbb{S}\left|  u_{3}\right\rangle
-\left\langle u^{3}\right|  \mathbb{SL}_{z}\left|  u_{3}\right\rangle
\end{array}
\right) \,,
\]
de donde:
\[
\left\langle u^{i}\right|  \left[  \mathbb{L}_{z},\mathbb{S}\right]  \left|
u_{j}\right\rangle =\left(
\begin{array}
[c]{ccccc}
0-0 &  & 0-0 &  & 1-\left(  {-}1\right)  \\
&  &  &  & \\
0-0 &  & 0-0 &  & 0-0\\
&  &  &  & \\
\left(  -1\right)  -1 &  & 0-0 &  & 0-0
\end{array}
\right)  =\left(
\begin{array}
[c]{ccccc}
0 &  & 0 &  & 2\\
&  &  &  & \\
0 &  & 0 &  & 0\\
&  &  &  & \\
-2 &  & 0 &  & 0
\end{array}
\right)\,.
\]

\item ¿$\mathbb{L}_{z},\mathbb{S}$ y $\left[  \mathbb{L}_{z},\mathbb{S}\right]$ serán biyectivas?
 
Veamos. Por definición $\mathbb{S}$ es biyectiva ya que cada vector tiene su imagen, $\mathbb{L}_{z}$ no lo es por cuanto el vector $\left|  u_{2}\right\rangle$ no tiene imagen y, finalmente $\left[  \mathbb{L}_{z},\mathbb{S}\right]$ tampoco será biyectiva dado que $\left\langle u^{i}\right|  \left[ \mathbb{L}_{z},\mathbb{S}\right]  \left|  u_{j}\right\rangle$ no tiene inversa ya que  
$\det\left|  \left\langle u^{i}\right|  \left[ \mathbb{L}_{z},\mathbb{S}\right]  \left|  u_{j}\right\rangle \right|  =0$. 

\item  Calculemos la dimensión del dominio, del rango y del núcleo de la transformaciones $\mathbb{L}_{z},\mathbb{S}$ y $\left[  \mathbb{L}_{z},\mathbb{S}\right]$.
\[
\begin{tabular}
[c]{lccc}
& {Dominio} & {Rango} & {Núcleo}\\
$\mathbb{L}_{z}$ & {3} & {2} & {1}\\
$\mathbb{S}$ & {3} & {3} & {0}\\
$\left[ \mathbb{L}_{z},\mathbb{S}\right]$ & {3} & {2} & {2}
\end{tabular}
\]
dado que $\left[  \mathbb{L}_{z},\mathbb{S}\right]  \left|  u_{2}\right\rangle =0$\,.
\end{enumerate}

\item  Buscaremos la expresión matricial para los operadores lineales de Pauli: $\mathds{R}^{2}\longmapsto\mathds{R}^{2}$ sabiendo que actúan como:
\[
{\sigma}_{z}\left|  +\right\rangle=\left|  +\right\rangle \,,\,\,
{\sigma}_{z}\left|  -\right\rangle =-\left|  -\right\rangle \,,\,\,
{\sigma}_{x}\left|  +\right\rangle _{x}  =\left|  +\right\rangle_{x}\,,\,\,
{\sigma}_{x}\left|  -\right\rangle _{x}=-\left|-\right\rangle _{x}\,,\,\,
{\sigma}_{y}\left|  +\right\rangle _{y} =\left|  +\right\rangle_{y}\,,\,\,
{\sigma}_{y}\left|  -\right\rangle _{y}=-\left|-\right\rangle _{y} 
\]
con:
\[
\left|+\right\rangle   \leftrightarrows\left(
\begin{array}
[c]{c}
1\\
0
\end{array}
\right)\,,\,\, \left|  -\right\rangle \leftrightarrows\left(
\begin{array}
[c]{c}
0\\
1
\end{array}
\right) \,,\,\,
\left|  +\right\rangle _{x}   =\dfrac{1}{\sqrt{2}}\left[  \left|
+\right\rangle +\left|  -\right\rangle \right]  \,,\,\,
\left|  -\right\rangle
_{x}=\dfrac{1}{\sqrt{2}}\left[  \left|  +\right\rangle -\left|  -\right\rangle\right] \,,
\]
\[
\left|  +\right\rangle _{y} =\dfrac{1}{\sqrt{2}}\left[  \left|
+\right\rangle +i\left|  -\right\rangle \right] \,,\,\,
\left|  -\right\rangle_{y}=\dfrac{1}{\sqrt{2}}
\left[  \left|  +\right\rangle -i\left|
-\right\rangle \right] \,.
\]

Ahora bien:
\[
\begin{array}
[c]{ccccc}
_{x}\left\langle +\right. \left| +\right\rangle_{x}=1\,, &  &
_{x}\left\langle+\right. \left| -\right\rangle _{x}=_{x}\left\langle -\right.  \left|+\right\rangle _{x}=0\,, &  &
_{x}\left\langle -\right.  \left|  -\right\rangle_{x}=1 \,, \\
&  &  &  & \\
_{y}\left\langle +\right.  \left|  +\right\rangle _{y}=1\,, &  & 
_{y}\left\langle+\right.  \left|  -\right\rangle _{y}=_{y}\left\langle -\right. \left|+\right\rangle _{y}=0\,, &  & 
_{y}\left\langle -\right.  \left|  -\right\rangle_{y}=1 \,.
\end{array}
\]

Es decir, los vectores $\left\{  \left|  +\right\rangle _{x},\left|
-\right\rangle _{x}\right\}  $ y $\left\{  \left|  +\right\rangle _{y},\left|
-\right\rangle _{y}\right\}  $ forman bases\ ortonormales, por lo que los
vectores $\left\{  \left|  +\right\rangle ,\left|  -\right\rangle \right\}  $
se pueden expresar en término de esas bases como:
\[
\begin{array}
[c]{ccc}
\left|+\right\rangle =\dfrac{1}{\sqrt{2}}\left[\left|+\right\rangle_{x}+\left|  -\right\rangle _{x}\right]\,, &  & \left|-\right\rangle=\dfrac{1}{\sqrt{2}}\left[\left| +\right\rangle _{x}-\left|  -\right\rangle_{x}\right] \,,\\
&  & \\
\left|+\right\rangle =\dfrac{1}{\sqrt{2}}\left[\left|+\right\rangle_{y}+\left| -\right\rangle _{y}\right]\,, &  & \left|-\right\rangle=\dfrac{-i}{\sqrt{2}}\left[ \left|  +\right\rangle _{y}-\left|-\right\rangle _{y}\right]\,.
\end{array}
\]

Así las expresiones matriciales serán:
\[
\left(  \mathbf{\sigma}_{z}\right)  _{j}^{i}=
\left(
\begin{array}
[c]{ccc}
\left\langle +\right|  \mathbf{\sigma}_{z}\left|  +\right\rangle  &  &
\left\langle +\right|  \mathbf{\sigma}_{z}\left|  -\right\rangle \\
&  & \\
\left\langle -\right|  \mathbf{\sigma}_{z}\left|  +\right\rangle  &  &
\left\langle -\right|  \mathbf{\sigma}_{z}\left|  -\right\rangle
\end{array}
\right)  =\left(
\begin{array}
[c]{ccc}
1 &  & 0\\
&  & \\
0 &  & -1
\end{array}
\right) \,.
\]

Para $\left(  \mathbf{\sigma}_{x}\right)  _{j}^{i} $: 
\begin{align*}
\left(  \mathbf{\sigma}_{x}\right)  _{j}^{i}  & =
\left(
\begin{array}
[c]{ccc}
\left\langle +\right|  \mathbf{\sigma}_{x}\left|  +\right\rangle  &  &
\left\langle +\right|  \mathbf{\sigma}_{x}\left|  -\right\rangle \\
&  & \\
\left\langle -\right|  \mathbf{\sigma}_{x}\left|  +\right\rangle  &  &
\left\langle -\right|  \mathbf{\sigma}_{x}\left|  -\right\rangle
\end{array}
\right)  \\
& \\
& =\frac12 \left(
\begin{array}
[c]{ccc}
\left[  _{x}\left\langle +\right|  +_{x}\left\langle -\right|
\right]  \mathbf{\sigma}_{x}\left[  \left|  +\right\rangle _{x}+\left|
-\right\rangle _{x}\right]   &  & \left[  _{x}\left\langle
+\right|  +_{x}\left\langle -\right|  \right]  \mathbf{\sigma}_{x}\left[
\left|  +\right\rangle _{x}-\left|  -\right\rangle _{x}\right]  \\
&  & \\
\left[  _{x}\left\langle +\right|  -_{x}\left\langle -\right|
\right]  \mathbf{\sigma}_{x}\left[  \left|  +\right\rangle _{x}+\left|
-\right\rangle _{x}\right]   &  & \left[  _{x}\left\langle
+\right|  -_{x}\left\langle -\right|  \right]  \mathbf{\sigma}_{x}\left[
\left|  +\right\rangle _{x}-\left|  -\right\rangle _{x}\right]
\end{array}
\right)  \\
& \\
& =\frac12\left(
\begin{array}
[c]{ccc}
\left[  _{x}\left\langle +\right|  +_{x}\left\langle -\right|
\right]  \left[  \left|  +\right\rangle _{x}-\left|  -\right\rangle
_{x}\right]   &  & \left[  _{x}\left\langle +\right|
+_{x}\left\langle -\right|  \right]  \left[  \left|  +\right\rangle
_{x}+\left|  -\right\rangle _{x}\right]  \\
&  & \\
\left[  _{x}\left\langle +\right|  -_{x}\left\langle -\right|
\right]  \left[  \left|  +\right\rangle _{x}-\left|  -\right\rangle
_{x}\right]   &  &\left[  _{x}\left\langle +\right|
-_{x}\left\langle -\right|  \right]  \left[  \left|  +\right\rangle
_{x}+\left|  -\right\rangle _{x}\right]
\end{array}
\right)  \\
& \\
& =\left(
\begin{array}
[c]{ccc}
0 &  & 1\\
&  & \\
1 &  & 0
\end{array}
\right)
\end{align*}

Y finalmente, para $\left(  \mathbf{\sigma}_{y}\right)  _{j}^{i}$:
\begin{align*}
\left(  \mathbf{\sigma}_{y}\right)  _{j}^{i}  & =\left(
\begin{array}
[c]{ccc}
\left\langle +\right|  \mathbf{\sigma}_{y}\left|  +\right\rangle  &  &
\left\langle +\right|  \mathbf{\sigma}_{y}\left|  -\right\rangle \\
&  & \\
\left\langle -\right|  \mathbf{\sigma}_{y}\left|  +\right\rangle  &  &
\left\langle -\right|  \mathbf{\sigma}_{y}\left|  -\right\rangle
\end{array}
\right)  \\
& \\
& =\frac12\left(
\begin{array}
[c]{ccc}
\left[  _{y}\left\langle +\right|  +_{y}\left\langle -\right|
\right]  \mathbf{\sigma}_{y}\left[  \left|  +\right\rangle _{y}+\left|
-\right\rangle _{y}\right]   &  & {-i}\left[  _{y}\left\langle
+\right|  +_{y}\left\langle -\right|  \right]  \mathbf{\sigma}_{y}\left[
\left|  +\right\rangle _{y}-\left|  -\right\rangle _{y}\right]  \\
&  & \\
{i}\left[  _{y}\left\langle +\right|  -_{y}\left\langle -\right|
\right]  \mathbf{\sigma}_{y}\left[  \left|  +\right\rangle _{y}+\left|
-\right\rangle _{y}\right]   &  & \left[  _{y}\left\langle
+\right|  -_{y}\left\langle -\right|  \right]  \mathbf{\sigma}_{y}\left[
\left|  +\right\rangle _{y}-\left|  -\right\rangle _{y}\right]
\end{array}
\right)  \\
& \\
& =\frac12\left(
\begin{array}
[c]{ccc}
\left[  _{y}\left\langle +\right|  +_{y}\left\langle -\right|
\right]  \left[  \left|  +\right\rangle _{y}-\left|  -\right\rangle
_{y}\right]   &  & {-i}\left[  _{y}\left\langle +\right|
+_{y}\left\langle -\right|  \right]  \left[  \left|  +\right\rangle
_{y}+\left|  -\right\rangle _{y}\right]  \\
&  & \\
{i}\left[  _{y}\left\langle +\right|  -_{y}\left\langle -\right|
\right]  \left[  \left|  +\right\rangle _{y}-\left|  -\right\rangle
_{y}\right]   &  & -\left[  _{y}\left\langle +\right|
-_{y}\left\langle -\right|  \right]  \left[  \left|  +\right\rangle
_{y}+\left|  -\right\rangle _{y}\right]
\end{array}
\right)  \\
& \\
& =\left(
\begin{array}
[c]{ccc}
0 &  & -i\\
&  & \\
i &  & 0
\end{array}
\right) \,.
\end{align*}

\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}}
\begin{enumerate}
\item Consideremos  la transformación lineal del ejercicio resuelto con anterioridad:
\[
\mathbb{T}: \mathds{R}^3 \rightarrow \mathds{R}^4  \,, \,\,
\mathbb{T}\left[ \left(x,y,z\right) \right]=\left(x+y,x-2z,x+2y+3z,y-2z\right)\,.
\]

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(vect)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T(x,y,z):=[x+y,x-2*z,x+2*y+3*z,y-2*z];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
T(x,y,z):=[x+y,x-2z,x+2y+3z,y-2z]
\end{math}
\newline

Escribimos ahora los vectores de la base en $\mathds{R}^4$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e1:[1,1,1,1];e2:[0,1,1,1];e3:[0,0,1,1]; e4:[0,0,0,1];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
[1,1,1,1]
\end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
[0,1,1,1]
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
[0,0,1,1] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
[0,0,0,1] 
\end{math}
\newline

Entonces:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve(T(1,1,0)-C11*e1-C12*e2-C13*e3-C14*e4,[C11,C12,C13,C14]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\left[ \left[ {\tt C}11=2 , {\tt C}12=-1 , {\tt C}13=2 , {\tt C}14=-2 \right]  \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[C11,C12,C13,C14]:[rhs(%[1]),rhs(%[2]),rhs(%[3]),rhs(%[4])];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
[2,-1,2,-2]
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve(T(1,0,1)-C21*e1-C22*e2-C23*e3-C24*e4,[C21,C22,C23,C24]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\left[ \left[ {\tt C}21=1 , {\tt C}22=-2 , {\tt C}23=5 , {\tt C}24=-6 \right]  \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[C21,C22,C23,C24]:[rhs(%[1]),rhs(%[2]),rhs(%[3]),rhs(%[4])];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
[1,-2,5,-6]
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve(T(0,1,1)-C31*e1-C32*e2-C33*e3-C34*e4,[C31,C32,C33,C34]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\left[ \left[ {\tt C}31=1 , {\tt C}32=-3 , {\tt C}33=7 , {\tt C}34=-6 \right]  \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[C31,C32,C33,C34]:[rhs(%[1]),rhs(%[2]),rhs(%[3]),rhs(%[4])];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
[1,-3,7,-6]
\end{math}
\newline

Calculemos ahora las componentes de la representación matricial:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T11: C11*(e1.e1) + C12*(e1.e2) +C13*(e1.e3) + C14*(e1.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
7
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T21: C21*(e1.e1) + C22*(e1.e2) +C23*(e1.e3) + C24*(e1.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
2
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T31: C31*(e1.e1) + C32*(e1.e2) +C33*(e1.e3) + C34*(e1.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
3
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T12: C11*(e2.e1) + C12*(e2.e2) +C13*(e2.e3) + C14*(e2.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
5
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T22: C21*(e2.e1) + C22*(e2.e2) +C23*(e2.e3) + C24*(e2.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
1
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T32: C31*(e2.e1) + C32*(e2.e2) +C33*(e2.e3) + C34*(e2.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
2
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T13: C11*(e3.e1) + C12*(e3.e2) +C13*(e3.e3) + C14*(e3.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o19) }
4
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i20) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T23: C21*(e3.e1) + C22*(e3.e2) +C23*(e3.e3) + C24*(e3.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o20) }
2
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i21) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T33: C31*(e3.e1) + C32*(e3.e2) +C33*(e3.e3) + C34*(e3.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o21) }
4
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i22) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T14: C11*(e4.e1) + C12*(e4.e2) +C13*(e4.e3) + C14*(e4.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o22) }
1
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i23) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T24: C21*(e4.e1) + C22*(e4.e2) +C23*(e4.e3) + C24*(e4.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o23) }
-2
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i24) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T34: C31*(e4.e1) + C32*(e4.e2) +C33*(e4.e3) + C34*(e4.e4);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o24) }
-1
\end{math}
\newline


Por lo tanto, la matriz que representa a la transformación es la siguiente:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i25) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
matrix([T11,T21,T31],[T12,T22,T32],[T13,T23,T33],[T14,T24,T34]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o25) }
\begin{pmatrix}
7 & 2 & 3 \\ 5 & 1 & 2 \\ 4 & 2 & 4 \\ 1 & -2 &  -1 \\ \end{pmatrix}
\end{math}
\newline

Podemos repetir los cálculos pero ahora con la base canónica:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i26) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e1:[1,0,0,0];e2:[0,1,0,0];e3:[0,0,1,0]; e4:[0,0,0,1];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o26) }
[1,0,0,0]
\end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o27) }
[0,1,0,0]
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o28) }
[0,0,1,0] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o29) }
[0,0,0,1] 
\end{math}
\newline

Por ser la base ortonormal el cálculo de la representación matricial de la transformación es directa:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i30) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve(T(1,1,0)-C11*e1-C12*e2-C13*e3-C14*e4,[C11,C12,C13,C14]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o30) }
 \left[ {\tt C}11=2 , {\tt C}12=1 , {\tt C}13=3 , {\tt C}14=1 \right]  
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i31) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[C11,C12,C13,C14]:[rhs(%[1]),rhs(%[2]),rhs(%[3]),rhs(%[4])];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o31) }
[2,1,3,1] 
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i32) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve(T(1,0,1)-C21*e1-C22*e2-C23*e3-C24*e4,[C21,C22,C23,C24]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o32) }
 \left[ {\tt C}21=1 , {\tt C}22=-1 , {\tt C}23=4 , {\tt C}24=-2 \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i33) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[C21,C22,C23,C24]:[rhs(%[1]),rhs(%[2]),rhs(%[3]),rhs(%[4])];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o33) }
[1,-1,4,-2]
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i34) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve(T(0,1,1)-C31*e1-C32*e2-C33*e3-C34*e4,[C31,C32,C33,C34]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o34) }
 \left[ {\tt C}31=1 , {\tt C}32=-2 , {\tt C}33=5 , {\tt C}34=-1 \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i35) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[C31,C32,C33,C34]:[rhs(%[1]),rhs(%[2]),rhs(%[3]),rhs(%[4])];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o35) }
[1,-2,5,-1]
\end{math}
\newline

La representación matricial que resulta ahora es:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i36) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
matrix([2,1,1],[1,-1,-2],[3,4,5], [1,-2,-1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o36) }
\begin{pmatrix}2 & 1 & 1 \\ 1 & -1 & -2 \\ 3 & 4 & 5 \\ 1 & -2 & -1  \\ \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i37) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage}
 
\item Dada una transformación $\mathbb{T}: \mathds{R}^2 \rightarrow \mathds{R}^3$, donde una base para $\mathds{R}^3$  es:
\[
\left|\mathrm{e}_{i}\right>=\{(1,2,1), (1,1,0),(0,0,3)\} \in \mathds{R}^3\,, 
\]
y las imágenes de esta base respecto a la base canónica en $\mathds{R}^2$ son:
\[
\mathbb{T}\left[ \left|\mathrm{e}_{1}\right>\right]= (3,3) \,,\,\,
\mathbb{T}\left[ \left|\mathrm{e}_{2}\right>\right]= (6,3) \,,\,\,
\mathbb{T}\left[ \left|\mathrm{e}_{3}\right>\right]= (3,6) \,.
\]
¿Qué transformación está asociada a las coordenadas canónicas en $\mathds{R}^3$? 

Escribamos los vectores.

 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
e1:[1,2,1];e2:[1,1,0];e3:[0,0,3];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\left[ 1 , 2 , 1 \right] 
\end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\left[ 1 , 1 , 0 \right]
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ 0 , 0 , 3 \right] 
\end{math}
\newline
 
Ahora introducimos las imágenes: 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Te1:[3,3];Te2:[6,3];Te3:[3,6];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\left[ 3 , 3 \right] 
\end{math}
 
%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ 6 , 3 \right] 
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\left[ 3 , 6 \right] 
\end{math}
 
Respecto a estas coordenadas la transformación tiene la siguiente representación vectorial:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A:matrix(Te1,Te2,Te3);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\begin{pmatrix}3 & 3 \\ 6 & 3 \\ 3 & 6 \\ \end{pmatrix}
\end{math}
\newline

Lo que queremos es buscar la representación matricial de la transformación pero respecto a la base canónica, por lo tanto debemos buscar primero el cambio de base a la base canónica en $\mathds{R}^3$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
E:invert(matrix([1,2,1],[1,1,0],[0,0,3]));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\begin{pmatrix}-1 & 2 & \frac{1}{3} \\ 1 & -1 & -\frac{1}{3} \\ 0
  & 0 & \frac{1}{3} \\ \end{pmatrix}
\end{math}
\newline

En la base canónica la transformación tiene la siguiente representación:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
AE:transpose(E.A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\begin{pmatrix}10 & -4 & 1 \\ 5 & -2 & 2 \\ \end{pmatrix}
\end{math}
\newline

Por lo tanto la transformación es:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
T(x,y,z):=[10*x-4*y+z,5*x-2*y+2*z];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
T(x,y,z):=[10x-4y+z,5x-2y+2z]
\end{math}
\newline

Podemos verificar si el resultado es el correcto.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve(T(1,2,1)-a*[1,0]-b*[0,1],[a,b]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\left[ \left[ a=3 , b=3 \right]  \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve(T(1,1,0)-a*[1,0]-b*[0,1],[a,b]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\left[ \left[ a=6 , b=3 \right]  \right] 
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve(T(0,0,3)-a*[1,0]-b*[0,1],[a,b]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
\left[ \left[ a=3 , b=6 \right]  \right] 
\end{math}

 
\end{enumerate}
 
\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}


\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Considere el espacio $\mathcal{P}(t)$ de polinomios de grado $N$ en $t$, vale decir, $\left|f\right>_{t} \leftrightarrow \sum_{n=0}^{N} a_{0}t^{n}$, considere además un operador $\mathbb{T} = \mathrm{e}^{x\mathbb{D}} \equiv \exp(x\mathbb{D})$, con $\mathbb{D} = \frac{\mathrm{d}}{\mathrm{d}t} $.
  \begin{enumerate}
  \item Muestre que $\mathbb{T} \left|p\right>_{t} = \left|p\right>_{t + x} $, esto es que el operador $\mathbb{T}$ puede ser considerado un operador traslación espacial para los polinomios $\mathcal{P}(t)$ de grado $N$.
  \item Considere que el espacio de polinomios está definido en el intervalo $[-1,1]$, que definimos un producto interno de la forma  $\left<f\right|\left.g\right> = \int_{-1}^{1} f g\, \mathrm{d}t $ y un espacio de polinomios de grado $N = 2$. ¿Cuál es la representación matricial de $\mathbb{T}$ en la base de polinomios de Legendre $\{P_{0},P_{1},P_{2}\}$? 
\end{enumerate}

\item Heredando el formalismo de la Mecánica Clásica, se construye en Mecánica Cuántica el operador hamiltoniano, hermítico, para un sistema unidimensional como: 
\[
\mathbb{H} = \frac{\mathbb{P}^{2}}{2m} + \mathbb{V}(\mathbb{X})\,, 
\]
donde $\mathbb{H}, \mathbb{P}$ y $\mathbb{X}$ son operadores, y $ \mathbb{V}(\mathbb{X})$ un operador que es una función de otro operador. Adicionalmente, uno puede construir el siguiente operador: $[\mathbb{ X}, \mathbb{P} ] = i \hbar \mathbb{I}$.
\begin{enumerate}
  \item Determine los siguientes conmutadores: $[\mathbb{H},\mathbb{P}]$,  $[\mathbb{H},\mathbb{X}]$ y  $[\mathbb{H},\mathbb{X}\mathbb{P}]$.  
  \item Suponga que $\mathbb{H}\left|\psi_{n}  \right>  = E_{n} 
 \left|\psi_{n}  \right>$ y entonces calcule la representación matricial del siguiente operador $ \left< \psi^{m} \right| [  \mathbb{A}, \mathbb{H}] 
 \left|\psi_{n}  \right> $, para un operador arbitrario $\mathbb{A}$. 
\end{enumerate}

\item Considere la representación matricial de un operador en la base canónica de la forma:
\[
\mathbb{A} = 
\left(\begin{array}{cc}
 1     &  a  \\
   0   &  1 
\end{array}
\right) 
\] 
¿Existirá algún operador $\mathbb{B}$, no singular, tal que $\mathbb{D}= \mathbb{B}^{-1}\mathbb{A}\mathbb{B}$, con  $\mathbb{D}$ un operador diagonal? Si ese fuera el caso
¿Cuál sería la representación matricial de ese operador $\mathbb{B}$,  en la base canónica? Justifique su respuesta.

\item Si 
\[
M_i^j(t)=
\left(\begin{array}{cc}
\cos(\omega t) & \mbox{sen}(\omega t) \\
-\mbox{sen}(\omega t) & \cos(\omega t)
\end{array}\right) \,.
\]
Demuestre que:
\[
\frac{\mathrm{d}^2\mathbb{M}(t)}{\mathrm{d} t^2}=
-\omega^2 \mathbb{M}(t) \,.
\]

\item Dada a matriz:
\[
A_i^j=
\left(\begin{array}{ccc}
1 & 1 & 0 \\
1 & 1 & 1 \\
0 & 1 & 1
\end{array}\right)\,.
\]
Evalúe: $\mathbb{A}^2$, $\mathbb{A}^3$, $\mathbb{A}^{\mathbb{A}x}$.

\item Dada la matriz:
\[
A_i^j=\left(\begin{array}{ccc}
0 & 0 & i \\
-i & 0 & 0 \\
0 & -1 & 0
\end{array}\right)\,.
\]
Demuestre que $\mathbb{A}^n=\mathbb{A}\mathbb{A}\mathbb{A}...= \mathbb{I}$, con $n\neq0$.


\item Considere el siguiente  ``operador vectorial'' ${\boldsymbol{\sigma}} =  \sigma_{x}  \mathbf{i} + \sigma_{y} \mathbf{j} + \sigma_{z} \mathbf{k}$, donde 
las matrices $\sigma_{x}, \sigma_{y}$, $\sigma_{z}$ se conocen como operadores de Pauli y su representación matricial en la base canónica es:
\[
\sigma_{x}=\left(
\begin{array}
[c]{cc}
0 & 1\\
1 & 0
\end{array}
\right)\,,\quad 
\sigma_{y}=\left(
\begin{array}
[c]{cc}
0 & -i\\
i & 0
\end{array}
\right)\,, \quad
\sigma_{z}=\left(
\begin{array}
[c]{cc}
1 & 0\\
0 & -1
\end{array}
\right)\,, \quad\mathbf{I}=\left(
\begin{array}
[c]{cc}
1 & 0\\
0 & 1
\end{array}
\right) \,.
\]
El vector dirección puede escribirse como:
\[
\mathbf{n} = \mathrm{sen}(\theta) \cos(\phi) \mathbf{i} +  
\mathrm{sen}(\theta) \mathrm{sen}(\phi) \mathbf{j} + 
\cos(\theta) \mathbf{k} = n_{x} \mathbf{i} + n_{y} \mathbf{j} + n_{z} \mathbf{k}\,,
\]  
con lo cual: ${\boldsymbol{\sigma}} \cdot \mathbf{n} = n_{x}  \sigma_{x} + n_{y}  \sigma_{y}+ n_{z}  \sigma_{z}$.
 
A partir de todo lo anterior calcule la representación matricial del siguiente operador en la base canónica
\[
\exp\left( \frac{i}{2} \psi \ {\boldsymbol{\sigma}} \cdot \mathbf{n} \right)  \,.
\]

\item Dadas las siguientes matrices:
\[
\mathbb{L}_x=
\left(
\begin{array}{ccc}
0 & \sqrt{2}/{2} & 0 \\
\sqrt{2}/{2} & 0 & \sqrt{2}/{2} \\
0 & \sqrt{2}/{2} & 0
\end{array}\right)\,, \quad
\mathbb{L}_y=
\left(
\begin{array}{ccc}
0 & i\sqrt{2}/{2} & 0 \\
-i\sqrt{2}/{2} & 0 & i\sqrt{2}/{2} \\
0 & -i\sqrt{2}/{2} & 0
\end{array}\right)\,, \quad
\mathbb{L}_z=
\left(
\begin{array}{ccc}
-1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{array}\right)\,.
\]
Demuestre las siguientes reglas de conmutación:
\begin{enumerate}
\item $\mathbb{L}_x\mathbb{L}_y-\mathbb{L}_y\mathbb{L}_x=i\mathbb{L}_z$.
\item $\mathbb{L}_y\mathbb{L}_z-\mathbb{L}_z\mathbb{L}_y=i\mathbb{L}_x$.
\item $\mathbb{L}_z\mathbb{L}_x-\mathbb{L}_x\mathbb{L}_z=i\mathbb{L}_y$.
\item $\mathbb{L}_x^2+\mathbb{L}_y^2+\mathbb{L}_z^2=2$.
\end{enumerate}
\item Calcule los siguientes  determinantes:
\[
\left|
\begin{array}
[c]{ccc}
2 & 0 & 2\\
0 & 1& 0 \\
2 & 0 & 0 
\end{array}
\right|\,, \quad 
\left|
\begin{array}
[c]{cccc}
1 & 1 & 2 & 3\\
1 & 2-x^2& 2  & 3\\
2 & 3 & 1 & 5 \\
2 & 3 & 1 & 9-x^2
\end{array}
\right|\,, \quad 
\left|
\begin{array}
[c]{cccc}
1 & 0 & 2 & 3 \\
0 & 1 & -2 & 1 \\
3 & -3 & 4 & -2 \\
-2 & 1 & -2 & 0
\end{array}
\right|\,, \quad 
\left|
\begin{array}
[c]{ccccc}
-2 & 5 & 0 & -1 & 3\\
1 & 0 & 3 & 7 & -2\\
3 & -1 & 0 & 5&-5 \\
2 & 6 & -4 & 1&2 \\
0 & -3 & -1 & 2& 3
\end{array}
\right|\,.
\]

\item Utilizando las propiedades de los determinantes resuelva para $x$
\[
\left|\begin{array}{ccc}x+2 & x+4 & x-3 \\x+3 & x & x+5 \\x-2 & x-1 & x+1\end{array}\right| =0\,.
\]

\item Utilizando la propiedad lineal de los determinantes calcule:
\[
\Delta=
\left|
\begin{array}
[c]{cc}
a+b & n+p \\
c+d & q+r
\end{array}
\right|\,.
\]

\item Encuentre el determinante de la matriz $4\times4$ definida por la función:
\[
A_{j}^{i}=  \frac{1}{i+j-x}\,.
\]


\item Muestre que si las filas de un determinante de orden $n$ son linealmente independientes, entonces sus columnas son también linealmente independientes. 

\item Dada la representación matricial de un operador:
\[
\left(
\begin{array}{ccc}
1 & 1 & 3 \\
1 & 1 & -3 \\
3 & -3 & -3
\end{array}\right)\,.
\]
\begin{enumerate}
\item Encuentre la representación diagonal y la base que diagonaliza esta matriz.
\item Muestre que la traza y el determinante de ambas representaciones matriciales coinciden.
\item Encuentre la matriz de transformación a la representación diagonal.  
\end{enumerate}

\item Pruebe que la representación matricial, $2 \times 2$ más general para un operador unitario y simétrico (vale decir: $\mathbb{S}\mathbb{S}^{\dagger} = \mathbb{I}$ y  $\mathbb{S}^{T} = \mathbb{S}$) puede ser escrita como:
  \[
 \mathbb{S} =  
 \left(
 \begin{array}{cc}
   \alpha \mathrm{e}^{2i\beta}  &  i\sqrt{1- \alpha^{2}} \mathrm{e^{i(\beta +\gamma)}} \\
     i\sqrt{1- \alpha^{2}} \mathrm{e^{i(\beta +\gamma)}}  &      \alpha \mathrm{e}^{2i\gamma}  
\end{array}
 \right) \,,
  \]
con $\alpha, \beta$ y $\gamma$ parámetros reales y, adicionalmente $0 \leq \alpha \leq 1$.
  
\item Resuelva los ejercicios anteriores utilizando {\bf Maxima}. 

\end{enumerate}


\section{Un zoológico de matrices cuadradas}
\label{zoologicodematrices}
A continuación presentaremos un conjunto de matrices importantes y de operaciones que serán de gran utilidad para el resto de desarrollo del tema\footnote{Para mayores detalles sobre éstas y otros tipos de matrices pueden consultar Antony, R. and Alemayehu, H., A NOTE ON SPECIAL MATRICES, \textit{Italian journal of pure and applied mathematics}, (2015), 587-604.}. Es bueno tener en mente que las matrices pueden tener todas sus elementos reales, diremos que son matrices pertenecientes al espacio vectorial de matrices reales $\mathds{R}_{n\times m}$ o tener como elementos números complejos, en este caso diremos que pertenecen al espacio vectorial de matrices complejas  $\mathds{C}_{n\times m}$.

\subsection{Matriz unidad y la matriz nula}
La matriz unitaria es aquella que sólo tiene elementos en la diagonal iguales a la unidad
$
\mathbb{I} \,\, \Rightarrow \,\, I_{j}^{i}=\delta_{j}^{i}\,.
$, y el resto cero.
Mientras que la matriz nula es aquella con todos los elementos iguales a cero
$
\mathbb{O} \,\, \Rightarrow \,\, 0_{j}^{i}=0\,.
$

\subsection{Matriz diagonal y diagonal a bloques}
Una matriz diagonal es aquella que contiene elementos únicamente en la diagonal las demás componentes son cero:  $A^{i}_{j} = 0 \,\, \forall \,\, i \neq j$. Como veremos más adelante, existen  algunos métodos para reducir una matriz a su forma diagonal. Una propiedad importante de estas matrices es que conmutan entre si, es decir, 
$ \mathbb{A} \mathbb{B}= \mathbb{B} \mathbb{A}$,  
si $ \mathbb{A}$ y $ \mathbb{B}$ son diagonales. Podemos tener matrices diagonales a bloques, vale decir:
\[
D_{j}^{i}=\left(
\begin{array}
[c]{cccc}
D_{1}^{1} & D_{2}^{1} & 0 & 0\\
D_{1}^{2} & D_{2}^{2} & 0 & 0\\
0 & 0 & D_{3}^{3} & D_{4}^{3}\\
0 & 0 & D_{3}^{4} & D_{4}^{4}
\end{array}
\right) \, .
\]

También tenemos  las triangulares superior e inferior:
\[
\check{D}_{j}^{i}=\left(
\begin{array}
[c]{cccc}
\check{D}_{1}^{1} & \check{D}_{2}^{1} & \check{D}_{3}^{1} & \check{D}_{4}
^{1}\\
0 & \check{D}_{2}^{2} & D_{3}^{2} & \check{D}_{4}^{2}\\
0 & 0 & \check{D}_{3}^{3} & \check{D}_{4}^{3}\\
0 & 0 & 0 & \check{D}_{4}^{4}
\end{array}
\right)\, ,  \qquad\text{y}\qquad\hat{D}_{j}^{i}=\left(
\begin{array}
[c]{cccc}
\hat{D}_{1}^{1} & 0 & 0 & 0\\
\hat{D}_{1}^{2} & \hat{D}_{2}^{2} & 0 & 0\\
\hat{D}_{1}^{3} & D_{2}^{3} & \hat{D}_{3}^{3} & 0\\
\hat{D}_{1}^{4} & \hat{D}_{2}^{4} & \hat{D}_{3}^{4} & \hat{D}_{4}^{4}
\end{array}
\right) \,.
\]

\subsection{Matriz transpuesta}
Tal y como discutimos en la sección \ref{MatricesdeOperadoresHermiticos}, cuando desarrollamos el concepto de matrices hermíticas conjungadas, la matriz transpuesta es aquella  que se obtiene intercambiando filas por columnas: $\mathbb{A}= A_{j}^{i}\,\, \Rightarrow \,\,  \mathbb{A}^T=A_{i}^{j}$.  Si $ \mathbb{A}= \mathbb{A}^T$, ($ A_{j}^{i} =A_{i}^{j}$), se dice que $ \mathbb{A}$ es simétrica. Y si $ \mathbb{A}=- \mathbb{A}^T$, ($ A_{j}^{i} =-A_{i}^{j}$), es llamada antisimétrica. Por lo tanto, una matriz cuadrada se puede representar por la suma de una matriz simétrica y una antisimétrica
\[
 \mathbb{A}=\frac{1}{2}\left[  \mathbb{A}+ \mathbb{A}^T \right] +
\frac{1}{2}\left[  \mathbb{A}- \mathbb{A}^T\right] \,.
\]

\subsection{Matriz de cofactores}
A una matriz $ \mathbb{A}$ le podemos asociar una de cofactores  
$ \mathbb{A}^c$ de la manera siguiente:
\[
A_{j}^{i}=\left(
\begin{array}
[c]{ccc}
a_{1}^{1} & a_{2}^{1} & a_{3}^{1}\\
a_{1}^{2} & a_{2}^{2} & a_{3}^{2}\\
a_{1}^{3} & a_{2}^{3} & a_{3}^{3}
\end{array}
\right)  \,\, \Rightarrow \,\,  
\left(  A^{c}\right)  _{j}^{i}=\left(
\begin{array}
[c]{ccc}
\left(  A^{c}\right)  _{1}^{1} & \left(  A^{c}\right)  _{2}^{1} & \left(
A^{c}\right)  _{3}^{1}\\
\left(  A^{c}\right)  _{1}^{2} & \left(  A^{c}\right)  _{2}^{2} & \left(
A^{c}\right)  _{3}^{2}\\
\left(  A^{c}\right)  _{1}^{3} & \left(  A^{c}\right)  _{2}^{3} & \left(
A^{c}\right)  _{3}^{3}
\end{array}
\right)\,.
\]

Donde los $\left( A^{c}\right)_{j}^{i}$ forman la matriz de cofactores, estos cofactores son:
\begin{align*}
\left(  A^{c}\right)  _{1}^{1}  &  =\left(  -1\right)  ^{1+1}\left|
\begin{array}
[c]{cc}
a_{2}^{2} & a_{3}^{2}\\
a_{2}^{3} & a_{3}^{3}
\end{array}
\right| \,, \quad\left(  A^{c}\right)  _{2}^{1}=\left(  -1\right)
^{1+2}\left| 
\begin{array}
[c]{cc}
a_{1}^{2} & a_{3}^{2}\\
a_{1}^{3} & a_{3}^{3}
\end{array}
\right| \,, \quad\left(  A^{c}\right)  _{3}^{1}=\left(  -1\right)
^{1+3}\left|
\begin{array}
[c]{cc}
a_{1}^{2} & a_{2}^{2}\\
a_{1}^{3} & a_{2}^{3}
\end{array}
\right| \,, \\
& \\
\left(  A^{c}\right)  _{1}^{2}  &  =\left(  -1\right)  ^{2+1}\left|
\begin{array}
[c]{cc}
a_{2}^{1} & a_{3}^{1}\\
a_{2}^{3} & a_{3}^{3}
\end{array}
\right|  \,, \quad\left(  A^{c}\right)  _{2}^{2}=\left(  -1\right)
^{2+2}\left|
\begin{array}
[c]{cc}
a_{1}^{1} & a_{3}^{1}\\
a_{1}^{3} & a_{3}^{3}
\end{array}
\right| \,, \quad\left(  A^{c}\right)  _{3}^{2}=\left(  -1\right)
^{2+3}\left|
\begin{array}
[c]{cc}
a_{1}^{1} & a_{2}^{1}\\
a_{1}^{3} & a_{2}^{3}
\end{array}
\right|\,, \\
& \\
\left(  A^{c}\right)  _{1}^{3}  &  =\left(  -1\right)  ^{3+1}\left|
\begin{array}
[c]{cc}
a_{2}^{1} & a_{3}^{1}\\
a_{2}^{2} & a_{3}^{2}
\end{array}
\right| \,, \quad\left(  A^{c}\right)  _{2}^{3}=\left(  -1\right)
^{3+2}\left|
\begin{array}
[c]{cc}
a_{1}^{1} & a_{3}^{1}\\
a_{1}^{2} & a_{3}^{2}
\end{array}
\right| \,, \quad\left(  A^{c}\right)  _{3}^{3}=\left(  -1\right)
^{3+3}\left|
\begin{array}
[c]{cc}
a_{1}^{1} & a_{2}^{1}\\
a_{1}^{2} & a_{2}^{2}
\end{array}
\right|\,.
\end{align*}

\subsection{Matriz adjunta}
Llamaremos matriz adjunta, $\mbox{adj}\left[ \mathbb{A}\right]$, a la traspuesta de la matriz de cofactores de una determinada matriz:
\[
\mbox{adj}\left[   \mathbb{A}\right]  =\left(   \mathbb{A}^{c}\right)^{T}\,\, \Rightarrow \,\,\mbox{adj}\left[A_{j}^{i}\right]  =\left(\left(A^{c}\right)_{j}^{i}\right)  ^{T}=\left(A^{c}\right)_{i}^{j}\,.
\]
Por ejemplo: $
 \mathbb{A}=\left(
\begin{array}
[c]{rrr}
1 & 2 & 3\\
4 & 5 & 6\\
7 & 8 & 9
\end{array}
\right)  \,\, \Rightarrow \,\, \mbox{adj}\left[  \mathbb{A}\right]
=\left(
\begin{array}
[c]{rrr}
-3 & 6 & -3\\
6 & -12 & 6\\
-3 & 6 & -3
\end{array}
\right)\,.
$

Una matriz será autoadjunta si $\mbox{adj}\left[ \mathbb{A}\right]  = \mathbb{A}$. 

\subsection{Matriz singular}
$ \mathbb{A}$ es singular  si  $\det| \mathbb{A}|=0$. Si $\det|\mathbb{A}| \neq 0$, existirá un número $r$ máximo de vectores filas, o columnas, linealmente independientes, llamaremos a ese número, $0\leq r \leq n $, el rango de la matriz.

\subsection{Matriz inversa}
Hemos visto que dada una transformación lineal biyectiva, podemos definir una inversa para esa transformación lineal. Esa transformación lineal tendrá como representación un matriz. Por lo tanto, dado un operador lineal $\mathbb{A}$ diremos que otro operador lineal $\mathbb{B}$ será su inverso (por la derecha) si
\[
\mathbb{AB}=\mathbb{I}\rightarrow\left< \mathrm{e}^{i}\right|  \mathbb{AB}\left|
\mathrm{e}_{j}\right> =\delta_{j}^{i}\rightarrow A_{k}^{i}B_{j}
^{k}=\delta_{j}^{i}\,.
\]

Ahora bien, como conocemos la matriz $A_{k}^{i}$ y la suponemos no singular ($\det\left|A_{k}^{i}\right| \neq0$) al tomar un $j$ fijo tendremos un sistema de $n$ ecuaciones lineales inhomogéneo con $n$ incógnitas: $B_{j}^{1},B_{j}^{2},B_{j}^{3},\cdots B_{j}^{n}$.  
Al resolver el sistema tendremos la solución. 

Un procedimiento para encontrar la inversa es el método de eliminación de Gauss-Jordan.  Veamos como
funciona para una matriz $3\times3$:
\[
\left(  \left.
\begin{array}
[c]{ccc}
A_{1}^{1} & A_{2}^{1} & A_{3}^{1}\\
A_{1}^{2} & A_{2}^{2} & A_{3}^{2}\\
A_{1}^{3} & A_{2}^{3} & A_{3}^{3}
\end{array}
\right|
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right)  \overset{\text{Gauss-Jordan}}{\rightarrow}\left(  \left.
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right|
\begin{array}
[c]{ccc}
B_{1}^{1} & B_{2}^{1} & B_{3}^{1}\\
B_{1}^{2} & B_{2}^{2} & B_{3}^{2}\\
B_{1}^{3} & B_{2}^{3} & B_{3}^{3}
\end{array}
\right)\,.
\]
Algunas propiedades importantes:
\[
\left( \mathbb{A}^{-1}\right)^{-1}=  \mathbb{A}\,,\quad
\left( \mathbb{A}^{T}\right)^{-1} = \left( \mathbb{A}^{-1}\right)^{T}\,,\quad
\left( \mathbb{A}^{\dag}\right)^{-1} = \left( \mathbb{A}^{-1}\right)^{\dag}\,,\quad
\left( \mathbb{A} \mathbb{B}\right)^{-1}= \mathbb{B}^{-1} \mathbb{A}^{-1} \,.
\]

\subsection{Matriz ortogonal}
Anteriormente mencionamos, en la sección \ref{RotacionCoordenadas}, que en el espacio real $\mathds{R}^3$ podemos tener una transformación de coordenadas cartesianas: $(x,y,z) \rightarrow ({\tilde x}, {\tilde y}, {\tilde z})$ que consiste en rotar, un ángulo $\theta$, uno de los sistemas respecto al otro. Si la rotación se hace, digamos alrededor del eje $z$, la relación entre las coordenadas es la siguiente:
\[
\left\{
\begin{array}{rl}
{\tilde x} =& x\cos(\theta)+y\ \mbox{sen}(\theta) \\
{\tilde y} =& -x\ \mbox{sen}(\theta)+y\cos(\theta)  \\
{\tilde z} = & z 
\end{array}
\right. \,\, \Rightarrow \,\, 
\left\{
\begin{array}{rl}
{\tilde x}^1 =& x^1\cos(\theta)+x^2\ \mbox{sen}(\theta) \\
{\tilde x}^2 =& -x^1\ \mbox{sen}(\theta)+x^2\cos(\theta)  \\
{\tilde x}^3 = & x^3 
\end{array}
\right. \,\, \Rightarrow \,\, 
{\tilde x}^i= {\tilde \alpha}_j^i{x}^j \,.
\]

Si la longitud desde el origen a algún punto $P$ del espacio es la misma para ambos sistemas de coordenadas
\[
x^i x_i={\tilde x}^i{\tilde x}_i=  
({\tilde \alpha}_k^i{x}^k)({\tilde \alpha}_i^j{x}_j) =
x^k x_j  {\tilde \alpha}_i^j{\tilde \alpha}_k^i \,.
\]
entonces se tiene que cumplir que: 
 $ 
{\tilde \alpha}_i^j{\tilde \alpha}_k^i=\delta_k^j\,.
$

Este resultado es una consecuencia de haber impuesto la condición para que las longitudes sean las mismas en ambos sistemas de coordenadas y se conoce como la {\it condición de ortogonalidad}. 
En el lenguaje de la matrices reales la condición de ortogonalidad se puede enunciar de diferentes formas, todas ellas equivalentes:
$
 \mathbb{A}^T \mathbb{A}= \mathbb{A} \mathbb{A}^T= \mathbb{I}\,,\quad
 \mathbb{A}^T= \mathbb{A}^{-1}\,,\quad
\sum_i A_i^jA_k^i=\delta_k^j\,.
$ 
y diremos que la matrices que las satisfacen son matrices ortogonales.

\subsection{Matrices complejas}
Las matrices con todas sus componentes reales no permite estudiar problemas que pueden aparecer en algunas ramas de la Física, como en la Mecánica Cuántica. Por lo tanto, de necesita generalizar nuestro estudio a espacios vectoriales  a el de espacios vectoriales de matrices complejas:  $\mathds{C}_{n\times n}$, donde en las  componentes de las matrices aparecen números complejos. 
Cuando las matrices son complejas surgen generalizaciones que podemos considerar, como mostramos a continuación.

\begin{itemize}
\item {\bf La matriz adjunta  o matriz  transpuesta conjugada.}
Estas matrices se construyen tomado el complejo conjugado de cada elemento y luego la transpuesta de la matriz. Se acostumbra denotarlas con 
$
 \mathbb{A}^\dagger=( \mathbb{A}^*)^T=( \mathbb{A}^T)^* \,.
$

\item {\bf La matriz hermítica.}
Son aquellas matrices que satisfacen la siguiente ecuación:
$\mathbb{A}^\dagger =  \mathbb{A}\,.$ 
Notemos que si  $ \mathbb{A}$ es real, entonces  $ \mathbb{A}^\dagger =  \mathbb{A}^T$, y por lo tanto las matrices hermíticas reales son matrices simétricas.

\item {\bf La matriz normal.}
Se llama matriz normal a aquella que satisface
$
 \mathbb{A}^\dagger \mathbb{A}= \mathbb{A} \mathbb{A}^\dagger\,.
$

\item {\bf La matriz unitaria.}
La matriz $ \mathbb{A}$ es unitaria si:
$
 \mathbb{A}^\dagger= \mathbb{A}^{-1} \,.
$
 Si $ \mathbb{A}$ es real, entonces $ \mathbb{A}^{-1} =  \mathbb{A}^T$, de manera que las matrices unitarias reales son ortogonales. Es decir, las matrices unitarias son el análogo a las matrices ortogonales reales y son de fundamental importancia en la Mecánica Cuántica porque dejan la norma de los vectores invariantes.
\end{itemize}

Existe una relación importante entre matrices normales y matrices unitarias y es el hecho de que una matriz $ \mathbb{A}$ es normal,  si y sólo si, existe una matriz diagonal $\mathbb{D}$ y una unitaria $\mathbb{U}$ tal que: 
$ \mathbb{A}=\mathbb{U}\mathbb{D}\mathbb{U}^\dagger$. 

Más adelante veremos que los valores en la diagonal de $\mathbb{D}$ se denominan los  {\it autovalores} de $ \mathbb{A}$ y las columnas de $\mathbb{U}$ son los {\it autovectores} de $ \mathbb{A}$.

\subsection{{\color{Fuchsia}Ejemplos}}
\begin{enumerate}
\item  Queremos hallar la matriz inversa de
$
\mathbb{A}=
\left(
\begin{array}
[c]{rrr}
2 & 3 & 4\\
2 & 1 & 1\\
-1 & 1 & 2
\end{array}
\right)\,.
$

Utilizando el método de Gauss-Jordan.  Entonces, escribimos la siguiente matriz aumentada:
\[
\left(  \left.
\begin{array}
[c]{rrr}
2 & 3 & 4\\
2 & 1 & 1\\
-1 & 1 & 2
\end{array}
\right|
\begin{array}
[c]{rrr}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right) \,.
\]
Si multiplicamos la segunda fila por $-1$ y la sumamos con la primera fila obtenemos:
\[
\left(  \left.
\begin{array}
[c]{rrr}
2 & 3 & 4\\
2 & 1 & 1\\
-1 & 1 & 2
\end{array}
\right|
\begin{array}
[c]{rrr}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right)  \rightarrow\left(  \left.
\begin{array}
[c]{rrr}
2 & 3 & 4\\
0 & 2 & 3\\
-1 & 1 & 2
\end{array}
\right|
\begin{array}
[c]{rrr}
1 & 0 & 0\\
1 & -1 & 0\\
0 & 0 & 1
\end{array}
\right)  \,.
\]
Ahora multiplicamos la tercera fila por $2$ y sumamos con la primera fila
\begin{eqnarray*}
\left(  \left.
\begin{array}
[c]{rrr}
2 & 3 & 4\\
0 & 2 & 3\\
-1 & 1 & 2
\end{array}
\right|
\begin{array}
[c]{rrr}
1 & 0 & 0\\
1 & -1 & 0\\
0 & 0 & 1
\end{array}
\right) \rightarrow
\left(  \left.
\begin{array}
[c]{rrr}
2 & 3 & 4\\
0 & 2 & 3\\
0 & 5 & 8
\end{array}
\right|
\begin{array}
[c]{rrr}
1 & 0 & 0\\
1 & -1 & 0\\
1 & 0 & 2
\end{array}
\right) \,.
\end{eqnarray*}
Multiplicamos la primera fila por $-2$ y sumamos con la tercera
\[
\left(  \left.
\begin{array}
[c]{rrr}
2 & 3 & 4\\
0 & 2 & 3\\
0 & 5 & 8
\end{array}
\right|
\begin{array}
[c]{rrr}
1 & 0 & 0\\
1 & -1 & 0\\
1 & 0 & 2
\end{array}
\right)  \rightarrow
\left(  \left.
\begin{array}
[c]{ccc}
-4 & -1 & 0\\
0 & 2 & 3\\
0 & 5 & 8
\end{array}
\right|
\begin{array}
[c]{rrr}
-1 & 0& 2\\
1 & -1 & 0\\
1 & 0 & 2
\end{array}
\right)\,.
\]
Multiplicamos la segunda fila por $-8/3$ y sumamos con la tercera, luego multiplicamos esa segunda fila por $-3$
\[
\left(  \left.
\begin{array}
[c]{ccc}
-4 & -1 & 0\\
0 & 2 & 3\\
0 & 5 & 8
\end{array}
\right|
\begin{array}
[c]{rrr}
-1 & 0& 2\\
1 & 1 & 0\\
1 & 0 & 2
\end{array}
\right) \rightarrow
\left(  \left.
\begin{array}
[c]{ccc}
-4 & -1 & 0\\
0 & 1 & 0\\
0 & 5 & 8
\end{array}
\right|
\begin{array}
[c]{rrr}
-1 & 0& 2\\
5 & -8 & -6\\
1 & 0 & 2
\end{array}
\right)\,.
\]
Multiplicamos la tercera fila por $-1/5$ y sumamos con la segunda fila, luego multiplicamos esa tercera fila por $-5/8$ 
\[
\left(  \left.
\begin{array}
[c]{ccc}
-4 & -1 & 0\\
0 & 1 & 0\\
0 & 5 & 8
\end{array}
\right|
\begin{array}
[c]{rrr}
-1 & 0& 2\\
5 & -8 & -6\\
1 & 0 & 2
\end{array}
\right) \rightarrow
\left(  \left.
\begin{array}
[c]{ccc}
-4 & -1 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right|
\begin{array}
[c]{rrr}
-1 & 0& 2\\
5 & -8 & -6\\
-3 & 5 & 4
\end{array}
\right)\,.
\]
Se suma la primera fila con la segunda y luego se multiplica esta primera fila que resulta por $-1/4$.
\[
\left(  \left.
\begin{array}
[c]{ccc}
-4 & -1 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right|
\begin{array}
[c]{rrr}
-1 & 0& 2\\
5 & -8 & -6\\
-3 & 5 & 4
\end{array}
\right) \rightarrow
\left(  \left.
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right|
\begin{array}
[c]{rrr}
-1 & 2& 1\\
5 & -8 & -6\\
-3 & 5 & 4
\end{array}
\right)\,.
\]
Por lo tanto, la matriz inversa es: 
$
\mathbb{A}^{-1}=
\left(
\begin{array}
[c]{rrr}
-1 & 2& 1\\
5 & -8 & -6\\
-3 & 5 & 4
\end{array}
\right)\,.
$

\item 
También se puede obtener la matriz inversa de la siguiente manera: 
$\mathbb{A}^{-1}= \frac{\mathrm{adj}\left[   \mathbb{A}\right] }{\det\left|\mathbb{A}\right|} \,.$

Dada la siguiente  matriz:
$
\mathbb{A} = \left(
\begin{array}
[c]{rrr}
3 & -2 & 2\\
1 & 2 & -3 \\
4 & 1 & 2 
\end{array}
\right)\,.
$
Primero procedemos a calcular el determinante
$
\det\left|\mathbb{A}\right|= 
\left[
\begin{array}
[c]{rrr}
3 & -2 & 2\\
1 & 2 & -3 \\
4 & 1 & 2 
\end{array}
\right]= 35 \,.
$

Calculemos ahora la matriz cofactor:
\begin{align*}
\left(  A^{c}\right)  _{1}^{1}  &  =\left(-1\right)^{2}\left|
\begin{array}
[c]{cc}
2 & -3\\
1 & 2
\end{array}
\right|  = 7 \,,
\,\, \left(  A^{c}\right)  _{2}^{1}=\left(  -1\right)^{3}\left|
\begin{array}
[c]{cc}
1& -3\\
4 & 2
\end{array}
\right| = 14 \,,
\,\, \left(  A^{c}\right)  _{3}^{1}=\left(  -1\right)^{4}\left|
\begin{array}
[c]{cc}
1 & 2\\
4 & 1
\end{array}
\right|= -7\,,  \\
& \\
\left(  A^{c}\right)  _{1}^{2}  &  =\left(  -1\right)^{3}\left|
\begin{array}
[c]{cc}
-2 & 2\\
1 & 2
\end{array}
\right| =  6\,,
\,\, \left(  A^{c}\right)  _{2}^{2}=\left(  -1\right)^{4}\left|
\begin{array}
[c]{cc}
3 & 2\\
4 & 2
\end{array}
\right|  = -2 \,,
\,\, \left(  A^{c}\right)  _{3}^{2}=\left(  -1\right)^{5}\left|
\begin{array}
[c]{cc}
3 & -2\\
4 & 1
\end{array}
\right| = -11\,, \\
& \\
\left(  A^{c}\right)  _{1}^{3}  &  =\left(  -1\right)  ^{4}\left|
\begin{array}
[c]{cc}
-2& 2\\
2& -3
\end{array}
\right|  = 2 \,,
\,\, \left(  A^{c}\right)  _{2}^{3}=\left(  -1\right)^{5}\left|
\begin{array}
[c]{cc}
3& 2\\
1 & -3
\end{array}
\right|  = 11\,,
\,\, \left(  A^{c}\right)  _{3}^{3}=\left(  -1\right)^{6}\left|
\begin{array}
[c]{cc}
3 & -2\\
1 & 2
\end{array}
\right|= 8\,.
\end{align*}

Por lo tanto, la matriz cofactor y la adjunta son:
\[
\mathbb{A}^{c}= \left(
\begin{array}
[c]{rrr}
7 & 14 & -7\\
6 & -2 & -11 \\
2 & 11 & 8 
\end{array}
\right) \,\, \Rightarrow \,\,    
\mbox{adj}\left[   \mathbb{A}\right]= 
\left(
\begin{array}
[c]{rrr}
7 & 6 & 2\\
14 & -2 & 11 \\
-7 & -11 & 8 
\end{array}
\right)\,.
\]

Y ahora si, la matriz inversa:
\[
\mathbb{A}^{-1}=  \frac{1}{35}
 \left(
\begin{array}
[c]{rrr}
7 & 6 & 2\\
14 & -2 & 11 \\
-7 & -11 & 8 
\end{array}
\right)\,.
\]
\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}}
En este ejercicio vamos a introducir algunos comandos básicos que nos permitirán realizar  operaciones típicas con matrices. Dada la siguiente matriz:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A:matrix([1,2,3],[4,8,5],[9,5,4]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o1) }
\begin{pmatrix}1 & 2 & 3 \\ 4 & 8 & 5 \\ 9 & 5 & 4 \\ 
\end{pmatrix}
\end{math}
\newline

La transpuesta es:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
transpose(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o2) }
\begin{pmatrix}1 & 4 & 9 \\ 2 & 8 & 5 \\ 3 & 5 & 4 \\ 
\end{pmatrix}
\end{math}
\newline

El comando {\bf adjoint(A)} calcula el adjunto de la matriz $ \mathbb{A}$. La matriz adjunta es la transpuesta de la matriz de cofactores de 
$ \mathbb{A}$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
adjoint(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle 
\parbox{8ex}{\color{labelcolor}(\%o3) }
$$\begin{pmatrix}7 & 7 & -14 \\ 29 & -23 & 7 \\ -52 & 13 & 0 \\ 
 \end{pmatrix}$$
\end{math}
\newline

Para la matriz inversa:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
invert(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o4) }
\begin{pmatrix}-\frac{1}{13} & -\frac{1}{13} & \frac{2}{13} \\ -
 \frac{29}{91} & \frac{23}{91} & -\frac{1}{13} \\ \frac{4}{7} & -
 \frac{1}{7} & 0 \\ 
 \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
determinant(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o5) }
-91
\end{math}
\newline

Si se quiere, se puede calcular la inversa con el determinante afuera:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
invert(A),detout;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o6) }
-\frac{\begin{pmatrix}7 & 7 & -14 \\ 29 & -23 & 7 \\ -52 & 13 & 0
  \\ \end{pmatrix}}{91}
\end{math}
\newline

Podemos comprobar que $\mathbb{A}^{-1}\mathbb{A}=\mathbb{I}$

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
invert(A).A;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o7) }
\begin{pmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ \end{pmatrix}
\end{math}
\newline

Para calcular la menor $(i,j)$ de la matriz, es decir, eliminar la 
fila $i$ y la columna $j$ de una matriz se debe escribir:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
minor(A,2,2);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o8) }
\begin{pmatrix}1 & 3 \\ 9 & 4 \\ \end{pmatrix}
\end{math}
\newline

Para generar una matriz triangular superior a partir de una matriz dada se debe usar el siguiente comando: {\bf triangularize}(). 
El comando {\bf echelon}() es equivalente pero 
normaliza a 1 el primer elemento no nulo de cada fila utilizando el método de eliminación de Gauss-Jordan. 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
triangularize(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o9) }
\begin{pmatrix}1 & 2 & 3 \\ 0 & -13 & -23 \\ 0 & 0 & 91 \\ 
 \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i10) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
echelon(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o10) }
\begin{pmatrix}1 & 2 & 3 \\ 0 & 1 & \frac{23}{13} \\ 0 & 0 & 1 \\ 
 \end{pmatrix}
\end{math}
\newline

Con el paquete {\bf nchrpl} se puede calcular la traza de una matriz

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
load(nchrpl)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
mattrace(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o12) }
13
\end{math}
\newline

Consideremos ahora la matriz 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M:matrix ([1, %i, 0], [1, 2 ,-%i], [%i, 1, 2] );
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o13) }
\begin{pmatrix}1 & i & 0 \\ -1 & 2 & -i \\ i & 1 & 2 \\ 
\end{pmatrix}
\end{math}
\newline

Para el exponente de matrices tenemos la opción {\bf domxexpt} (true o false). Cuando es {\it true} el $\exp(M)$ se interpreta 
como la matriz cuyo elemento $i,j$ es igual a $\exp (M[i,j])$. 
En el otro caso, $\exp(M) $ se interpreta como $( x )^{M}$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
(1-x)^M;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o14) }
\begin{pmatrix}1-x & \left(1-x\right)^{i} & 1 \\ \frac{1}{1-x} & 
 \left(1-x\right)^2 & \frac{1}{\left(1-x\right)^{i}} \\ \left(1-x
 \right)^{i} & 1-x & \left(1-x\right)^2 \\ 
 \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
domxexpt:false$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
(1-x)^M;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o16) }
\left(1-x\right)^{\begin{pmatrix}1 & i & 0 \\ -1 & 2 & -i \\ i & 1
  & 2 \\ 
\end{pmatrix}}
\end{math}
\newline

Y en cuanto al cálculo del rango de una matriz

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
rank(M);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle
\parbox{8ex}{\color{labelcolor}(\%o17) }
3
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18) \end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
kill(all)$
\end{verbatim}}
\end{minipage} 

\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}

\subsection{{\color{OliveGreen}Ejercicios}}

\begin{enumerate}

\item Considere las matrices:
\[
 \mathbb{A}=
\left(
\begin{array}{ccc}
0 & -i & i \\
i & 0 & -i \\
-i & i & 0
\end{array}\right)\,, \quad \quad
 \mathbb{B}= \frac{1}{\sqrt{8}} 
\left(
\begin{array}{ccc}
\sqrt{3} & -\sqrt{2} & -\sqrt{3} \\
1 & \sqrt{6} & -1 \\
2 & 0 & 2
\end{array}\right)\,.
\]
Diga si son: (a) reales, (b) diagonales, (c) simétricas, (d)  antisimétricas, (e) singulares, (f) ortogonales, (g) hermíticas, (h) antihermíticas,  (i) unitarias o  (j) normales. 

\item Dada una matriz hermítica
$\mathbb{H} = \left(\begin{array}{cc}10 & 3i \\-3i & 0\end{array}\right)$,  
construya la matriz unitaria $ \mathbb{U}$, tal que $ \mathbb{U}^{\dag} \mathbb{H} \mathbb{U} =  \mathbb{D} $ donde $ \mathbb{D}$ es una matriz real y diagonal.

\item Utilizando el método de eliminación de Gauss-Jordan encuentre la inversa de la matriz: 
\[
 \mathbb{A}=
\left(\begin{array}{ccc}
1 & 2 & 3 \\
4 & 8 & 5 \\
9 & 5 & 4
\end{array}
\right)
\]

\item Encuentre la matriz inversa de las siguientes matrices:
\[
 \mathbb{A}=
\left(\begin{array}{ccc}
2 & 4 & 3 \\
1 & -2 & -2 \\
-3 & 3 & 2
\end{array}
\right)\,, \quad
 \mathbb{B}=
\left(\begin{array}{ccc}
1 & 1 & 0 \\
1 & 1 & 1 \\
0 & 1 & 1
\end{array}
\right)\,, \quad
 \mathbb{C}=
\left(\begin{array}{cccc}
1/2 & 1/2 & 1/2 & 1/2 \\
1/2 & 1/2 & -1/2 & -1/2  \\
1/2 & -1/2 & 1/2 & -1/2  \\
1/2 & -1/2 & -1/2 & 1/2  
\end{array}
\right)\,.
\]
Además, calcule: $ \mathbb{B}^T( \mathbb{A})^{-1} \mathbb{B}$ y $(2 \mathbb{A}+( \mathbb{B})^{-1} \mathbb{B}^T) \mathbb{A}^T$.

 \item Considere matrices ortogonales, reales $3 \times 3$  las cuales, adicionalmente cumplen con: $\det|\mathbb{M}| = 1$.
 
\begin{enumerate}
\item ¿Cuántos parámetros reales son necesarios para caracterizar unívocamente a este tipo de matrices?
  \item ¿Este tipo de matrices forman grupo bajo la multiplicación de matrices?
  \item Si ahora considera la matrices ortogonales reales con $\det|\mathbb{M}| = -1$ ¿Este tipo de matrices formarán grupo bajo la multiplicación? Justifique su respuesta.
\end{enumerate}

\item Si $ \mathbb{A}$ y $ \mathbb{B}$ son dos matrices hermíticas que no conmutan:
$\mathbb{A} \mathbb{B}-  \mathbb{B} \mathbb{A}= i \mathbb{C}\,,$ 
entonces pruebe que $ \mathbb{C}$ es hermítica.

\item  Si $ \mathbb{A}=\exp{(i\alpha \mathbb{B})}\,, \, \alpha \in \mathds{R}$.  
Demuestre que si $ \mathbb{B}$ es hermítica entonces $ \mathbb{A}$ es unitaria. 

\item Pruebe que el producto directo de dos matrices unitarias resulta en una matriz unitaria.

\item  Resuelva los problemas anteriores utilizando {\bf Maxima}.

\end{enumerate}

\section{Sistemas de ecuaciones lineales}
\label{SistemasEcuacionesLineales}
\index{Sistemas de ecuaciones lineales}
\index{Ecuaciones Lineales!Sistema de}
Una de las aplicaciones más útiles del álgebra de matrices es la
resolución de sistemas de ecuaciones lineales. 
\begin{eqnarray*}
A_{1}^1x^1+A_{2}^1x^2+ \cdots A_{n}^1 x^n &=& c^1 \\
A_{1}^2x^1+A_{2}^2x^2+ \cdots A_{n}^2 x^n &=& c^2 \\
&\vdots& \\
A_{1}^mx^1+A_{2}^mx^2+ \cdots A_{n}^m x^n &=& c^m   \,,   
\end{eqnarray*}

Este sistema puede ser expresado de una forma más compacta: 
\[
A_{i}^{\alpha}x^{i}=c^{\alpha}\quad\text{con }\,\, i=1,2,..,n\quad\text{y } \,\, \alpha=1,2,..,m \,,
\]
por lo tanto, tendremos $m$ ecuaciones lineales para $n$ incógnitas $\left(x^{1},x^{2},\cdots x^{n}\right)$. Las cantidades $A_{i}^{\alpha}$ resultan ser las componentes de la matriz de los coeficientes. 

Si todos los $c^i$ son cero el sistema se denomina homogéneo, en caso contrario inhomogéneo. Puede resultar que el conjunto de las incógnitas $\{x^i\}$ represente una solución, infinitas soluciones o que simplemente no exista una solución para el sistema.

Este problema puede ser pensado como un problema de un operador $\mathbb{A}$ en el espacio vectorial de transformaciones lineales $\mathcal{L}\left( \mathbf{V,W}\right)$,  donde $\dim\left(  \textbf{\em V}\right)=n$ y $\dim\left(  \textbf{\em W}\right)  =m$, con las $c^{\alpha}$ las componentes del vector transformado:
\[
\left|  {c}\right> =\mathbb{A}\left|  {x}\right>
\rightarrow c^{\alpha}=A_{i}^{\alpha}x^{i} \,.
\]

El operador $\mathbb{A}$ aplicará todo vector de $\textbf{\em V}$ en algún subespacio (o todo el espacio) de $\textbf{\em W}$. A este subespacio se le denomina el rango o recorrido de $\mathbb{A}$ y su dimensión es igual al rango de la matriz $ \mathbb{A}$. 
Si $\mathbb{A}$ es no singular, entonces existe algún subespacio de $\textbf{\em V}$ que es aplicado al vector cero de $\textbf{\em W}$, es decir, se cumple que $\mathbb{A}\left| x'\right>=\left| 0\right>$, donde al conjunto de vectores $\{\left| x'\right>\}$ se le llama el espacio nulo de $ \mathbb{A}$.

A continuación, mostraremos algunos métodos para la resolución de sistemas de ecuaciones lineales. 

\subsection{Eliminación de Gauss-Jordan}

Uno de los métodos más utilizados es el de la eliminación de \textit{Gauss-Jordan}, el cual se basa en el intercambio de ecuaciones y la multiplicación apropiada e inteligente por constantes y resta de ecuaciones. La idea es construir una matriz triangular superior para poder luego despejar desde abajo. 

Veamos como se aplica el método para resolver el sistema de ecuaciones. Primeramente escribimos el siguiente arreglo:

\[
\begin{array}
[c]{c}
a\\
b\\
c
\end{array}
\left[  \left.
\begin{array}
[c]{ccc}
2 & 3 & -1\\
4 & 4 & -3\\
-2 & 3 & -1
\end{array}
\right|
\begin{array}
[c]{c}
5\\
3\\
1
\end{array}
\right] \,,
\]
entonces para eliminar $x$ de la fila $c$ (o la ecuación $c$) sumamos la fila $a$ con la $c,a+c$ y esta nueva ecuación será la nueva $c$
\[
\begin{array}
[c]{c}
a\\
b\\
c^{\prime}
\end{array}
\left[  \left.
\begin{array}
[c]{ccc}
2 & 3 & -1\\
4 & 4 & -3\\
0 & 6 & -2
\end{array}
\right|
\begin{array}
[c]{c}
5\\
3\\
6
\end{array}
\right]\,,
\]
ahora $-2a+b$ será la nueva $b$
\[
\begin{array}
[c]{c}
a\\
b^{\prime}\\
c^{\prime}
\end{array}
\left[  \left.
\begin{array}
[c]{ccc}
2 & 3 & -1\\
0 & -2 & -1\\
0 & 6 & -2
\end{array}
\right|
\begin{array}
[c]{c}
5\\
-7\\
6
\end{array}
\right] \,,
\]
finalmente 3$b^{\prime}+c^{\prime}$
\[
\begin{array}
[c]{c}
a\\
b^{\prime}\\
c^{\prime\prime}
\end{array}
\left[  \left.
\begin{array}
[c]{ccc}
2 & 3 & -1\\
0 & -2 & -1\\
0 & 0 & -5
\end{array}
\right|
\begin{array}
[c]{c}
5\\
-7\\
-15
\end{array}
\right] \,.
\]

Este sistema es equivalente al primer sistema de ecuaciones,  el lector puede verificar que poseen el mismo determinante.  Po lo tanto, la solución emerge rápidamente:
\[
-5z=-15\rightarrow z=3 \,, 
\quad -2y - z=-7\rightarrow-2y-3=-7\rightarrow y=2 \,,
\quad 2x+3\left(  2\right)  -3=5\rightarrow x=1 \,.
\]

Es bueno recalcar que los sistemas de ecuaciones lineales no necesariamente tienen solución y a veces tienen más de una solución.

\subsection{El método de la matriz inversa}

El análisis matricial  nos puede ayudar a investigar sobre las posibles soluciones de un sistema de ecuaciones lineales, como vimos, en notación de matrices el sistema se puede escribir de la manera siguiente:
\[
\left(\begin{array}{cccc}
A_{1}^1 & A_{2}^1 & \cdots & A_{n}^1 \\
A_{1}^2 & A_{2}^2 & \cdots & A_{n}^2 \\ 
\vdots & \vdots &   &   \\
A_{1}^m & A_{2}^m & \cdots & A_{n}^m
\end{array}\right)
\left(\begin{array}{c}
x^{1}  \\
x^{2}  \\ 
\vdots   \\
x^{n} 
\end{array}\right) = 
\left(\begin{array}{c}
c^{1}  \\
c^{2}  \\ 
\vdots   \\
c^{m} 
\end{array}\right) \,\, \Rightarrow \,\,   \mathbb{A} \mathbb{X}= \mathbb{C}  \,.
\]

Podemos considerar a cada columna de la matriz $ \mathbb{A}$ como un vector, entonces el {\it rango} $r$ de la matriz $ \mathbb{A}$ será igual al número de vectores linealmente independientes, este número es también la dimensión del espacio vectorial que expanden estos vectores. 

Con respecto a las posibles soluciones del sistema podemos decir:
\begin{enumerate}
\item Si $ \mathbb{C}$ pertenece al rango de ${\mathbf A}$ y además
$r=n$, entonces todos los vectores del conjunto 
son linealmente independientes y el sistema tendrá como única 
solución al conjunto $\{x^1, x^2, \dots x^n\}$.

\item Si $ \mathbb{C}$ pertenece al rango de ${\mathbf A}$ y además
$r<n$, entonces únicamente $r$ vectores serán linealmente independientes. Esto significa que podremos escoger los coeficientes de los $n-r$ vectores de manera arbitraria sin dejar de satisfacer el sistema. Por tanto, existirá un número infinito de soluciones, que expanden un espacio vectorial de dimensión 
$n-r$.  

\item La otra posibilidad es que el sistema no tenga solución, en este caso $ \mathbb{C}$ no pertenece al rango de ${\mathbb A}$. 
\end{enumerate}

Cuando el sistema es homogéneo, $ \mathbb{C}=\mathbb{O}$, claramente existe una solución que viene a ser la trivial: $\{x^1= x^2 = \dots = x^n=0\}$, además, si $r = n$ ésta será la única solución.  Es bueno anotar que si hay menos ecuaciones que incógnitas ($m<n$) entonces automáticamente  $r<n$, por lo tanto un conjunto de ecuaciones lineales homogéneas con menos ecuaciones que incógnitas siempre tiene una infinidad de soluciones.

Debemos considerar el importante caso cuando $m=n$, es decir, la matriz $ \mathbb{A}$ es cuadrada y existe igual número de ecuaciones como de incógnitas. Al ser cuadrada la matriz se tiene que la condición 
$r=n$ implica que la matriz es no singular ($\det| \mathbb{A}| \neq0$).   El caso $r<n$ se corresponde a que $\det| \mathbb{A}|=0$. 

Para resolver el sistema de ecuaciones consideremos lo siguiente: dada la matriz $ \mathbb{A}$, $n\times n$  no singular, entonces $ \mathbb{A}$ tiene una matriz inversa, tal que:
\[
 \mathbb{A} \mathbb{X}= \mathbb{C} \,\, \Rightarrow \,\,  \mathbb{X}= \mathbb{A}^{-1} \mathbb{C} \,.
\]


\subsection{Factorización LU } 
El método de la matriz inversa es relativamente simple, pero laborioso  cuando se trata de resolver sistemas de ecuaciones muy grandes. La factorización o descomposición LU (Lower-Upper) es una técnica, existen varias de este tipo, para factorizar una matriz como el producto de una matriz triangular inferior y una matriz triangular superior. 
Si  $ \mathbb{A}$ es un matriz cuadrada no singular, entonces: 
\[
 \mathbb{A} \mathbb{X}= \mathbb{C} \,\, \Rightarrow \,\,  \mathbb{A}= \mathbb{L} \mathbb{U}\,,
\]
las matrices $ \mathbb{L}$ y $ \mathbb{U}$ serán únicas. 

Este método es básicamente una versión de el método de Gauss-Jordan, pero es más eficiente a la hora de ser implementado en algoritmos computacionales tanto algebraicos como numéricos. 

Para matrices $3\times 3$ el método se implementa de la siguiente manera. 
\[
 \mathbb{A}=
\left(\begin{array}{ccc}
1 & 0 & 0 \\
L_{1}^2 & 1 & 0 \\
L_{1}^3 & L_{2}^3 & 1
\end{array}
\right)
\left(\begin{array}{ccc}
U_{1}^1 & U_{2}^1 & U_{3}^1 \\
0 & U_{2}^2  & U_{3}^2  \\
0 & 0 & U_{3}^3 
\end{array}
\right)=
\left(\begin{array}{ccc}
U_{1}^1 & U_{2}^1 & U_{3}^1 \\
L_{1}^2U_{1}^1 & L_{1}^2U_{2}^1+U_{2}^2 & L_{1}^2U_{3}^1+U_{3}^2\\
L_{1}^3U_{1}^1 & L_{1}^3U_{2}^1+L_{2}^3U_{2}^2 & 
L_{1}^3U_{3}^1+L_{2}^3 U_{3}^2+U_{3}^3
\end{array}
\right)\,,
\]
las nueve incógnitas se obtienen igualando con las componentes conocidas de la matriz $ \mathbb{A}$. Con $ \mathbb{L}$ y $ \mathbb{U}$ determinadas tenemos entonces que
\[
 \mathbb{L} \mathbb{U} \mathbb{X}= \mathbb{C}\,,
\]
por lo tanto, se debe realizar los cálculos en dos partes:
\[
\mbox{primero} \quad   \mathbb{L} \mathbb{Y}= \mathbb{C} \quad 
\mbox{y luego} \quad  \mathbb{U} \mathbb{X}= \mathbb{Y} \,.
\]

 
Con la matrices $ \mathbb{L}$ y $ \mathbb{U}$ se puede calcular el determinante de la matriz $ \mathbb{A}$ de una manera más directa, es fácil ver que:
\[
\det| \mathbb{A}|=\det| \mathbb{L}| \det| \mathbb{U}|=\det| \mathbb{U}| = 
\prod_{i=1}^n U_{i}^i\,.
\]


\subsection{Método de Cramer} 
Un método alternativo para resolver sistemas de ecuaciones lineales es la conocida {\it regla de Cramer}. El funcionamiento del método lo podemos ilustrar con un sistema de tres ecuaciones y tres incógnitas:
\begin{eqnarray*}
A_{1}^1x^1+A_{2}^1x^2+  A_{3}^1 x^3 &=& c^1 \\
A_{1}^2x^1+A_{2}^2x^2+ A_{3}^2 x^3 &=& c^2 \\
A_{1}^3x^1+A_{2}^3x^2+  A_{3}^3 x^3 &=& c^3   \,,   
\end{eqnarray*}

Como mencionamos anteriormente, el determinate de una matriz, $\det|\mathbb{A}|=|\mathbb{A}|$, 
\[
| \mathbb{A}|=
\left|\begin{array}{ccc}
A_{1}^1 & A_{2}^1 & A_{3}^1 \\
A_{1}^2 & A_{2}^2 & A_{3}^2 \\
A_{1}^3 & A_{2}^3 & A_{3}^3
\end{array}
\right| \,,
\]
no cambia bajo la operación, por ejemplo, de sumarle a la primera columna las cantidades:
\[
| \mathbb{A}|=
\left|\begin{array}{ccc}
A_{1}^1 & A_{2}^1 & A_{3}^1 \\
A_{1}^2 & A_{2}^2 & A_{3}^2 \\
A_{1}^3 & A_{2}^3 & A_{3}^3
\end{array}
\right| = 
\left|\begin{array}{ccc}
A_{1}^1 + (x^2/x^1)A_{2}^1 + (x^3/x^1)A_{3}^1 & A_{2}^1 & A_{3}^1 \\
A_{1}^2 + (x^2/x^1)A_{2}^2 + (x^3/x^1)A_{3}^2 & A_{2}^2 & A_{3}^2 \\
A_{1}^3 + (x^2/x^1)A_{2}^3 + (x^3/x^1)A_{3}^3 & A_{2}^3 & A_{3}^3
\end{array}
\right| 
\] 
lo cual es igual a:
\[
| \mathbb{A}|=
\frac{1}{x^1}\left|\begin{array}{ccc}
c^{1} & A_{2}^1 & A_{3}^1 \\
c^{2} & A_{2}^2 & A_{3}^2 \\
c^{3} & A_{2}^3 & A_{3}^3
\end{array}
\right| \equiv \frac{1}{x^1}|\Delta_1|\,.
\] 

Se puede hacer lo mismo con las otras dos columnas para obtener:
\[
x^1=\frac{|\Delta_1|}{| \mathbb{A}|}\,,\quad x^2=\frac{|\Delta_2|}{| \mathbb{A}|}\,,\quad x^3=\frac{|\Delta_3|}{| \mathbb{A}|}\,.
\]

Donde los determinantes, $|\Delta_n|$, son los determinantes de Cramer:
\[
|\Delta_1|= 
\left|\begin{array}{ccc}
c^{1} & A_{2}^1 & A_{3}^1 \\
c^{2} & A_{2}^2 & A_{3}^2 \\
c^{3} & A_{2}^3 & A_{3}^3
\end{array}
\right|\,, \quad
|\Delta_2|= 
\left|\begin{array}{ccc}
A_{1}^1 & c^{1} & A_{3}^1 \\
A_{1}^2 & c^{2} & A_{3}^2 \\
A_{1}^3 & c^{3} & A_{3}^3
\end{array}
\right|\,, \quad
|\Delta_3|= 
\left|\begin{array}{ccc}
A_{1}^1 & A_{2}^1 &  c^{1} \\
A_{1}^2 & A_{2}^2 &  c^{2} \\
A_{1}^3 & A_{2}^3 &  c^{3}
\end{array}
\right|\,.
\]
Se puede demostrar que si $| \mathbb{A}|\neq 0$, entonces la solución así obtenida es única. 

\subsection{{\color{Fuchsia}Ejemplos}}

\begin{enumerate}

\item Resolver el sistema:
\[
2x^1 +4x^2 +3x^3 =4\,, \,\,
x^1 - 2x^2 - 2x^3 = 0\,, \,\,
-3x^1 +3x^2 +2x^3 =-7\,.
\]
Utilizando el método de la matriz inversa. 

Este sistema de ecuaciones se puede escribir como:
\[
\mathbb{A} \mathbb{X}= \mathbb{C} \,\, \Rightarrow \,\, 
\left(\begin{array}{cccc}
2 & 4 & 3 \\
1 & -2& -2 \\ 
-3 & 3 & 2
\end{array}\right)
\left(\begin{array}{c}
x^{1}  \\
x^{2}  \\ 
x^{3} 
\end{array}\right) = 
\left(\begin{array}{c}
4  \\
0 \\ 
-7
\end{array}\right) \,,
\]
donde $\mathbb{A}$ es la matriz de los coeficientes.

Luego de calcular la matriz inversa de $\mathbb{A}$  entonces resulta que 
\[
\mathbb{X}= \mathbb{A}^{-1} \mathbb{C}  \,\, \Rightarrow \,\,
\left(\begin{array}{c}
x^{1}  \\
x^{2}  \\ 
x^{3} 
\end{array}\right) = \frac{1}{11}
\left(\begin{array}{cccc}
2 & 1 & -2 \\
4 & 13& 7 \\ 
-3 & -18 & -8
\end{array}\right)
\left(\begin{array}{c}
4  \\
0 \\ 
-7
\end{array}\right)= 
\left(\begin{array}{c}
2  \\
-3 \\ 
4
\end{array}\right)\,.
\]

Existe entonces  una única solución al sistema, que es: 
$\{x^1= 2, x^2 =-3, x^3=4\}$. 

\item Resolver el mismo sistema de ecuaciones anteriormente estudiado:
\[
2x^1 +4x^2 +3x^3 =4\,, \,\,
x^1 - 2x^2 - 2x^3 = 0\,, \,\,
-3x^1 +3x^2 +2x^3 =-7\,.
\]
Pero utilizando el método de factorización $ \mathbb{L}\mathbb{U}$.

La matriz de los coeficientes es:
\[
 \mathbb{A}=
\left(\begin{array}{ccc}
2 & 4 & 3 \\
1 & -2 & -2 \\
-3 & 3 & 2
\end{array}
\right)
\]
\begin{itemize}
\item Cálculo de $ \mathbb{L}$ y $ \mathbb{U}$
\[
\left(\begin{array}{ccc}
2 & 4 & 3 \\
1 & -2 & -2 \\
-3 & 3 & 2
\end{array}
\right)=
\left(\begin{array}{ccc}
U_{1}^1 & U_{2}^1 & U_{3}^1 \\
L_{1}^2U_{1}^1 & L_{1}^2U_{2}^1+U_{2}^2 & L_{1}^2U_{3}^1+U_{3}^2\\
L_{1}^3U_{1}^1 & L_{1}^3U_{2}^1+L_{2}^3U_{2}^2 & 
L_{1}^3U_{3}^1+L_{2}^3 U_{3}^2+U_{3}^3
\end{array}
\right)
\]
Con los valores de inicio: $U_{1}^1=2,\ U_{2}^1=4, \ U_{3}^1=3$ se procede a resolver  las ecuaciones:

\[
\begin{array}{lcl}
L_{1}^2U_{1}^1=1 & & L_{1}^3U_{1}^1=-3  \\
L_{1}^2U_{2}^1+U_{2}^2=-2 & & L_{1}^2U_{3}^1+U_{3}^2=-2  \\
L_{1}^3U_{2}^1+L_{2}^3U_{2}^2=3 & & 
L_{1}^3U_{3}^1+L_{2}^3 U_{3}^2+U_{3}^3=2  \\
\end{array}
\]
por lo tanto
\[
 \mathbb{A}= \mathbb{L} \mathbb{U}=
\left(\begin{array}{ccc}
1 & 0 & 0 \\
1/2 & 1 & 0 \\
-3/2 & -9/4 & 1
\end{array}
\right)
\left(\begin{array}{ccc}
2 & 4 & 3 \\
0 & -4  & -7/2  \\
0 & 0 &-{11}/{8} 
\end{array}
\right)
\]
\item Resolvemos primero $ \mathbb{L} \mathbb{Y}= \mathbb{C}$, lo cual es bastante fácil
\[
\left(\begin{array}{ccc}
1 & 0 & 0 \\
1/2 & 1 & 0 \\
-3/2 & -9/4 & 1
\end{array}
\right)
\left(\begin{array}{ccc}
y^1 \\
y^2 \\
y^3
\end{array}
\right)=
\left(\begin{array}{ccc}
4 \\
0 \\
-7
\end{array}
\right) \,\, \Rightarrow \,\,  
\left\{
\begin{array}{l}
y^1=4 \\
y^2=-2 \\
y^3=-\frac{11}{2}
\end{array}
\right.
\]

\item Con este resultado resolvemos: $ \mathbb{U} \mathbb{X}= \mathbb{Y}$, y obtenemos la solución
\[
\left(\begin{array}{ccc}
2 & 4 & 3 \\
0 & -4  & -7/2  \\
0 & 0 &-{11}/{8} 
\end{array}
\right)
\left(\begin{array}{ccc}
x^1 \\
x^2 \\
x^3
\end{array}
\right)=
\left(\begin{array}{ccc}
4 \\
-2 \\
-11/2
\end{array}
\right)\,\, \Rightarrow \,\,  
\left\{
\begin{array}{l}
x^1=2 \\
x^2=-3 \\
x^3=4
\end{array}
\right.
\]

\end{itemize}


\item Resolver, nuevamente, el sistema de ecuaciones:
\[
2x^1 +4x^2 +3x^3 =4\,, \,\,
x^1 - 2x^2 - 2x^3 = 0\,, \,\,
-3x^1 +3x^2 +2x^3 =-7\,.
\]
Utilizando ahora los determinantes de Cramer.

Esto es:
\[
 \mathbb{A}= \mathbb{X} \mathbb{C}\,\, \Rightarrow \,\,  
\left(\begin{array}{ccc}
2  &  4 & 3 \\
1  & -2 & -2  \\
-3 &  3 & 2 
\end{array}
\right)
\left(\begin{array}{ccc}
x^1 \\
x^2 \\
x^3
\end{array}
\right)=
\left(\begin{array}{ccc}
4 \\
0\\
-7
\end{array}
\right) \,\, \Rightarrow \,\,  | \mathbb{A}|=11\,.
\]

Los diferentes determinantes de Cramer son:
\[
|\Delta_1|= 
\left|\begin{array}{ccc}
4 & 4 & 3 \\
0 & -2 & -2 \\
-7 & 3 & 2
\end{array}
\right|=22 \,, \quad
|\Delta_2|= 
\left|\begin{array}{ccc}
2 & 4 & 3 \\
1 & 0 & -2 \\
-3 & -7 & 2
\end{array}
\right|= -33 \,, \quad
|\Delta_3|= 
\left|\begin{array}{ccc}
2 & 4 &  4 \\
1 & -2 & 0 \\
-3 & 3 &  -7
\end{array}
\right|= 44\,.
\]
de manera que
\[
x^1=\frac{22}{11}= 2 \,,\quad x^2=-\frac{33}{11}=-3 \,,\quad 
x^3=\frac{44}{11}= 4\,.
\]

\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}}

\begin{enumerate}
\item Resolveremos el siguiente sistema se ecuaciones lineales por el método de Gauss-Jordan
\begin{eqnarray*}
2 x^1+4 x^2+ 3 x^3 &=& 4 \\
 x^1-2 x^2-2 x^3 &=& 0 \\
-3 x^1+3 x^2+ 2 x^3  &=& -7      
\end{eqnarray*}

Introducimos el sistema de ecuaciones:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu1:2*x1+4*x2+3*x3=4; ecu2:x1-2*x2-2*x3=0; ecu3:-3*x1+3*x2+2*x3=-7;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
3{\tt x_3}+4\,{\tt x_2}+2\,{\tt x_1}=4
 \end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
-2\,{\tt x_3}-2\,{\tt x_2}+{\tt x_1}=0
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
2{\tt x_3}+3\,{\tt x_2}-3\,{\tt x_1}=-7
 \end{math}
\newline

Construimos la matriz de coeficientes y la denominaremos $A$  

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A:coefmatrix([ecu1,ecu2,ecu3],[x1,x2,x3]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\begin{pmatrix}2 & 4 & 3 \\ 1 & -2 & -2 \\ -3 & 3 & 2 \\ 
 \end{pmatrix}
 \end{math}
\newline

Le agregamos la columna con los elementos independientes

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M:addcol(A,[4,0,-7]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\begin{pmatrix}2 & 4 & -3 & 4 \\ 1 & -2 & -2 & 0 \\ -3 & 3 & 2 & -7  \\ \end{pmatrix}
 \end{math}
\newline

Reducimos la matriz $M$ utilizando el algoritmo de Gauss-Jordan.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
echelon(M);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\begin{pmatrix}
1 & -2 & -2 & 0 \\ 
0 & 1 & \frac{7}{8} & \frac{1}{2} \\ 
0 & 0 & 1 & 4 \\ 
 \end{pmatrix}
 \end{math}
 \newline
 
De la última fila es fácil determinar que ${\tt x_3}=4$. Los otros dos valores que resultan son ${\tt x_2}=-3$ y  ${\tt x_1}=2$. 

Por supuesto podemos recurrir al cálculo directo:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i7) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([ecu1,ecu2,ecu3],[x1,x2,x3]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\left[ {\tt x_1}=2 , {\tt x_2}=-3 , {\tt x_3}=4 \right] 
\end{math}

\item Resolvamos nuevamente el mismo sistema 
\begin{eqnarray*}
2 x^1+4 x^2+ 3 x^3 &=& 4 \\
 x^1-2 x^2-2 x^3 &=& 0 \\
-3 x^1+3 x^2+ 2 x^3  &=& -7      
\end{eqnarray*}
pero ahora por el método de la matriz inversa.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i8) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu1:2*x1+4*x2+3*x3=4; ecu2:x1-2*x2-2*x3=0; ecu3:-3*x1+3*x2+2*x3=-7;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
3\,{\tt x_3}+4\,{\tt x_2}+2\,{\tt x_1}=4
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
-2\,{\tt x_3}-2\,{\tt x_2}+{\tt x_1}=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
2\,{\tt x_3}+3\,{\tt x_2}-3\,{\tt x_1}=-7
\end{math}
\newline

La matriz de cofactores:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i11) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A:coefmatrix([ecu1,ecu2,ecu3],[x1,x2,x3]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\begin{pmatrix}2 & 4 & 3 \\ 1 & -2 & -2 \\ -3 & 3 & 2 \\ 
\end{pmatrix}
\end{math}
\newline

La matriz inversa de $A$ la denominaremos $Ainv$, y la calcularemos con el determinante afuera:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Ainv:invert(A),detout;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
\frac{\begin{pmatrix}2 & 1 & -2 \\ 4 & 13 & 7 \\ -3 & -18 & -8 \\ 
 \end{pmatrix}}{11}
\end{math}
\newline

La matriz con los términos inhomogéneos lo llamaremos $C$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
C:matrix([4,0,-7]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\begin{pmatrix}4 & 0 & -7 \\ 
\end{pmatrix}
\end{math}
\newline

Para que finalmente podamos hacer la siguiente multiplicación de matrices:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
X:Ainv.C;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
\begin{pmatrix}2 \\ -3 \\ 4 \\ \end{pmatrix}
\end{math}
\newline

Por lo tanto: ${\tt x_1}=2$, ${\tt x_2}=-3$, ${\tt x_3}=4$.


\item Vamos ahora a resolver el mismo sistema pero usando la técnica de factorización $ \mathbb{L} \mathbb{U}$. 

Le podemos pedir al programa que factorize la matriz $A$ por medio de la función {\bf lu factor(A)}, consultar el manual de {\bf Maxima} para más detalles.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
U:lu_factor(A)$
\end{verbatim}}
\end{minipage}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
F:get_lu_factors(U);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
\left[ \begin{pmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 
 \end{pmatrix} , \begin{pmatrix}1 & 0 & 0 \\ \frac{1}{2} & 1 & 0 \\ -
 \frac{3}{2} & -\frac{9}{4} & 1 \\ \end{pmatrix} , \begin{pmatrix}2
  & 4 & 3 \\ 0 & -4 & -\frac{7}{2} \\ 0 & 0 & -\frac{11}{8} \\ 
 \end{pmatrix} \right]
\end{math}
\newline

Por lo tanto, aquí tendremos que: $\mathbb{L}=F[2]$ y $\mathbb{U}=F[3]$. 

Podemos resolver entonces la primera parte, $\mathbb{L} \mathbb{Y}= \mathbb{C}$: 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Y:invert(F[2]).C;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
\begin{pmatrix}4 \\ -2 \\ -\frac{11}{2} \\ \end{pmatrix}
\end{math}
\newline

Y ahora $ \mathbb{U} \mathbb{X}= \mathbb{Y}$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
X:invert(F[3]).Y;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o19) }
\begin{pmatrix}2 \\ -3 \\ 4 \\ \end{pmatrix}
\end{math}
\newline

Por lo que resulta que: ${\tt x_1}=2$, ${\tt x_2}=-3$, ${\tt x_3}=4$.

Notemos que:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i20) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
determinant(A)=determinant(F[2])*determinant(F[3]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o20) }
11=11
\end{math}

\item Utilicemos ahora los determinantes de Cramer. Para tal fin, construiremos las matrices eliminando las columnas correspondientes.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i21) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A1:col(A,1)$ A2:col(A,2)$ A3:col(A,3)$ C:transpose(matrix([4,0,-7]))$
\end{verbatim}}
\end{minipage}
\newline

Las tres matrices son entonces:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i22) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
D1:addcol(C,A2,A3);D2:addcol(A1,C,A3);D3:addcol(A1,A2,C);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o22) }
\begin{pmatrix}4 & 4 & 3 \\ 0 & -2 & -2 \\ -7 & 3 & 2 \\ 
 \end{pmatrix}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o23) }
\begin{pmatrix}2 & 4 & 3 \\ 1 & 0 & -2 \\ -3 & -7 & 2 \\ 
 \end{pmatrix}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o24) }
\begin{pmatrix}2 & 4 & 4 \\ 1 & -2 & 0 \\ -3 & 3 & -7 \\ 
 \end{pmatrix}
\end{math}
\newline

Por lo tanto, los determinantes generan la solución a problema:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i25) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
x1:determinant(D1)/determinant(A);x2:determinant(D2)/determinant(A);
x3:determinant(D3)/determinant(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o25) }
2
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o26) }
-3
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o27) }
4
\end{math}

\item Existen algunas consideraciones a tomar en cuenta. 

\begin{itemize}
\item El sistema puede contener más incógnitas que ecuaciones, por ejemplo:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i28) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu1:x+y+z=1; ecu2:x+2*y+z=0;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o28) }
z+y+x=1
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o29) }
z+2\,y+x=0
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i30) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([ecu1,ecu2,ecu3],[x,y,z]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o30) }
\left[ x=2-{\tt \%r_1} , y=-1 , z={\tt \%r_1} \right] 
\end{math}
\newline

El programa nos está señalando que las soluciones, en este caso infinitas, dependerán de un parámetro que aquí es indicado con el símbolo  ${\tt \%r_1}$. Una solución puede ser entonces la siguiente: $\{x=2 , y=-1 , z=0\}$.


\item El sistema tiene más ecuaciones que incógnitas, por ejemplo:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i31) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu1:x+y=5; ecu2:x+2*y=8; ecu3:3*x+y=9;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o31) }
y+x=5
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o32) }
2\,y+x=8
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o33) }
y+3\,x=9
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i34) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([ecu1,ecu2,ecu3],[x,y]);
\end{verbatim}}
\end{minipage}
solve: dependent equations eliminated: (3)

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o34) }
\left[ x=2 , y=3  \right]
\end{math}
\newline
Aquí, {\bf Maxima} encuentra una solución al eliminar una de las ecuaciones dependientes. 

\item El sistema no tiene solución, por ejemplo:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i35) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ecu1:x+y=5; ecu2:x+2*y=8; ecu3:3*x+y=10;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o35) }
y+x=5
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o36) }
2\,y+x=8
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o37) }
y+3\,x=10
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i38) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
linsolve([ecu1,ecu2,ecu3],[x,y]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o38) }
\left[ \,\,\, \right]
\end{math}
\end{itemize}

\end{enumerate}

\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}


\subsection{{\color{OliveGreen}Ejercicios}}

\begin{enumerate}
\item Resuelva, utilizando dos de los métodos anteriormente vistos, el siguiente sistema de ecuaciones:
\begin{eqnarray*}
2x+3y+ z &=& 11 \\
x + y + z &=& 6 \\
5x - y + 10z  &=& 34     
\end{eqnarray*}
\item Resuelva, utilizando el método de la matriz inversa, el siguiente sistema de ecuaciones:
\begin{eqnarray*}
x^1 +2x^2 +3x^3  &=& 1 \\
3x^1 +4x^2 +5x^3 &=& 2 \\
x^1 +3x^2 +4x^3   &=& 3     
\end{eqnarray*}
\item Demuestre que el siguiente sistema sólo tiene solución si 
$\eta=1$ o $\eta=2$
\begin{eqnarray*}
x + y + z &=& 1 \\
x + 2y + 4z &=& \eta \\
x + 4y + 10z &=& \eta^2     
\end{eqnarray*}
\item Encuentre las condiciones sobre $\eta$ para que al resolver el sistema 
\begin{eqnarray*}
x^1 + \eta x^2  &=& 1 \\
x^1 - x^2 +3x^3 &=& -1 \\
2x^1 -2x^2 +\eta x^3   &=& -2     
\end{eqnarray*}
\begin{enumerate}
\item tenga una solución
\item no tenga solución
\item tenga infinitas soluciones
\end{enumerate}
Encuentre todas las soluciones que puedan existir.

\item Encuentre la factorización LU de las siguientes matrices:
\[
 \mathbb{A}=\left(
\begin{array}{ccc}
3  &  6 & 9 \\
1  &  0 & 5  \\
2 &  -2 & 16 
\end{array}
\right) \,, \qquad
 \mathbb{B}=\left(
\begin{array}{cccc}
2  &  -3 & 1 & 3 \\
1  &  4 & -3  & -3\\
5 &  3 & -1 & -1 \\ 
3 &  -6 & -3 & 1
\end{array}
\right)\,.
\]
\begin{itemize}
\item Resuelva $ \mathbb{A} \mathbb{X}= \mathbb{C}$, cuando: $\mathbb{C}= (21 \ 9 \ 28)^T$ y 
$\mathbb{C}= (21 \ 7 \ 22)^T$.
\item Resuelva $ \mathbb{B} \mathbb{X}= \mathbb{C}$ cuando: $\mathbb{C}= (-4 \ 1 \ 8 \ -5)^T$ y 
$\mathbb{C}= (-10 \ 0 \ -3\ -24)^T$.
\end{itemize}

\item Utilizando la regla de Cramer resuelva el siguiente sistema:
\begin{eqnarray*}
13 x^1 + 22 x^2 - 13 x^3 &=& 4 \\
10x^1 - 8x^2 -10x^3 &=& 44 \\
9x^1 -18x^2 -9 x^3   &=& 72     \,.
\end{eqnarray*}

\item Utilizando cualquiera de los métodos anteriores resuelva:
\begin{eqnarray*}
 x^1 +  x^2 + x^3 +  x^4 + x^5 &=& 7 \\
 3x^1 +  2x^2 + x^3 +  x^4 - 3x^5  &=& -2 \\
 x^2 + 2x^3 +  2x^4 + 6x^5    &=& 23 \\
 5x^1 +  4x^2 + 3x^3 +  3x^4 - x^5 &=& 12     \,.
\end{eqnarray*}

\item Determine el rango de la siguiente matriz:
\[
\left(\begin{array}{cccccc}
1 & -2 & 3 & -1 & -1 & -2 \\
2 & -1 & 1 & 0 & -2 & -2 \\
-2 & -5 & 8 & -4 & 3 & -1 \\
6 & 0 & -1 & 2 & -7 & -5 \\
-1 & -1 & 1 & -1 & 2 & 1
\end{array}\right)
\]

\item ¿Cuál es la condición para que las siguientes rectas se intercepten en un mismo punto?
\[
a_1x +  b_1y + c_1  = 0\,, \,\,\, 
a_2x +  b_2y + c_2  = 0\,, \,\,\,
a_3x +  b_3y + c_3  = 0\,,
\]

\item Resuelva los problemas anteriores utilizando {\bf Maxima}.
 
\end{enumerate}


\section{Autovectores y autovalores}
\label{SecAutovectoresAutovalores}
\index{Autovalores y autovectores}

La ecuación de Schrödinger independiente del tiempo de la Mecánica Cuántica es una ecuación, que escrita en la forma de operadores, tiene la siguiente forma:
\[
\mathbb{H} \left|\Psi_E \right \rangle = E|\Psi_E\rangle\,,
\]
donde $\mathbb{H}$ es un operador diferencial de segundo orden llamado el hamiltoniano y $\Psi_E$ la función de onda, en este caso representada por  $|\Psi_E\rangle$, {\it un estado propio} de 
$\mathbb{H}$ que corresponde a {\it un valor propio} de la energía 
$E$. 

Por tratarse de un espacio de Hilbert, es decir, un espacio vectorial con un producto interno bien definido, resulta de interés buscar soluciones en lo que se denomina el estado ligado de la ecuación de Schrödinger. Esto significa buscar  los $|\Psi_E\rangle$ en el espacio de las funciones de cuadrado integrable introduciendo una base unidimensional para representar   $\Psi_E$ y una matriz para representar $\mathbb{H}$, en otras palabras: buscar una representación matricial para la ecuación de Schrödinger. 

Los vectores propios, $|\Psi_E\rangle$, autovectores o ``eigenvectores'' de un operador lineal $\mathbb{H}$ son los vectores no nulos que, cuando son transformados por el operador, dan lugar a un múltiplo escalar  de sí mismos, con lo que no cambian su dirección. Este escalar $E$ recibe el nombre de valor propio, autovalor, valor característico o ``eigenvalor''. De manera que 
una transformación queda completamente determinada por sus vectores propios y valores propios. Un espacio propio, autoespacio, ``eigenespacio'' o subespacio fundamental asociado al valor propio $E$ es el conjunto de vectores propios con un valor propio común.

De manera general, llamaremos a $\left|  \psi\right> $ un autovector del operador $\mathbb{A}$ si se cumple que
\begin{equation}
\label{EcAutovalores1}
 \mathbb{A}\left|  \psi\right> =\lambda\left|  \psi\right>\,,
\end{equation}
en este caso $\lambda$ (que, en general será un número complejo) se denomina el autovalor correspondiente al autovector $\left|\psi\right>$.  La ecuación (\ref{EcAutovalores1}) es conocida en la literatura como la ecuación de autovalores y se cumple para algunos valores particulares de los autovalores $\lambda$ y, obviamente esto conduce a algunos vectores $\left|  \psi\right>$, los autovectores. Al conjunto de los autovalores se le denomina el espectro del operador $\mathbb{A}$.
\index{Espectro de un operador}
\index{Autovector!Angulo de fase}
\index{Angulo de fase de un autovector}

Nótese que si $\left|  \psi\right> $ es autovector de $\mathbb{A}$ para un determinado autovalor $\lambda$ entonces  $\left|{\phi}\right> =\alpha\left|  \psi\right> $ (un vector proporcional a $\left|  \psi\right>$, con $\alpha$ un número complejo) también es un autovector para el mismo autovalor. Esto representa una incómoda ambigüedad: dos autovectores que corresponden al mismo autovalor. Un intento de eliminarla es
\textbf{siempre} considerar vectores $\left|  \psi\right> $ normalizados, i.e. $\left< \psi\right.  \left|  \psi\right> =1$.
Sin embargo, no deja de ser un intento que no elimina la  ambigüedad del todo porque siempre queda el  ángulo de fase arbitrario. Esto es, el vector ${ e}^{i\theta}\left|  \psi\right>$, con $\theta$ un número
real arbitrario, tiene la misma norma del vector $\left|  \psi\right>$. 
Sin embargo esta arbitrariedad es inofensiva y en Mecánica Cuántica las predicciones obtenidas con $\left|  \psi\right> $ son las mismas que con ${ e}^{i\theta}\left|  \psi\right> $.

\subsection{Autovalores, autovectores e independencia lineal}
\label{SecAutovaloresIndependenciaLineal}
\index{Autovalores y autovectores!Independencia lineal}
\index{Independencia Lineal!Autovalores y autovectores}
La relación que existe entre los autovalores e independencia lineal de los autovectores asociados con cada autovalor es una de las herramientas más útiles que disponemos. Expondremos a continuación tres teoremas que establecen esta relación. 
\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema 1}: Sean $\left\{  \left|  {\psi}_{1}\right> ,\ \left|  {\psi}_{2}\right> ,\ \left|  {\psi}_{3}\right> ,\cdots\left|  {\psi}_{k}\right> \right\}  $ autovectores del operador $\mathbb{A}: \textbf{\em V}^{m}\rightarrow \textbf{\em V}^{n}$. 
Supongamos  que existen $k$ autovalores: 
$\left\{  \lambda_{1},\lambda_{2},\cdots,\lambda_{k}\right\}$, distintos correspondientes a cada uno de los autovectores $\left|  {\psi}_{j}\right>$, entonces los $\left\{  \left| {\psi}_{1}\right> ,\ \left|  {\psi}_{2}\right>,\cdots,\left|  {\psi}_{k}\right> \right\}$ son linealmente independientes. 
\end{mdframed}
\textbf{Demostración} La demostración de este teorema es por inducción y resulta elegante y sencilla.

\begin{itemize}
\item  Primeramente demostramos para  $j=1$.\newline Obvio que el resultado
se cumple y es trivial para el caso $k=1$ (un autovector $\left|  {\psi}_{1}\right> $ 
que corresponde a un autovalor $\lambda_{1}$ es obvia y
trivialmente linealmente independiente).

\item  Seguidamente supondremos que se cumple para $j=k-1$. \newline Si existen
 $\left\{  \left|  {\psi}_{1}\right> ,\ \left|{\psi}_{2}\right> ,\ \left|  {\psi}_{3}\right> ,\cdots\left|  {\psi}_{k-1}\right> \right\}$ autovectores de $\mathbb{A}$ correspondientes a $\left\{  \lambda_{1},\lambda_{2},\cdots,\lambda_{k-1}\right\}  $ entonces los $\left\{  \left|  {\psi}_{1}\right> ,\ \left|  {\psi}_{2}\right> ,\ \left|{\psi}_{3}\right> ,\cdots\left|  {\psi}_{k-1}\right>\right\}  $ son linealmente independientes.

\item  Ahora lo probaremos para $j=k$. \newline Por lo cual si tenemos $k$
autovectores $\left\{  \left|  {\psi}_{1}\right> ,\ \left| {\psi}_{2}\right> ,\ \left|  {\psi}_{3}\right> ,\cdots\left|  {\psi}_{k}\right> \right\} $, podremos construir una combinación lineal con ellos, y si esa combinación lineal se anula serán linealmente independientes.
\[
c^{j}\left|  {\psi}_{j}\right> =0\quad\text{con }j=1,2,\cdots,k \,,
\]
al aplicar el operador $\mathbb{A}$ a esa combinación lineal, obtenemos:
$
c^{j}\mathbb{A}\left|  {\psi}_{j}\right> =0 \,\, \Rightarrow \,\,
c^{j}\lambda_{j}\left|  {\psi}_{j}\right> =0 \,,
$ 
multiplicando por $\lambda_{k}$ y restando miembro a miembro resulta:
\[
c^{j}\left(  \lambda_{j}-\lambda_{k}\right)  \left|  {\psi}_{j}\right> =0\quad\text{con }j=1,2,\cdots,k-1 \,,
\]
(nótese que el último índice es $k-1$) pero, dado que los $k-1$
vectores $\left|  \mathbf{\psi}_{j}\right> $ son linealmente
independientes, entonces tendremos $k-1$ ecuaciones $c^{j}\left(  \lambda_{j}-\lambda_{k}\right)  =0$, una para cada $j=1,2,\cdots,k-1$. Dado que $\lambda_{j}\neq\lambda_{k}$ necesariamente llegamos a que $c^{j}=0$ para $j=1,2,\cdots,k-1$ y dado que:
\[
c^{j}\left|  {\psi}_{j}\right> =0\quad\text{con }j=1,2,\cdots,k \,\, \Rightarrow \,\, c^{j}\neq0 \,,
\]
con lo cual si
$
c^{j}\left|  {\psi}_{j}\right> =0\,\, \Rightarrow \,\, c^{j}=0\quad\text{con }j=1,2,\cdots,k \,,
$ 
y los $\left\{  \left|  {\psi}_{1}\right> ,\ \left|{\psi}_{2}\right> ,\ \left|  {\psi}_{3}\right>
,\cdots\left|  {\psi}_{k}\right> \right\} $ son linealmente independientes y queda demostrado el teorema. \,  $\blacktriangleleft$
\end{itemize}

Es importante acotar que este teorema nos recalca que si $\mathbb{A}: \textbf{\em V}^{m}\rightarrow \textbf{\em V}^{n}$  y $\mathbb{A}$ tiene $k \leq n$ autovalores $\left\{  \lambda_{1},\lambda_{2},\cdots,\lambda_{k}\right\}$ entonces existirán, \textbf{cuando menos}, $k \leq n$ autovectores $\left\{  \left| {\psi}_{1}\right>,\left|  {\psi}_{2}\right>, \cdots ,\left|  {\psi}_{k}\right> \right\}$ linealmente independientes, uno para cada autovalor. Hacemos énfasis en ese \textbf{cuando menos}, $k \leq n$ autovectores linealmente independientes, porque significa que el espacio $\textbf{\em V}^{m}$ no podrá ser expandido por los autovectores de $\mathbb{A}$. Ese punto lo analizaremos en la sección \ref{PolinomioCaracteristico}. 

El inverso de este teorema no se cumple. Esto es,  si $\mathbb{A}: \textbf{\em V}^{m}\rightarrow \textbf{\em V}^{n}$ tiene  $\left\{ \left|  {\psi}_{1}\right>, \left|  {\psi}_{2} \right>, \left|  {\psi}_{3}\right>,\cdots ,\left| {\psi}_{n}\right> \right\} $ autovectores linealmente independientes, no se puede concluir que existan $n$ autovalores $\left\{\lambda_{1},\lambda_{2},\cdots,\lambda_{n}\right\}$ distintos correspondientes a cada uno de los autovectores $\left|  {\psi}_{j}\right>$. 

El teorema anterior lo complementa el siguiente que lo presentaremos sin demostración. 
\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema 2}: Cualquier operador lineal $\mathbb{A}: \textbf{\em V}^{n}\rightarrow \textbf{\em V}^{n}$  tendrá un máximo de $n$ autovalores distintos. 
 Adicionalmente, si $\mathbb{A}$ tiene \textit{precisamente} $n$ autovalores: $\left\{  \lambda_{1},\lambda_{2},\cdots,\lambda_{n}\right\}$,  entonces los $n$ autovectores, $\left\{  \left|  {\psi}_{1}\right>, \left| {\psi}_{2}\right> ,\cdots ,\left|  {\psi}_{n}\right> \right\}$ (uno para cada autovalor), forman una base para $\textbf{\em V}^{n}$ y la representación matricial, en esa base, del operador será diagonal
\begin{equation}
\label{RepDiagonal1}
\left< \mathbf{\psi}^{i}\right|  \mathbb{A}\left|  {\psi}_{j}\right> =
A_{j}^{i}=\mathrm{diag}\left(  \lambda_{1},\lambda_{2},\cdots,\lambda_{n}\right)\,.
\end{equation}
\end{mdframed}
Es muy importante recalcar el significado de este teorema. Si la dimensión del espacio vectorial $\dim\left(\textbf{\em V}^{n}\right)  =n$ y por consiguiente la representación matricial será $n \times n$, entonces $\mathbb{A}$ tendrá un MÁXIMO de $n$ autovalores $\left\{  \lambda_{1},\lambda_{2},\cdots,\lambda_{n}\right\}$. Por otro lado, si tenemos $n$ autovalores distintos $\left\{  \lambda_{1},\lambda_{2},\cdots,\lambda_{n}\right\}$ TENDREMOS $n$ autovectores LINEALMENTE INDEPENDIENTES y la representación matricial será diagonal con los autovalores en la diagonal: $\mathrm{diag}\left(  \lambda_{1},\lambda_{2},\cdots,\lambda_{n}\right)$. 

Una versión complementaria al teorema anterior se obtiene si consideramos  que además existe una base ortogonal, $\left\{\left|\mathrm{e}_{i}\right>\right\}$, para $\textbf{\em V}$. Por lo tanto, la representación matricial de la expresión de la ecuación (\ref{RepDiagonal1})  es la siguiente:
\begin{equation}
\label{EcAutovalores}
 \mathbb{A}\left|  \psi\right> =\lambda\left|  \psi\right> \xRightarrow{\left\{\left|\mathrm{e}_{i}\right>\right\}} \left< \mathrm{e}^{i}\right|  \mathbb{A}\left|  \mathrm{e}_{j}\right> \left< \mathrm{e}^{j}\right.  \left|  \psi\right> =
\left< \mathrm{e}^{i}\right|  \lambda\left| \psi\right> = \lambda\left< \mathrm{e}^{i}\right.  \left| \psi\right> \,\, \Rightarrow \,\, 
A_{j}^{i}\ c^{j}=\lambda c^{i}\,,
\end{equation}
claramente, la base ortonormal,  $\left\{ \left|\mathrm{e}_{i}\right>  \right\} $, genera una representación diagonal de $\mathbb{A}$, entonces $A_{j}^{i}\propto \delta_{j}^{i}$, y esto lo podemos resumir en el siguiente teorema que presentaremos sin demostración para concluir esta sección. 

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema 3}:  Dado un operador lineal $\mathbb{A}:\textbf{\em V}^{n} \rightarrow \textbf{\em V}^{n}$, si la representación matricial de $\mathbb{A}$ es diagonal, 
$\left< \mathrm{e}^{i}\right|  \mathbb{A} \left|  \mathrm{e}_{j}\right> = A_{j}^{i} \propto \delta_{j}^{i}$, entonces existe una base ortogonal $\left\{ \left|  \mathrm{e}_{1}\right>, \left| \mathrm{e}_{2}\right>,\cdots , \left|  \mathrm{e}_{n}\right> \right\}$ y un conjunto de cantidades  $\left\{ \lambda_{1},\lambda_{2},\cdots,\lambda_{n}\right\}  $ tales que se cumple
$\mathbb{A}\left|  \mathrm{e}_{i}\right> =\lambda_{i}\ \left| \mathrm{e}_{i}\right>$ con 
$i=1,2,\cdots n \,. $
\end{mdframed}
 
\subsection{El polinomio característico, autovalores y autovectores de un operador}
\index{Polinomio característico}
\index{Autovalores y Autovectores!Polinomio caraterístico}
\label{SecPolinomioCaracteristico}
Nos toca ahora generar un método para calcular los autovalores $\left\{ \lambda_{j} \right\}$, con $k = 1, 2, \cdots k \leq n$ para un operador lineal $\mathbb{A}:\textbf{\em V}^{n}\rightarrow \textbf{\em V}^{n}$ suponiendo que existe una base ortonormal   $\left\{  \left|  \mathrm{e}_{1}\right>, \left|  \mathrm{e}_{2}\right>, \cdots\left|  \mathrm{e}_{n}\right> \right\}$. 
Entonces la ecuación (\ref{EcAutovalores}) nos ilustra  la representación matricial de la ecuación de autovalores:
\[
\left< \mathrm{e}^{i}\right|  \mathbb{A}\left|  \mathrm{e}_{j}\right> \left< \mathrm{e}^{j}\right|  \left|  \psi
\right> =\lambda\left< \mathrm{e}^{i}\right.  \left| \psi\right> \,\, \Rightarrow \,\, A_{j}^{i}\ c^{j}=\lambda c^{i}
\,\, \Rightarrow \,\,\left( A_{j}^{i}-\lambda\delta_{j}^{i}\right) c^{j}=0\,,
\]
con $j=1,2,\cdots, n$. El conjunto de ecuaciones: $\left( A_{j}^{i}-\lambda\delta_{j}^{i}\right)  c^{j}=0$, puede ser considerado un sistema (lineal y homogéneo) de ecuaciones con $n$ incógnitas $c^{j}$, el cual tendrá  solución si el determinante de los coeficientes se anula. Tendremos entonces que $\left(  A_{j}^{i}-\lambda\delta_{j}^{i}\right)  c^{j}=0  \,\, \Rightarrow \,\, \det\left|  \mathbb{A}-\lambda\mathbb{I}\right|  =0$, es decir:

\begin{equation}
\label{PolinomioCaracteristico}
P\left(\lambda \right) =\det\left|A_{j}^{i} -\lambda\delta_{j}^{i}\right|  =0\,.
\end{equation}

Esta ecuación se denomina ecuación característica (o secular) y a partir de ella emergen los autovalores (el espectro) del operador $\mathbb{A}$, esto es:
\[
\det\left| A_{j}^{i}-\lambda\delta_{j}^{i}\right| = 
\left|
\begin{array}
[c]{cccc}
A_{1}^{1}-\lambda &  A_{2}^{1} & \cdots &  A_{n}^{1}\\
A_{1}^{2} & A_{2}^{2}-\lambda &  &  A_{n}^{2}\\
\vdots &  & \ddots & \\
A_{1}^{n} & A_{2}^{n} &  & A_{n}^{n}-\lambda
\end{array}
\right| = 0\,.
\]

Es importante señalar que el polinomio característico será independiente de la base a la cual esté referida la representación matricial $\left< \mathrm{e}^{i}\right|  \mathbb{A}\left| \mathrm{u}_{j}\right> $ del operador $\mathbb{A}$, porque hereda del determinante, esa invariancia de la representación matricial tal y como vimos en la página \pageref{InvariaciaDeterminante}.

El resultado será un polinomio de grado $n$ (el polinomio característico). Las raíces de este polinomio serán los autovalores que estamos buscando. Es claro que estas raíces podrán ser reales y distintas, algunas reales e iguales y otras imaginarias. 

Para el caso de raíces reales, tendremos el mismo número de raíces que el grado del polinomio, generado por la representación matricial del operador con ese mismo grado. Es decir,  tendremos un operador $\mathbb{A}$ con una representación matricial de dimensión $n \times n$,  con  $n$ autovalores distintos, que estará asociados a $n$ autovectores que serán linealmente independientes y que generarán una representación matricial diagonal. 

\subsection{El caso degenerado}
\label{SecAutovalorDegenerado}
\index{Autovalores!Multiplicidad algebraica}
\index{Autovectores!Multiplicidad geométrica}
Sea el operador lineal $\mathbb{A}:\textbf{\em V}^{n}\rightarrow \textbf{\em V}^{n}$ y calculamos el polinomio característico de grado $n$ a partir de (\ref{PolinomioCaracteristico}). Se puede dar el caso que al menos una de las raíces del polinomio característico presenten algún grado de multiplicidad. Entonces el polinomio característico se podrá factorizar de la forma:
\begin{equation}
\label{PolCaracteristicoFactorizado}
P\left(\lambda \right) =\det\left|A_{j}^{i} -\lambda\delta_{j}^{i}\right| = (\lambda - \lambda_{1})^{k}(\lambda - \lambda_{2})\cdots(\lambda - \lambda_{m})\cdots(\lambda - \lambda_{n}).
\end{equation}
Entonces existirán $n -k$ raíces simples que podrán ser asociadas con $n -k$ autovectores linealmente independientes y una raíz, $\lambda_1$, con multiplicidad $k$ que podrá ser asociada con $1, 2, \cdots $ hasta $k$ autovectores linealmente independientes con los anteriores. Es decir, ese autovalor estará asociado a un subespacio vectorial, denominado autoespacio \textbf{\em S}$_{\lambda_1}$ tal que 
$\mathrm{dim}(\textbf{\em S}_{\lambda_1}) \leq$ grado de multiplicidad del autovalor  $\lambda_1$\footnote{Este problema se conoce en la literatura como la relación entra la multiplicidad algebraica del autovalor y la multiplicidad geométrica del autoespacio}. La demostración general de la afirmación anterior queda fuera de los alcance de este trabajo, pero en la próxima sección ilustraremos la multiplicidad algebraica $vs$ multiplicidad geométrica con varios ejemplos. Más adelante, cuando analicemos el caso particular de los autovalores para la matrices hermíticas en la sección \ref{AutoValoresMatricesHermiticas}, retomaremos la relación de la multiplicidad del autovalor y la dimensión del autoespacio que genera este autovalor degenerado. 


\subsection{{\color{Fuchsia}Ejemplos}}

\begin{enumerate}
\item \textbf{Reflexión respecto al plano }$xy$. Si $\mathbb{R}:
\textbf{\em V}^{3}\rightarrow \textbf{\em V}^{3}$ es tal que 
$\mathbb{R}\left|  \psi\right>=\left|  \tilde{\psi}\right> $ 
donde se ha realizado una reflexión en el plano $xy$. Esto es
\[
\mathbb{R}\left|  {\mathrm{i}}\right> =\left|  \mathrm{i}\right>;\quad
\mathbb{R}\left|  \mathrm{j}\right> =\left|  \mathrm{j}\right> ;\quad
\mathbb{R}\left|  \mathrm{k}\right> =-\left|\mathrm{k}\right>\,,
\]
con $\left|  \mathrm{i}\right> ,\left| \mathrm{j}\right> ,\left|\mathrm{k}\right> $ los vectores unitarios cartesianos. Es claro que
cualquier vector en el plano $xy$ será autovector de $\mathbb{R}$ con un autovalor $\lambda=1$, mientras que cualquier otro vector $\left| \psi\right> \in\textbf{\em V}^{3}$ y que no esté en el mencionado plano cumple con $\left|  \psi\right> =c\left| \mathrm{k}\right>$ y también será autovector de $\mathbb{R}$ pero esta vez con un autovalor $\lambda=-1$.

\item \textbf{Dos visiones de rotaciones de ángulo fijo }$\theta$.  
La rotaciones de un vector en el plano pueden verse de dos maneras.

\begin{enumerate}
\item  Se considera el plano como un espacio vectorial \textit{real}
$ \textbf{\em V}^{2}$ con una base cartesiana canónica: \newline $\left|  \mathrm{i}
\right> =\left(1,0\right)$\,, $\ \left| \mathrm{j} \right>=\left(  0,1\right)$, esto es, si:
\[
\mathbb{R}\left|  {a}\right> =\lambda\left|  {a}
\right> \,\, \Rightarrow \,\, \text{ el ángulo de rotación} =n\pi\,,
\quad\text{con }n\text{ entero}.
\]

\item  Igualmente si consideramos el plano complejo unidimensional, expresemos cualquier vector en el plano en su forma polar $\left| {z} \right> =r{ e}^{i\theta}$ por lo cual:
\[
\mathbb{R}\left|  {z}\right> =
r{ e}^{i\left( \theta+\alpha\right)  }={ e}^{i\alpha}\left|  {z}\right>\,,
\]
si queremos $\lambda={ e}^{i\alpha}$ reales, necesariamente 
$\alpha=n\pi$ con $n$ entero.
\end{enumerate}

\item \textbf{Proyectores: autovalores y autovectores}. 
\index{Proyectores!Autovalores y autovectores}
Es interesante plantearse la ecuación de autovalores con la definición del proyector para un determinado \textit{autoespacio}. Esto es, dado $P_{\psi}=\left|\psi\right> \left< \psi\right|$ si este proyector cumple con una ecuación de autovalores para un $\left|  \varphi\right> $ supuestamente arbitrario
\[
P_{\psi}\left|  \varphi\right> =\lambda\ \left|  \varphi\right>
\,\, \Rightarrow \,\, P_{\psi}\left|  \varphi\right> =\left(  \left|
\psi\right> \left< \psi\right|  \right)  \left|  \varphi
\right> \,\, \Rightarrow \,\,   \left|  \varphi\right> \propto\left|
\psi\right>\,,
\]
es decir, necesariamente $\left|  \varphi\right> $ es colineal con
$\left|  \psi\right> $.  Más aún, si ahora el $\left|\varphi\right> $ no es tan arbitrario sino que es ortogonal a $\left|
\psi\right> ,$ $\left< \psi\right.  \left|  \varphi\right>=0
\,\, \Rightarrow \,\,  \lambda=0$, entonces el espectro
del operador $P_{\psi}=\left|  \psi\right> \left< \psi\right| $
es $0$ y $1$, el primero de los cuales es infinitamente degenerado y el segundo es simple. Esto nos lleva a reflexionar que si existe un autovector de un determinado operador, entonces su autovalor es distinto de cero, pero pueden existir autovalores nulos que generan un autoespacio infinitamente degenerado.

\item \textbf{El operador diferenciación}. $\mathbb{D}\left|  {f} \right> \rightarrow D\left(  f\right)=f^{\prime}$. 
Los autovectores del operador diferenciación necesariamente deben satisfacer la ecuación:
\[
\mathbb{D}\left|  {f}\right> =\lambda\left|  {f}\right> \rightarrow D\left(  f\right)  (x)  =f^{\prime}(x) = \lambda f(x) \,,
\]
la solución a esta ecuación será una exponencial. Esto es, $\left|  {f}\right> \rightarrow f(x)  =c{ \mathrm{e}}^{\lambda x}$, con $c\neq0$, y donde las $f(x)$ se denominarán \textit{autofunciones} del operador.

\item {\bf Ejemplos de autovalores y autovectores matrices reales}.
\index{Autovectores!Matrices reales}
\index{Autovalores!Matrices reales}
El procedimiento para el cálculo de los autovectores y autovalores es el siguiente. Una vez obtenidas  las raíces del polinomio característico (los autovalores), se procede a determinar el autovector, $\left|  \mathbf{\psi}_{j}\right>$,  correspondiente a cada autovalor. Distinguiremos en esta determinación casos particulares dependiendo del tipo de raíz del polinomio característico.
Ilustraremos estos casos con ejemplos  para el caso específico de matrices reales  $3\times3$.

\begin{enumerate}
\item \textbf{Todos los autovalores reales son distintos.} Hemos presentado y analizado en la sección \ref{SecAutovaloresIndependenciaLineal} tres teoremas que discuten la relación entre los autovalores, y la independencia lineal de los autovectores asociados con éstos. En esta sección ilustraremos el caso que surge cuando las raíces del polinomio característico (\ref{PolinomioCaracteristico}) son reales y distintas. 

Consideremos la siguiente representación matricial de un operador $\mathbb{A}$ en la base canónica 
\[
\left< \mathrm{e}^{i}\right|  \mathbb{A}\left|  \mathrm{e}
_{j}\right> =\left(
\begin{array}
[c]{ccc}
2 & 1 & 3\\
1 & 2 & 3\\
3 & 3 & 20
\end{array}
\right) \,\, \Rightarrow \,\, \det\left| A_{j}^{i}-\lambda\delta_{j}
^{i}\right|  =\left|
\begin{array}
[c]{ccc}
2-\lambda & 1 & 3\\
1 & 2-\lambda & 3\\
3 & 3 & 20-\lambda
\end{array}
\right|  =0\,,
\]
con lo cual el polinomio característico (\ref{PolinomioCaracteristico}) queda expresado como:
\[
\lambda^{3}-24\lambda^{2}+65\lambda-42=\left(  \lambda-1\right)  \left( \lambda-2\right)  \left(  \lambda-21\right)  =0 \,,
\]
y es claro que tiene 3 raíces distintas. Para proceder a calcular los autovectores correspondientes a cada autovalor resolvemos la ecuación de autovalores (\ref{EcAutovalores}) para cada autovalor. Esto es:
\begin{itemize}
\item $\lambda_{1}=1$
\[
\left(
\begin{array}
[c]{ccc}
2 & 1 & 3\\
1 & 2 & 3\\
3 & 3 & 20
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  =1 \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) \,\, \Longleftrightarrow \,\,
\begin{array}
[c]{ccc}
2x^{1}+x^{2}+3x^{3} & = & x^{1}\\
x^{1}+2x^{2}+3x^{3} & = & x^{2}\\
3x^{1}+3x^{2}+20x^{3} & = & x^{3} \,,
\end{array}
\]
que constituye un sistema de ecuaciones algebraicas de 3 ecuaciones con 3 incógnitas. Resolviendo el sistema tendremos el primer autovector:
\[
\left|  \psi \right>_{\lambda_1=1}= \left|  \psi_{1} \right> =
\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =  \alpha\left(
\begin{array}
[c]{r}
-1\\
1\\
0
\end{array}
\right) \,,
\]
con $\alpha$ un número distinto de cero. Este $\alpha$ indica la indeterminación que discutimos a comienzos de la sección \ref{SecAutovectoresAutovalores} correspondiente a esa 
ambigüedad que impone la presencia de una constante arbitraria de proporcionalidad. 

\item $\lambda_{2}=2$
\[
\left(
\begin{array}
[c]{ccc}
2 & 1 & 3\\
1 & 2 & 3\\
3 & 3 & 20
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  =2\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) \,\, \Longleftrightarrow \,\,
\begin{array}
[c]{ccc}
2x^{1}+x^{2}+3x^{3} & = & 2x^{1}\\
x^{1}+2x^{2}+3x^{3} & = & 2x^{2}\\
3x^{1}+3x^{2}+20x^{3} & = & 2x^{3} \,.
\end{array}
\]
Resolviendo el sistema se tiene el segundo autovector
\[
\left|  \psi \right>_{\lambda_2}= \left|  \psi_{2} \right> =
\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\beta
\left(
\begin{array}
[c]{r}
-3\\
-3\\
1
\end{array}
\right)\,.
\]

\item $\lambda_{3}=21$
\[
\left(
\begin{array}
[c]{ccc}
2 & 1 & 3\\
1 & 2 & 3\\
3 & 3 & 20
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  =21\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  \,\, \Longleftrightarrow \,\,
\begin{array}
[c]{ccc}
2x^{1}+x^{2}+3x^{3} & = & 21x^{1}\\
x^{1}+2x^{2}+3x^{3} & = & 21x^{2}\\
3x^{1}+3x^{2}+20x^{3} & = & 21x^{3}\,.
\end{array}
\]
Al resolver este sistema el tercer autovector
\[
\left|  \psi \right>_{\lambda_3}= \left|  \psi_{3} \right> =
\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\gamma
\left(
\begin{array}
[c]{c}
1\\
1\\
6
\end{array}
\right) \,.
\]
\end{itemize}
Como hemos dicho, podemos eliminar la ambigüedad de la fase arbitraria si normalizamos los autovectores:
\[
\left|  \hat{\psi}_{1} \right> =
\frac{1}{\sqrt{2}}\left(
\begin{array}
[c]{r}
-1\\
1\\
0
\end{array}
\right), \qquad 
\left|  \hat{\psi}_{2} \right> =
\frac{1}{\sqrt{19}}\left(
\begin{array}
[c]{r}
-3 \\
-3 \\
 1
\end{array}
\right), \qquad 
\left|  \hat{\psi}_{3} \right> =
\frac{1}{\sqrt{38}}\left(
\begin{array}
[c]{r}
1 \\
1 \\
6
\end{array}
\right)\,.
\]

Notemos que $\left< \hat{\psi}^{i} \right. \left| \hat{\psi}_{j} \right> = \delta^{i}_{j} $, es decir la base de autovectores es, necesariamente, ortonormal. 

\item  \textbf{Autovalores degenerados.} En esta sección discutiremos algunos ejemplos de autovalores degenerados, vale decir cuando alguna de las raíces del polinomio característico (\ref{PolinomioCaracteristico}) tiene una multiplicidad.  Arriba en la página \pageref{SecAutovalorDegenerado} discutimos el caso de la diferencia de la multiplicidad algebraica y la multiplicidad geométrica. Esto es que la dimensión del autoespacio siempre es menor o igual a la multiplicidad de la raíz degenerada del polinomio característico $\mathrm{dim}(\textbf{\em S}_{\lambda_1}) \leq$ grado de multiplicidad del autovalor. 
\begin{enumerate}
  \item \textbf{Multiplicidad geométrica igual a multiplicidad algebraica}.  
  Consideremos una matriz real  $3\times3$  
\index{Autovalores degenerados}
\[
\left< \mathrm{e}^{i}\right|  \mathbb{A}\left|  \mathrm{e}_{j}\right> =\left(
\begin{array}
[c]{ccc}
4 & -3 & 1\\
4 & -1 & 0\\
1 & 7 & -4
\end{array}
\right)  \,\, \Rightarrow \,\,  \det\left| A_{j}^{i}-\lambda\delta_{j}^{i}\right|  =\left|
\begin{array}
[c]{ccc}
4-\lambda & -3 & 1\\
4 & -1-\lambda & 0\\
1 & 7 & -4-\lambda
\end{array}
\right|  =0 \,,
\]
con lo cual el polinomio característico queda expresado como:
\[
\lambda^{3}+\lambda^{2}-5\lambda-3=\left(  \lambda+3\right)  \left(\lambda-1\right)  ^{2}=0 \,,
\]
y es claro que tiene 2 raíces iguales y una distinta. En este caso $\lambda=1$ es un autovalor degenerado de orden 2. 

Ahora  resolveremos la ecuación de autovalores para cada autovalor.
\begin{itemize}
\item $\lambda_{1}=-3$
\[
\left(
\begin{array}
[c]{ccc}
4 & -3 & 1\\
4 & -1 & 0\\
1 & 7 & -4
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  =-3\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  \,\, \Longleftrightarrow \,\,
\begin{array}
[c]{rcc}
4x^{1}-3x^{2}+x^{3} & = & -3x^{1}\\
4x^{1}-x^{2} & = & -3x^{2}\\
x^{1}+7x^{2}-4x^{3} & = & -3x^{3}\,.
\end{array}
\]
\\
Resolviendo  
$
\left|  \psi \right>_{\lambda_1}=
\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\alpha\left(
\begin{array}
[c]{r}
-1\\
2\\
13
\end{array}
\right)  \quad \Rightarrow \quad
\left|  \hat{\psi}_{1} \right> =
 \frac{1}{\sqrt{174}} \left(
\begin{array}
[c]{r}
-1\\
2\\
13
\end{array}
\right)
$

\item $\lambda_{2}=1$ (autovalor degenerado de orden 2)
\[
\left(
\begin{array}
[c]{ccc}
4 & -3 & 1\\
4 & -1 & 0\\
1 & 7 & -4
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  \,\, \Longleftrightarrow \,\,
\begin{array}
[c]{rcc}
4x^{1}-3x^{2}+x^{3} & = & x^{1}\\
4x^{1}-x^{2} & = & x^{2}\\
x^{1}+7x^{2}-4x^{3} & = & x^{3}\,.
\end{array}
\]
\\
Resolviendo el sistema tendremos el segundo autovector
\[
\left|  \psi \right>_{\lambda_2}=
\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\alpha\left(
\begin{array}
[c]{c}
1\\
2\\
3
\end{array}
\right) \quad \Rightarrow \quad 
\left|  \hat{\psi}_{2} \right> =
 \frac{1}{\sqrt{14}} \left(
\begin{array}
[c]{r}
1\\
2\\
3
\end{array}
\right)\,.
\]

Claramente $\left\{ \left|  \hat{\psi}_{1} \right>, \left|  \hat{\psi}_{2} \right> \right\}$ son linealmente independientes como nos los había anunciado el teorema 1 en la sección \ref{SecAutovaloresIndependenciaLineal}, sin embargo, esos autovectores no presentan ninguna relación de ortogonalidad $\left< \hat{\psi}^{i} \right. \left| \hat{\psi}_{j} \right> \neq \delta^{i}_{j} $. 

Más, aún, este autovector puede descomponerse en infinitas parejas de vectores linealmente independientes que expenden el subespacio $\mathbb{S}_{\lambda = 1}$, asociado con  $\lambda$ = 1, el autovalor degenerado de multiplicidad $k = 2$. Una posible combinación lineal podría ser:
\[
\left|  \psi \right>_{\lambda_2}=
\alpha \left| \psi_1 \right>_{\lambda_2}+  \beta   \left| \psi_2 \right>_{\lambda_2}=  
\alpha\left(
\begin{array}
[c]{r}
1\\
0\\
3
\end{array}
\right)  +\beta\left(
\begin{array}
[c]{r}
0\\
2\\
0
\end{array}
\right) \,.
\]
Por lo tanto, la multiplicidad geométrica --la dimensión del autoespacio, \textbf{\em S}$_{\lambda_2 = 1}$, asociado con el autovalor degenerado $\lambda = 1$-- coincide con la multiplicidad algebraica del autovalor $\lambda = 1$ como raíz del polinomio característico. Es decir, podemos determinar la dimensión del subespacio que alberga al vector $\left|  \psi \right>_{\lambda_2}$. Es importante recalcar que ninguno de los (infinitos) dos vectores linealmente independientes, será autovector de $\mathbb{A}$ por separado. Unicamente lo será la combinación lineal en la que resulta $\left|  \psi \right>_{\lambda_2}$.
\end{itemize}

\item  \textbf{Multiplicidad geométrica menor a multiplicidad algebraica.} Consideremos, para ilustrar este caso la siguiente matriz real  $3\times3$  
\index{Autovalores degenerados}
\[
\left< \mathrm{e}^{i}\right|  \mathbb{A}\left|  \mathrm{e}_{j}\right> =\left(
\begin{array}
[c]{ccc}
1 & 1 & 2 \\
0 & 1 & 3 \\
0 & 0 & 2
\end{array}
\right)  \,\, \Rightarrow \,\,  \det\left|  A_{j}^{i}-\lambda\delta_{j}^{i}\right|  =
\left|
\begin{array}
[c]{ccc}
1-\lambda & 1 & 2 \\
0 & 1-\lambda & 3 \\
0 & 0 & 2-\lambda
\end{array}
\right|  =0 \,.
\]
En este caso,  el polinomio característico es:
\[
\lambda^{3}-4\lambda^{2}+5\lambda-2=\left(  \lambda-2\right)  \left(\lambda-1\right)^{2}=0 \,.
\]
Una vez más, el polinomio característico tiene 2 raíces iguales y una distinta y, como en el caso anterior $\lambda=1$ es un autovalor degenerado de orden 2.
Pasemos a resolver la ecuación de autovalores para cada autovalor.
\begin{itemize}
\item $\lambda_{1}=2$
\[
\left(
\begin{array}
[c]{ccc}
\begin{array}
[c]{ccc}
1 & 1 & 2 \\
0 & 1 & 3 \\
0 & 0 & 2
\end{array}
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  = 2\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  \,\, \Longleftrightarrow \,\,
\begin{array}
[c]{rcc}
x^{1} +x^{2} +2x^{3} & = & 2x^{1} \\
x^{2} +3x^{3} & = & 2x^{2}    \\
2x^{3} & = & 2x^{3}\,.
\end{array}
\] 

Resolviendo el sistema obtenemos al primer autovector:  
$
\left|  \psi_1 \right>=
\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\alpha
\left(
\begin{array}
[c]{r}
5\\
3\\
1
\end{array}
\right) \, .
$

\item $\lambda_{2}=1$
\[
\left(
\begin{array}
[c]{ccc}
\begin{array}
[c]{ccc}
1 & 1 & 2 \\
0 & 1 & 3 \\
0 & 0 & 2
\end{array}
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  = \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  \,\, \Longleftrightarrow \,\,
\begin{array}
[c]{rcc}
x^{1} +x^{2} +2x^{3} & = & x^{1} \\
x^{2} +3x^{3} & = & x^{2}    \\
2x^{3} & = & x^{3} 
\end{array}
\] 

Con el correspondiente segundo autovector:  
$
\left|  \psi_2 \right>=
\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\alpha
\left(
\begin{array}
[c]{r}
1\\
0\\
0
\end{array}
\right) \, .
$ 

Una vez más los dos vectores NO son ortogonales, $\left< \psi^{i} \right. \left| \psi_{j} \right> \neq \delta^{i}_{j} $, pero SI linealmente independientes. En este caso, asociado al autovalor $\lambda_2 = 1$, tenemos además del autovector $\left|  \psi_2 \right>$, el autovector nulo y esto genera una imposibilidad de determinar la dimensión del autoespacio. Los autovectores siguen siendo linealmente independientes, pero aquí la dimensión del autoespacio es NECESARIAMENTE 1,  $\mathrm{dim}(\textbf{\em S}_{\lambda_2 = 1}) = 1$, por lo tanto ilustra que la multiplicidad geométrica es menor que la multiplicidad aritmética de las raíces del polinomio característico. 

\end{itemize}

\item  Vamos ilustrar un tercer ejemplo que se presenta para uno de los autovalores degenerados. Consideremos entonces otra matriz $3\times3$ con autovalores repetidos
\[
\left< \mathrm{e}^{i}\right|  \mathbb{A}\left|  \mathrm{e}
_{j}\right> =\left(
\begin{array}
[c]{ccc}
2 & 1 & 1\\
2 & 3 & 2\\
3 & 3 & 4
\end{array}
\right) \,\, \Rightarrow \,\, \det\left|  A_{j}^{i}-\lambda\delta_{j}
^{i}\right|  =\left|
\begin{array}
[c]{ccc}
2-\lambda & 1 & 1\\
2 & 3-\lambda & 2\\
3 & 3 & 4-\lambda
\end{array}
\right|  =0 \,,
\]
con lo cual el polinomio característico es:
\[
\lambda^{3}+\lambda^{2}-5\lambda-3=\left(  \lambda-7\right)  \left(
\lambda-1\right)  ^{2}=0 \,,
\]
que tiene 2 raíces iguales y una distinta. En este caso $\lambda=1$ vuelve a ser un autovalor degenerado de orden 2. Volvemos a calcular los autovectores correspondientes a cada autovalor 

\begin{itemize}
\item $\lambda_{1}=7$
\[
\left(
\begin{array}
[c]{ccc}
2 & 1 & 1\\
2 & 3 & 2\\
3 & 3 & 4
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  =7\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  \,\, \Longleftrightarrow \,\, 
\begin{array}
[c]{rcc}
2x^{1}+x^{2}+x^{3} & = & 7x^{1}\\
2x^{1}+3x^{2}+3x^{3} & = & 7x^{2}\\
3x^{1}+3x^{2}+4x^{3} & = & 7x^{3}\,.
\end{array}
\]
Al resolver el sistema 
\[
\left|  \psi \right>_{\lambda_1}=
\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\alpha\left(
\begin{array}
[c]{c}
1\\
2\\
3
\end{array}
\right)\,.
\]

\item $\lambda_{2}=1$. En este caso el autovalor degenerado de orden 2 presenta una pequeña patología. Veamos
\[
\left(
\begin{array}
[c]{ccc}
2 & 1 & 1\\
2 & 3 & 2\\
3 & 3 & 4
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  =\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  \,\,\Longleftrightarrow \,\,
\begin{array}
[c]{rcc}
2x^{1}+x^{2}+x^{3} & = & x^{1}\\
2x^{1}+3x^{2}+2x^{3} & = & x^{2}\\
3x^{1}+3x^{2}+4x^{3} & = & x^{3}\,.
\end{array}
\]
Resolviendo
\[
\left|  \psi \right>_{\lambda_2}=
\left\{ 
\begin{array}
[c]{c}
\left|  \psi_1 \right>_{\lambda_2}=\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\alpha\left(
\begin{array}
[c]{r}
1\\
0\\
-1
\end{array}
\right) \\
\\
\left|  \psi_2 \right>_{\lambda_2}=\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\beta\left(
\begin{array}
[c]{r}
0\\
1\\
-1
\end{array}
\right)
\end{array}
\right.
\]
con lo cual el autovector $\left|  \psi \right>_{\lambda_2} $ correspondiente al autovalor $\lambda_{2}=1$, está asociado con a dos vectores linealmente independientes $\left\{\left|  \psi_1 \right>_{\lambda_2}, \left|  \psi_2 \right>_{\lambda_2}\right\}$ y, por lo tanto aquí también la multiplicidad aritmética coincide con la multiplicidad geométrica. Una vez más los autovectores, correspondientes a distintos autovalores no son ortogonales pero si linealmente independientes.
\end{itemize}

\end{enumerate}


\item  Finalmente, consideremos una matriz $3\times3$ con 1 autovalor real y dos autovalores complejos
\[
\left< \mathrm{e}^{i}\right|  \mathbb{A}\left|  \mathrm{e}
_{j}\right> =\left(
\begin{array}
[c]{ccc}
1 & 2 & 3\\
3 & 1 & 2\\
2 & 3 & 1
\end{array}
\right)  \quad \Rightarrow \quad \det\left|  A_{j}^{i}-\lambda\delta_{j}
^{i}\right|  =\left|
\begin{array}
[c]{ccc}
1-\lambda & 2 & 3\\
3 & 1-\lambda & 2\\
2 & 3 & 1-\lambda
\end{array}
\right|  =0 \,,
\]
con lo cual el polinomio característico queda expresado como:
\[
\lambda^{3}-3\lambda^{2}-15\lambda-18=\left(  \lambda-6\right)  \left(\lambda^{2}+3\lambda+3\right)  =0 \,.
\]
En este caso $\lambda_1=6$ es un autovalor real. Adicionalmente existen dos autovalores
complejos, uno el complejo conjugado del otro: $\lambda_2=-\frac{1}{2}\left(  3+i\sqrt{3}\right)  $ y ${\lambda}_3=-\frac{1}{2}\left(3-i\sqrt{3}\right)$. Para proceder a calcular los autovectores
correspondientes a cada autovalor resolvemos la ecuación de
autovalores para cada autovalor real. En este caso existe \textbf{un único} autovalor real $\lambda=6$.
\[
\left(
\begin{array}
[c]{ccc}
1 & 2 & 3\\
3 & 1 & 2\\
2 & 3 & 1
\end{array}
\right)  \left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  =6\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right)  \,\, \Longleftrightarrow \,\,
\begin{array}
[c]{rcc}
4x^{1}-3x^{2}+x^{3} & = & 6x^{1}\\
4x^{1}-x^{2} & = & 6x^{2}\\
x^{1}+7x^{2}-4x^{3} & = & 6x^{3}\,.
\end{array}
\]
Tendremos que para $\lambda= 6$ 
\[
\left|  \psi \right>_{\lambda_1}=
\left(
\begin{array}
[c]{c}
x^{1}\\
x^{2}\\
x^{3}
\end{array}
\right) =\alpha\left(
\begin{array}
[c]{c}
1\\
1\\
1
\end{array}
\right) \,.
\]

\end{enumerate}

\item Sean $\mathbb{A}$ y $\mathbb{B}$ dos operadores hermíticos, con autovalores no degenerados y un operador unitario definido como: $\mathbb{U} = \mathbb{A} + i\mathbb{B}$. Vamos a mostrar que:

\begin{enumerate}

\item Si $\mathbb{A}$ y $\mathbb{B}$  conmutan, $[\mathbb{B}, \mathbb{A} ] = 0$, los autovectores de $\mathbb{A}$ también lo son de  $\mathbb{B}$.

Si $\left\{  \left| {u}_{i} \right> \right\}$ son autovectores de $\mathbb{A}$ entonces 
\[
 \mathbb{A}  \left| {u}_{i} \right> = \lambda_{i}  \left| {u}_{i} \right> 
 \,\, \Rightarrow \,\, 
  \mathbb{B} \mathbb{A}   \left| {u}_{i} \right> = \lambda_{i}   \mathbb{B} \left| {u}_{i} \right>\,, \quad 
  \mathrm{como} \; [\mathbb{B}, \mathbb{A} ] = 0\,,\quad \mathrm{entonces} \; 
   \mathbb{A}   \mathbb{B}  \left| {u}_{i} \right> = \lambda_{i}   \mathbb{B} \left| {u}_{i} \right>\,.
\]

Por lo tanto, $ \mathbb{B} \left| {u}_{i} \right>$ es un autovector de 
 $\mathbb{A} $. Pero la solución para la ecuación de autovectores $\left( \mathbb{A} - \lambda_{i} \mathbb{I} \right)  \left| {u}_{i} \right> = 0 $ es única, por lo cual todos los autovectores de $\mathbb{A}$ son proporcionales. Esto es: $ \mathbb{B} \left| {u}_{j} \right> = \mu_{j} \left| {u}_{j} \right>$, con lo cual queda demostrado que los autovectores de $\mathbb{A}$ son autovectores de $\mathbb{B}$.

\item Si $\mathbb{U} \left| {v}_{i} \right> = \nu_{i}  \left| {v}_{i} \right>$, entonces $ |\mu_{i}| = 1 $. 

Es claro que:
$
 \left< {v}^{j} \right| \mathbb{U}^{\dag}  \mathbb{U} \left| {v}_{i} \right>  =   \left< {v}^{j} \right| \mathbb{I} \left| {v}_{i} \right>   
\,\, \Rightarrow \,\,\mu^{\ast}_{j} \mu_{i}   \left< {v}^{j} \right. \left| {v}_{i} \right> = \left< {v}^{j} \right| \mathbb{I} \left| {v}_{i} \right>  \,\, \Rightarrow \,\,  \mu^{2}_{i} = 1 \,.
$
\end{enumerate}

\item Dada una matriz de la forma
\[
 \mathbb{A} = \left(\begin{array}{ccc}1 & \alpha & 0 \\\beta & 1 & 0 \\0 & 0 & 1\end{array}\right) \quad \mathrm{y} \quad  
\mathbb{A} \left| {v}_{i} \right> = \lambda_{i}  \left| {v}_{i} \right>
\]
con $\alpha$ y $\beta$ números complejos distintos de cero. Vamos a encontrar:
\begin{enumerate} 
\item Las relaciones que deben cumplir $\alpha$ y  $\beta$ para que $\lambda_{i}$ sea real. 

El polinomio característico y la condición para que $\lambda$ sea real es: 
 \[
 (1 -\lambda)(1 -2\lambda + \lambda^{2} -\alpha\beta) = 0 \quad \Rightarrow \lambda = 1 \pm \sqrt{\alpha\beta}
\,\, \Rightarrow \,\, \alpha\beta > 0 \; \wedge \; \alpha\beta \in \mathds{R} \,.
 \]
 
 \item Las relaciones que deben cumplir $\alpha$ y  $\beta$ para que $ \left< {v}^{j} \right. \left| {v}_{i} \right> = \delta^{j}_{i}$.
  
Los autovalores y autovectores para esta matriz serán:
 \[
\lambda_{1} = 1 \Rightarrow  \left| {v}_{1} \right> = 
\left(\begin{array}{c}0 \\0 \\1\end{array}\right)\,, \,\,
\lambda_{2} = 1 + \sqrt{\alpha\beta}   \Rightarrow   \left| {v}_{2} \right> = 
\left(\begin{array}{c}\frac{\beta}{\sqrt{\alpha\beta}} \\1 \\0\end{array}\right)\,, \,\,
\lambda_{3} = 1 + \sqrt{\alpha\beta}  \Rightarrow  \left| {v}_{3} \right> = 
\left(\begin{array}{c}\frac{-\beta}{\sqrt{\alpha\beta}} \\1 \\0\end{array}\right)\,,
 \]
con lo cual:
$
  \left< {v}^{2} \right. \left| {v}_{3} \right> = 0 \,\, \Rightarrow \,\,  \frac{\beta^{2}}{\alpha\beta} = 1\,\, \Rightarrow \,\, |\alpha | = | \beta | \,.
$
 
\item Supongamos que $\mathbb{A} $ es hermítica, encontremos  las relaciones que deben cumplir $\alpha$ y  $\beta$.
 
Si  $\mathbb{A} $ es hermítica, entonces $\alpha^{\ast} = \beta$, con lo cual se cumplen automáticamente ambas aseveraciones. 
\end{enumerate}

\item Dadas las siguientes matrices: 
\[
\mathbb{A} = \left(\begin{array}{cc}6 & -2 \\-2 & 9\end{array}\right)\,, \quad
\mathbb{B} = \left(\begin{array}{cc}1 & 8 \\8 & -11\end{array}\right)\,, \quad
\mathbb{C} = \left(\begin{array}{cc}-9 & -10 \\-10 & 5\end{array}\right)\,, \quad
\mathbb{D} = \left(\begin{array}{cc}14 & 2 \\2 & 11\end{array}\right)\,.
\]

Determinemos cuales conmutan entre ellas y busquemos la base de autovectores comunes.

Notamos que: $[  \mathbb{A},  \mathbb{B} ] = [  \mathbb{A},  \mathbb{D} ] = [  \mathbb{D},  \mathbb{B} ] = 0$, y
\[
[  \mathbb{A},  \mathbb{C} ] = \left(\begin{array}{cc}0 & 2 \\-2 & 0\end{array}\right)\,, \quad
[  \mathbb{B},  \mathbb{C} ] = \left(\begin{array}{cc}0 & -8 \\8 & 0\end{array}\right) \,, \quad
[  \mathbb{D},  \mathbb{C} ] = \left(\begin{array}{cc}0 & -2 \\2 & 0\end{array}\right) 
\]

Los autovectores comunes a $ \mathbb{A},  \mathbb{B},  \mathbb{D}$, serán: 
\[
\left| {u}_{1} \right> = \left(\begin{array}{r} -\frac{1}{2} \\1 \end{array}\right) \,,\quad
\left| {u}_{2} \right> = \left(\begin{array}{c} 2 \\1 \end{array}\right)\,.
\]

\item Dada la representación matricial de dos operadores 
\[
 \mathbb{A} = \left(\begin{array}{ccc}0 & 0 & 1 \\0 & 1 & 0 \\1 & 0 & 0\end{array}\right) 
\quad \mathrm{y} \quad 
 \mathbb{B} = \left(\begin{array}{ccc}0 & 1 & 1 \\1 & 0 & 1 \\1 & 1 & 0\end{array}\right)
\]
\begin{enumerate}
\item Evaluemos $\left[  \mathbb{A},  \mathbb{B} \right] $.

\[
  \mathbb{A}  \mathbb{B}  = \left(\begin{array}{ccc}0 & 0 & 0 \\1 & 0 & 0 \\0 & 0 & 0\end{array}\right) =   \mathbb{B}   \mathbb{A}
\,\, \Rightarrow \,\, \left[  \mathbb{A},  \mathbb{B} \right] =0
\]

\item Vamos a mostrar que $  \mathbb{A}$ tiene por autovalores $\lambda_{1}= 1$ y $\lambda_{2}= -1$, con $\lambda_{1}$ un autovalor degenerado. Y construyamos luego la base de autovectores para $  \mathbb{A}$.

  \[
  \left(\begin{array}{ccc}
  0 & 0 & 1 \\
  0 & 1 & 0 \\
  1 & 0 & 0
  \end{array}\right) 
  \left(\begin{array}{c}x \\y \\z\end{array}\right) = \lambda \left(\begin{array}{c}x \\y \\z\end{array}\right) 
  \,\, \Rightarrow \,\,
   \left(\begin{array}{ccc}-\lambda & 0 & 1 \\0 & 1 -\lambda & 0 \\1 & 0 & -\lambda\end{array}\right) 
    \left(\begin{array}{c}x \\y \\z\end{array}\right) \,.
  \]
  Entonces para: 
  \[
    \left|\begin{array}{ccc}
    -\lambda & 0 & 1 \\
    0 & 1 -\lambda & 0 \\
    1 & 0 & -\lambda
    \end{array}\right| =
    \lambda (1 - \lambda) \lambda  - (1 - \lambda) = \left(\lambda^{2} - 1 \right) (1 - \lambda) =0 \,,
  \]
se tienen dos autovalores: $\lambda = 1$ y $\lambda = -1$. Para el caso de $\lambda = -1$ se cumple que:
  \[
   \left(\begin{array}{ccc}0 & 0 & 1 \\0 & 1 & 0 \\1 & 0 & 0\end{array}\right)    \left(\begin{array}{c}x \\y \\z\end{array}\right) = - \left(\begin{array}{c}x \\y \\z\end{array}\right) \,\, \Rightarrow \,\,
    \begin{array}{l}  z = -x \\ y = - y \\x = -z \end{array}
  \] 
con lo cual el autovector asociado con el autovalor $\lambda = -1$ tendrá la forma de: 
  \[
  \left| {u} \right\rangle_{-1} = \alpha \left(\begin{array}{r}1 \\ 0 \\-1\end{array}\right)\,.
  \]
Para $\lambda = 1$ se cumple:
 \[
 \left(\begin{array}{ccc}0 & 0 & 1 \\0 & 1 & 0 \\1 & 0 & 0\end{array}\right)    \left(\begin{array}{c}x \\y \\z\end{array}\right) =  \left(\begin{array}{c}x \\y \\z\end{array}\right) \,\, \Rightarrow \,\,
 \begin{array}{l}  z = x \\ y =  y \\ x = z \end{array}
  \] 
hay dos vectores linealmente independientes asociados con $\lambda = 1$, a saber:
  \[
  \left| {u} \right\rangle_{1a} = \beta \left(\begin{array}{r}1 \\ 0 \\1\end{array}\right)  \quad \mathrm{y} \quad
   \left| {u} \right\rangle_{1b} =    \left(\begin{array}{r}0 \\ y \\ 0\end{array} \right) \quad \mathrm{ con } \; {y} \; \mathrm{arbitrario}\,.
  \]

Nótese que estos tres autovectores: $\{  \left| {u} \right\rangle_{1a}, \left| {u} \right\rangle_{1b},  \left| {u} \right\rangle_{-1} \}$ son ortogonales entre si.
  
\item ¿Cuál es la representación matricial de $\mathbb{A}$ en la base de autovectores?
   
Veamos lo siguiente:
 \[
 \tilde{A}^{i}_{j} =
   \left(\begin{array}{ccc}
\left\langle {u}^{1} \right| \mathbb{A} \left| {u}_{1} \right\rangle & 
\left\langle {u}^{1} \right| \mathbb{A} \left| {u}_{2} \right\rangle  & 
\left\langle {u}^{1} \right| \mathbb{A} \left| {u}_{3} \right\rangle \\
\left\langle {u}^{2} \right| \mathbb{A} \left| {u}_{1} \right\rangle  & 
\left\langle {u}^{2} \right| \mathbb{A} \left| {u}_{2} \right\rangle & 
\left\langle {u}^{2} \right| \mathbb{A} \left| {u}_{3} \right\rangle \\
 \left\langle {u}^{3} \right| \mathbb{A} \left| {u}_{1} \right\rangle & 
 \left\langle {u}^{3} \right| \mathbb{A} \left| {u}_{2} \right\rangle & 
 \left\langle {u}^{3} \right| \mathbb{A} \left| {u}_{3} \right\rangle \end{array}\right) = 
  \left(\begin{array}{ccc}1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & -1\end{array}\right) \,,
  \]
ya que los autovectores forman una base ortogonal. Obviamente se cumple que: 
  \[
{\det}|\mathbb{A}| =  {\det}| \mathbb{\tilde{A}}|= -1  \quad \mathrm{y} \quad
\mathrm{Tr}( \mathbb{A}) =   \mathrm{Tr}(\mathbb{\tilde{A}}) =1 \,.  
   \]   
   
\item A partir de los autovectores de $\mathbb{A}$ vamos a calcular los autovalores y autovectores de $ \mathbb{B}$. 

Claramente $\mathbb{B} \left| {u}_{-1} \right\rangle = - \left| {u}_{-1} \right\rangle$ con lo cual tenemos el primer autovector de $\mathbb{B}$ asociado al autovalor $\lambda = -1$. Para encontrar los otros autovectores tendremos:
    \[
  \left(\begin{array}{ccc}-\lambda & 1 & 1 \\1 & -\lambda & 1 \\1 & 1 & -\lambda \end{array}\right)
   \left(\begin{array}{r}1 \\ y \\1\end{array}\right) =  \left(\begin{array}{r}0 \\ 0 \\ 0 \end{array}\right) \,.
    \]
 \end{enumerate}

\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}} 

En este ejercicio aprenderemos a resolver el problema de autovalores y autovectores. Primeros lo haremos de manera indirecta, realizando los cálculos por pasos, y luego utilizando las funciones específicas del programa. 

\begin{enumerate}
\item  Dada la matriz $ \mathbb{A}$ del primer ejemplo anteriormente resuelto:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A:matrix([2,1,3], [1,2,3], [3,3,20]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\begin{pmatrix}2 & 1 & 3 \\ 1 & 2 & 3 \\ 3 & 3 & 20 \\ 
 \end{pmatrix}
\end{math}
\newline

Generamos la matriz identidad $3\times 3$ con el comando {\bf ident(n)}.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
I:ident(3);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\begin{pmatrix}1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 
\end{pmatrix}
\end{math}
\newline

Escribimos la ecuación de autovalores de manera matricial y la llamaremos $M$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M:A-lambda*I;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\begin{pmatrix}2-\lambda & 1 & 3 \\ 1 & 2-\lambda & 3 \\ 3 & 3 & 20
 -\lambda \\ 
\end{pmatrix}
\end{math}
\newline

El determinante nos dará el polinomio característico, luego resolvemos para $\lambda$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
factor(determinant(M));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
-\left(\lambda-21\right)\,\left(\lambda-2\right)\,\left(\lambda-1
 \right)
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve(%=0);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ \lambda=1 , \lambda=2 , \lambda=21 \right]
\end{math}
\newline

Los autovalores son: $ \lambda_1=1$, $ \lambda_2=2$ y $ \lambda_3=21$.  Para cada autovalor evaluaremos una matriz correspondiente:


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M1:M,lambda=1; M2:M,lambda=2; M3:M,lambda=21;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\begin{pmatrix}1 & 1 & 3 \\ 1 & 1 & 3 \\ 3 & 3 & 19 \\ 
 \end{pmatrix}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\begin{pmatrix}0 & 1 & 3 \\ 1 & 0 & 3 \\ 3 & 3 & 18 \\ 
 \end{pmatrix}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\begin{pmatrix}-19 & 1 & 3 \\ 1 & -19 & 3 \\ 3 & 3 & -1 \\ 
 \end{pmatrix}
\end{math}
\newline

Necesitamos ahora resolver, para cada autovalor, la ecuación de autovectores: $ \mathbb{A} \mathbb{X}=\lambda_i \mathbb{X}$. Así que podemos escribir las siguientes matrices:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
M1:M1.[x,y,z]; M2:M2.[x,y,z]; M3:M3.[x,y,z];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\begin{pmatrix}3\,z+y+x \\ 3\,z+y+x \\ 19\,z+3\,y+3\,x \\ 
 \end{pmatrix}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\begin{pmatrix}3\,z+y \\ 3\,z+x \\ 18\,z+3\,y+3\,x \\ \end{pmatrix}
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\begin{pmatrix}3\,z+y-19\,x \\ 3\,z-19\,y+x \\ -z+3\,y+3\,x \\ 
 \end{pmatrix}
\end{math}
\newline

Iremos ahora resolviendo cada uno de los sistemas de ecuaciones. Para $\lambda_1=1$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ec1:M1[1,1]=0; ec2:M1[2,1]=0; ec3:M1[3,1]=0;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
3\,z+y+x=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
3\,z+y+x=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
19\,z+3\,y+3\,x=0
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve([ec1,ec2,ec3],[x,y,z]);
\end{verbatim}}
{solve: dependent equations eliminated: (2)}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
\left[ \left[ x=-{\it \%r_4} , y={\it \%r_4} , z=0 \right] \right] 
\end{math}
\newline

Recordemos que {\bf Maxima} nos indica con el símbolo 
$\it \%r_n$ que se trata de una constante. Así que de las infinitas soluciones escogemos alguna de las más simples: $(1,-1,0)$. 

Repetimos los cálculos para el segundo autovalor, es decir,  
$\lambda_2=2$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ec1:M2[1,1]=0; ec2:M2[2,1]=0; ec3:M2[3,1]=0;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
3\,z+y=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o17) }
3\,z+x=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
18\,z+3\,y+3\,x=0
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve([ec1,ec2,ec3],[x,y,z]);
\end{verbatim}}
{solve: dependent equations eliminated: (3)}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o19) }
\left[ \left[ x=-3\,{\it \%r_5} , y=-3\,{\it \%r_5} , z={\it \%r_5}\right]  \right]
\end{math}
\newline

Por lo tanto un autovector puede ser: $(1,1,-1/3)$. 

Finalmente, para $\lambda_3=21$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i20) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
ec1:M3[1,1]=0; ec2:M3[2,1]=0; ec3:M3[3,1]=0;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o20) }
3\,z+y-19\,x=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o21) }
3\,z-19\,y+x=0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o22) }
-z+3\,y+3\,x=0
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i23) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve([ec1,ec2,ec3],[x,y,z]);
\end{verbatim}}
{solve: dependent equations eliminated: (3)}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o23) }
\left[ \left[ x=\frac{{\it \%r_6}}{6} , y=\frac{{\it \%r_6}}{6} , z=
 {\it \%r_6} \right]  \right] 
\end{math}
\newline


Podemos tomar como autovector a: $(1,1,6)$.  

\item {\bf Maxima} ofrece la posibilidad de resolver el problema de autovalores a través de un determinado número de funciones. Por ejemplo, la función {\bf charpoly} nos dará el polinomio característico directamente de la matriz que queremos estudiar y respecto de la variable que seleccionemos, es este caso será  $\lambda$.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i24) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
charpoly(A,lambda);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o24) }
\lambda+\left(\left(2-\lambda\right)\,\left(20-\lambda\right)-9
 \right)\,\left(2-\lambda\right)+3\,\left(3-3\,\left(2-\lambda\right)
 \right)-11
\end{math}


%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i25) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
factor(%);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o25) }
-\left(\lambda-21\right)\,\left(\lambda-2\right)\,\left(\lambda-1
 \right)
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i26) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
solve(%=0);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o26) }
\left[ \lambda=1 , \lambda=2 , \lambda=21 \right] 
\end{math}
\newline

El programa también permite obtener los autovalores directamente de la matriz problema. Esto se hace con el comando {\bf eigenvalues(M)}. El resultado será una lista conformada a su vez por dos listas: la primera sublista la forman los valores propios de la matriz $M$ y la segunda con su multiplicidad correspondientes. Veamos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i27) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
eigenvalues(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o27) }
\left[ \left[ 1 , 2 , 21 \right]  , \left[ 1 , 1 , 1 \right]  \right]
\end{math}
\newline

La función para calcular los autovectores es {\bf eigenvectors(M)}. El resultado será  una lista con dos elementos; el primero está formado por dos listas: la primera con los valores propios de $M$ y la segunda con su respectiva multiplicidad, el segundo elemento es una lista de listas de vectores propios, una por cada valor propio.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i28) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
eigenvectors(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o28) }
\left[ \left[ \left[ 1 , 2 , 21 \right]  , \left[ 1 , 1 , 1\right]  \right]  , \left[ \left[ \left[ 1 , -1 , 0 \right] \right]  , \left[ \left[ 1 , 1 , -\frac{1}{3} \right]  \right]  ,  \left[ \left[ 1 , 1 , 6 \right]  \right]  \right]  \right] 
\end{math}
\newline

\item Consideremos otro de los ejemplos tratado anteriormente. Dada la matriz:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i29) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
B:matrix([4,-3,1], [4,-1,0], [1,7,-4]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o29) }
\begin{pmatrix}4 & -3 & 1 \\ 4 & -1 & 0 \\ 1 & 7 & -4 \\ 
 \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i30) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
eigenvectors(B);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o30) }
\left[ \left[ \left[ -3 , 1 \right]  , \left[ 1 , 2 \right] 
  \right]  , \left[ \left[ \left[ 1 , -2 , -13 \right]  \right]  , 
 \left[ \left[ 1 , 2 , 3 \right]  \right]  \right]  \right] 
\end{math}
\newline

Notemos que sólo se obtienen dos autovectores. 

\item Consideremos la matriz: 

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i31) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
C:matrix([2,1,1], [2,3,2], [3,3,4]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o31) }
\begin{pmatrix}2 & 1 & 1 \\ 2 & 3 & 2 \\ 3 & 3 & 4 \\ 
\end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i32) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
eigenvectors(C);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o32) }
\left[ \left[ \left[ 7 , 1 \right]  , \left[ 1 , 2 \right] 
  \right]  , \left[ \left[ \left[ 1 , 2 , 3 \right]  \right]  , 
 \left[ \left[ 1 , 0 , -1 \right]  , \left[ 0 , 1 , -1 \right] 
  \right]  \right]  \right] 
\end{math}
\newline

\item Y finalmente, dada la matriz:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i33) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
D:matrix([1,2,3], [3,1,2], [2,3,1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o33) }
\begin{pmatrix}1 & 2 & 3 \\ 3 & 1 & 2 \\ 2 & 3 & 1 \\ 
\end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i34) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
eigenvectors(D);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o34) }
\left[ \left[ \left[ -\frac{\sqrt{3}\,i+3}{2} , \frac{\sqrt{3}\,i-3}{2} , 6 \right]  , \left[ 1 , 1 , 1 \right]  \right] ,\left[ \left[ \left[ 1 , \frac{\sqrt{3}\,i-1}{2} , -\frac{\sqrt{3}\,i+1}{2} \right]  \right] \right] \right. , 
\end{math}

\begin{math} 
\left. \left.\qquad \qquad \quad 
\left[ \left[ 1 , -\frac{\sqrt{3}\,i+1}{2} , \frac{\sqrt{3}\,i-1}{2} \right]  \right],\left[ \left[ 1 , 1 , 1\right]  \right]  \right]  \right]
\end{math}

\end{enumerate}

\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}


\subsection{{\color{OliveGreen}Ejercicios}}

\begin{enumerate}
\item Si $\left| v_{1}\right>$ y $\left| v_{2}\right>$ son autovectores del operador lineal $\mathbb{A}$ que corresponden a distintos autovalores. Muestre que $\alpha \left| v_{1}\right>+\beta \left| v_{2}\right>$ ($\alpha\neq 0, \beta \neq 0$) no puede ser un autovector de  $\mathbb{A}$.
\item Demuestre que si todo vector de un espacio vectorial $\textbf{\em V}$ es un autovector del operador lineal $\mathbb{A}$, entonces $\mathbb{A}=\lambda \mathbb{I}$. Donde $\mathbb{I}$ es el operador identidad.
\item Demuestre que si el operador lineal $\mathbb{A}$ conmuta con todos operadores que actúan em un determinado espacio vectorial, entonces $\mathbb{A}=\lambda \mathbb{I}$.
\item Si un operador lineal $\mathbb{A}$ tiene un autovector $\left| v_{0}\right>$ con autovalor $\lambda_0$. Demuestre que $\left| v_{0}\right>$ es también un autovector del operador $\mathbb{A}^2$ con autovalor $\lambda_0^2$.
\item Aun si un operador lineal $\mathbb{A}$ no tiene autovectores el operador $\mathbb{A}^2$ puede llegar a tenerlos. Demuestre que si $\mathbb{A}^2$ tiene un autovector con un autovalor no degenerado $\lambda_0=\mu^2$, entonces $\mathbb{A}$ tiene un autovector.


\item Encuentre los autovalores y autovectores de las siguientes matrices: 
\[ 
\mathbb{A}= \left(\begin{array}{ccc}
1 & 3 & -1 \\
3 & 4 & -2 \\
-1 & -2 & 2
\end{array}\right) \,,\,\,
\mathbb{B}= \left(\begin{array}{ccc}
1 & 0 & 0 \\
-3 & 1 & 0 \\
4 & -7 & 1
\end{array}\right)\,,\,\,
\mathbb{C}= \left(\begin{array}{cccc}
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0
\end{array}\right)\,,\,\,
\mathbb{D}= \left(\begin{array}{cccc}
0 & 1 & 0 & 0\\
1 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{array}\right) \,,
\]
y la matriz $\mathbb{E}=\mbox{diag}(1,1,-1,-1)$. Verifique si los autovectores son ortogonales entre ellos.

\item Demuestre que la matriz 
\[ 
 \mathbb{A} = \left(\begin{array}{ccc}
2 & 0 & 0 \\
-6 & 4 & 4 \\
3 & -1 & 0
\end{array}\right)\,,
\]
no tiene tres autovectores linealmente independientes y que cualquier 
autovector tiene la forma:
\[
\left(\begin{array}{c}
\lambda \\ 3 \lambda -2\nu \\ \nu
\end{array}\right) \,.
\]


\item Construimos un sistema con tres partículas, de masa $m_i$ rígidamente unidas, colocadas en tres puntos distintos de la siguiente forma:
 \[
 m_{1} = 1 \rightarrow 
 \left(
 \begin{array}{r}
      1   \\
      1  \\
      -2     
\end{array}
 \right)\,, \quad 
 m_{2} = 2 \rightarrow 
 \left(
 \begin{array}{r}
      -1   \\
      -1  \\
      0     
\end{array}
 \right) \quad \mathrm{y} \quad
 m_{3} = 1 \rightarrow 
 \left(
 \begin{array}{c}
      1   \\
      1  \\
      2     
\end{array}
 \right) \,.
 \]
 \begin{enumerate}
  \item Encuentre la matriz del tensor de inercia.
  \item Diagonalize esa matriz y encuentre los ejes principales de inercia.
\end{enumerate}


\item En Mecánica Cuántica, el problema del oscilador armónico simple puede ser representado por un problema de autovalores
\[
\mathbb{L}|\psi> = \lambda |\psi> \,\, \Rightarrow \,\, 
<x|\mathbb{L}|\psi> = \lambda <x|\psi> \, \leftrightarrow \,
\mathbb{L} \psi(x) = \lambda \psi(x)\,, \quad \mathrm{donde:} \quad 
\mathbb{L} \, \leftrightarrow \, \frac{\mathrm{d}^{2} }{\mathrm{d}x^{2}} +\frac{x^{2}}{4} +\frac{1}{2} \,.
\] 
Si construimos un par de operadores: 
\[
\mathbb{L}_{+} =  \frac{x}{2} +\frac{\mathrm{d}}{\mathrm{d}x} \quad \mathrm{y} \quad
\mathbb{L}_{-} = \frac{x}{2} - \frac{\mathrm{d}}{\mathrm{d}x} \,,
\]
utilice los resultados del problema anterior y construya las nuevas autofunciones de $\mathbb{L}$ con sus autovalores.

\item Considere las siguiente representación matricial para dos operadores: 
 \[
 <u^{i}(\mathbb{A})u_{j}> = A^{i}_{j} =
\left(
\begin{array}{cc}
  1    & \delta   \\
  \delta    &   1
\end{array}\right)  \quad \mathrm{y} \quad
 <u^{m}|\mathbb{B}|u_{n}> = B^{m}_{n} =
\left(
\begin{array}{cc}
  1    & 1   \\
  \delta^{2}    &   1
\end{array}\right)\,.
 \]\begin{enumerate}
 
 \item Muestre como actúan $\mathbb{A}$ y  $\mathbb{B}$  sobre un vector genérico $|\psi>$ expandido en esa base $\{ |u_{1}>,  |u_{2}> \}$  (vale decir $|\psi> = \alpha |u_{1}> + \beta |u_{2}>$).
  \item Muestre que los autovalores de $\mathbb{A}$ son degenerados para $\delta = 0$ y que sus autovectores son ortogonales para $\delta \neq 0$ e incluso para $\delta \rightarrow 0$.
    \item Muestre que también $\mathbb{B}$ tiene autovalores degenerados para $\delta = 0$ y encuentre la expresión (en función de $0 \leq \delta \leq 1$) para el coseno del ángulo entre dos autovectores. 
\end{enumerate}
\item Dada la siguiente representación matricial de un operador en la base canónica:
\[
\mathbb{M} = 
\left(
\begin{array}{cc}
   2   &  i \sqrt{2}   \\
   -i \sqrt{2}    &   3
\end{array}
\right)\,, \quad 
| {1} > =
\left(\begin{array}{c}
     1    \\
     0  
\end{array}
\right)\,,  \quad 
| {2} > =
\left(\begin{array}{c}
     0    \\
     1  
\end{array}
\right) \,.
\]
\begin{enumerate}
  \item Encuentre los autovectores $\{ | \varphi_{1} > , | \varphi_{2} > \} $ para ese operador en la base canónica.
  \item Encuentre las representaciones matriciales de los operadores proyección sobre los auto espacios, $\mathbb{P}_{ | \varphi_{i} >} =  | \varphi_{i} > < \varphi^{i} |$, en esa misma base canónica.
  \item Encuentre las representaciones matriciales de los operadores proyección sobre los complementos ortogonales de los autoespacios $U_{m}^{n} =  | \varphi_{m} > < \varphi^{n} |$ en esa misma base y con ella calcule $M = M^{i}_{j}U^{j}_{i}$.
\end{enumerate}

\item  Las matrices $\boldsymbol{\sigma}_{x},\boldsymbol{\sigma}_{y}$, $\boldsymbol{\sigma}_{z}$ se conocen con el nombre de matrices de Pauli :
\[
\boldsymbol{\sigma}_{x}=\left(
\begin{array}
[c]{cc}
0 & 1\\
1 & 0
\end{array}
\right)\,, \quad \boldsymbol{\sigma}_{y}=\left(
\begin{array}
[c]{cc}
0 & -i\\
i & 0
\end{array}
\right)\,, \quad\boldsymbol{\sigma}_{z}=\left(
\begin{array}
[c]{cc}
1 & 0\\
0 & -1
\end{array}
\right)\,, \quad\mathbf{I}=\left(
\begin{array}
[c]{cc}
1 & 0\\
0 & 1
\end{array}
\right)\,.
\]

\begin{enumerate}
\item  Muestre si las matrices de Pauli $\boldsymbol{\sigma}_{x},\boldsymbol{\sigma}_{y}$, $\boldsymbol{\sigma}_{z}$, conjuntamente con la matriz identidad, $\mathbf{I}$ forman un grupo respecto a la siguiente operación:
\[
\boldsymbol{\sigma}_{j}\odot\boldsymbol{\sigma}_{k}\equiv\boldsymbol{\sigma}_{j}\boldsymbol{\sigma}_{k}=i\epsilon_{jkm}
\boldsymbol{\sigma}^{m}+\delta_{jk}\mathbf{I} \quad\text{con }j,k,m=x,y,z
\]
donde $\epsilon_{jkm}$ es el símbolo de Levi-Civita y $i=\sqrt{-1}$. 

\item  Muestre si las matrices de Pauli $\boldsymbol{\sigma}_{x},\boldsymbol{\sigma}_{y}$, $\boldsymbol{\sigma}_{z},$
conjuntamente con la matriz identidad $\mathbf{I}$ son linealmente independientes.

\item ¿Las matrices de Pauli forman base para un espacio vectorial de matrices complejas $2x2$? ¿Por qué?  Si forman una base exprese la matriz
\[
\left(\begin{array}
[c]{cc}
3 & i\\
5 & 1
\end{array}
\right)\,,
\] 
en términos de esa base.

\item  Derive la expresión general para $\left[ \boldsymbol{\sigma}_{j},\boldsymbol{\sigma}_{k}\right]$.

\item  Suponga ahora que $\boldsymbol{\sigma}_{z}$ actúa de la siguiente forma:
$
 \boldsymbol{\sigma}_{z}\left|  +\right\rangle =\left|  +\right\rangle \,,\,\,  
 \boldsymbol{\sigma}_{z}\left|  -\right\rangle =-\left|  -\right\rangle \,,
$
con:
\[
\left|  +\right\rangle \leftrightarrows\left(
\begin{array}
[c]{c}
1\\
0
\end{array}
\right)\,, \quad\left|  -\right\rangle \leftrightarrows\left(
\begin{array}
[c]{c}
0\\
1
\end{array}
\right)\,.
\]
Encuentre la expresión para los autovalores y autovectores de las otras matrices de Pauli:
\[
 \boldsymbol{\sigma}_{x}\left|  +\right\rangle _{x} =\lambda_{+x}\left|+\right\rangle _{x}\,,\quad \boldsymbol{\sigma}_{x}\left|  -\right\rangle _{x}=\lambda_{-x}\left|  -\right\rangle _{x} \,,\quad
 \boldsymbol{\sigma}_{y}\left|  +\right\rangle _{y}=\lambda_{+y}\left|+\right\rangle _{y}\,, \quad \boldsymbol{\sigma}_{y}\left|  -\right\rangle_{y}=\lambda_{-y}\left|  -\right\rangle _{y} \,.
\]

\item  Muestre que cualquier representación matricial de un operador genérico $\mathbb{M}$ puede ser expresado como combinación lineal de las matrices de Pauli.
   
 \item El polinomio característico para ese operador genérico 
 $\mathbb{M}$  se puede expresar como 
 \[ 
 P_{\lambda} = \lambda^{2} - \lambda \mathrm{Tr}(\mathbb{M}) + \mathrm{det}|\mathbb{M}| \,.
 \]
Donde los  $\lambda$ son sus autovalores.
\end{enumerate}

\item Resuelva con {\bf Maxima} los ejercicios anteriores.

\end{enumerate}

\section{Autovalores y autovectores de matrices importantes}
\label{autovectoresmatricesimportantes}
Consideremos la ecuación de autovalores (\ref{EcAutovalores1}) y la definición de operadores adjuntos que hemos discutido en la sección \ref{OpAdjunto}. Podemos afirmar sin perder generalidad que dado un conjunto de vectores 
$\left\{  \left| \mathrm{u}_{1}\right> ,\ \left|  \mathrm{u}_{2}\right> , \cdots ,\left|  \mathrm{u}_{n}\right> \right\}$ y  de cantidades  
$\left\{  \lambda_{1} ,\lambda_{2},\cdots,\lambda_{n}\right\}$ que cumplan con  
$\mathbb{A}\left|  \mathrm{u}_{i}\right> =\lambda_{i}\ \left| \mathrm{u}_{i}\right>$ con $i=1,2,\cdots  n$, entonces  
\begin{equation}
\label{AutoMatrizGenerica1}
\left.
\begin{array}{ccc}
 \mathbb{A}\left|  \mathrm{u}_{i}\right> =\lambda_{i}\ \left| \mathrm{u}_{i}\right>     &  \Rightarrow &  
\left<\mathrm{u}^{j}\right| \mathbb{A}\left|  \mathrm{u}_{i}\right> =\lambda_{i}\left<\mathrm{u}^{j}\right. \left| \mathrm{u}_{i}\right>  \\
      &    & \\
\left<\mathrm{u}^{j}\right|  \mathbb{A}^{\dag} =  \lambda_{j}^{*} \left<\mathrm{u}^{j}\right|  &  \Rightarrow  & 
\left<\mathrm{u}^{j}\right|  \mathbb{A}^{\dag}\left|  \mathrm{u}_{i}\right>  =  \lambda_{j}^{*} \left<\mathrm{u}^{j}\right. \left|  \mathrm{u}_{i}\right>
\end{array} 
\right\} \Rightarrow \left<\mathrm{u}^{j}\right| \mathbb{A} - \mathbb{A}^{\dag}\left|  \mathrm{u}_{i}\right> =
(\lambda_{i} -\lambda_{j}^{*}) \left<\mathrm{u}^{j}\right. \left|  \mathrm{u}_{i}\right> \, .
\end{equation} 
Vale decir, hemos proyectado las ecuaciones de autovalores de un operador genérico y su adjunto a lo largo de los vectores y las formas correspondientes, $\left<\mathrm{u}^{j}\right|$ y $\left|  \mathrm{u}_{i}\right>$, para luego restar esas expresiones miembro a miembro. 

Es evidente que si no conocemos el tipo de operador, poco se puede decir de sus autovectores y autovalores. Hay que imponer algunas restricciones sobre el tipo de operador para poder sacar algunas conclusiones respecto al tipo de autovalores y autovectores que ese operador pueda tener. Por ejemplo:
\begin{itemize}
  \item Si $\mathbb{A}$ es hermítico,  $\mathbb{A} = \mathbb{A}^{\dag}$ y autovectores son distintos,  ($i \neq j$) entonces de la ecuación (\ref{AutoMatrizGenerica1}) se deduce que esos autovectores serán ortogonales, $ \left<\mathrm{u}^{j}\right. \left|  \mathrm{u}_{i}\right> \propto \delta^{j}_{i}$.   
  \item Si $\mathbb{A}$ es hermítico,  $\mathbb{A} = \mathbb{A}^{\dag}$ y autovectores son los mismos,  ($i = j$) entonces los autovalores son reales: $\lambda_{i} =\lambda_{i}^{*}$.
\end{itemize}
Este caso lo analizaremos en la próxima sección. 

\subsection{Autovalores y autovectores de matrices hermíticas y unitarias}
\label{AutoValoresMatricesHermiticas}

Tal y como mencionamos en la sección \ref{OperadoresHermiticos} un operador hermítico o autoadjunto cumple con $\mathbb{A}^{\dagger}=\mathbb{A}$ y luego en la sección \ref{MatricesdeOperadoresHermiticos}, comprobamos que su representación matricial es igual a su traspuesta conjugada. Por lo tanto, la representación matricial de un operador autoadjunto es una matriz simétrica con números reales en su diagonal. Veamos ahora cómo se comportan sus autovalores.

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
 \textbf{Teorema 4}: Supongamos un operador lineal $\mathbb{A}: \textbf{\em V}^{n} \rightarrow \textbf{\em V}^{n}$, hermítico: $\mathbb{A}=\mathbb{A}^{{\dagger}}$, con autovalores: $\left\{  \lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}\right\}$. Entonces:

\begin{itemize}
\item  Los autovalores $\left\{  \lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}\right\} $ son reales.
\item  Los autovectores $\left\{  \left|  \psi_{1}\right>, \left|  \psi_{2}\right>, \cdots , \left|  \psi
_{n}\right> \right\} $,  correspondientes a cada uno de los autovalores, serán ortogonales. 
\end{itemize}
\end{mdframed}

\textbf{Demostración}:
\begin{itemize}
\item  Para demostrar que los autovalores $\left\{  \lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right\}$ son reales, proyectamos la ecuación de autovalores en cada uno de los autovectores:
\[
\mathbb{A}\left|  \psi\right> =\lambda\left|  \psi\right>
\,\, \Rightarrow \,\,
\left< \psi\right|  \mathbb{A}\left|
\psi\right> =\lambda\left< \psi\right.  \left|  \psi\right>\,.
\]

Ahora bien, dado que $\left< \psi\right.  \left|  \psi\right> $ es real, si demostramos que $\left< \psi\right|  \mathbb{A}\left| \psi\right> $ es real, estará demostrado que $\lambda$ lo será
también. Pero como $\mathbb{A}$ es hermítico:
\[
\left< \psi\right|  \mathbb{A}\left|  \psi\right> ^{\ast}= 
\left< \psi\right|  \mathbb{A}^{\dagger}\left| \psi\right> =
\left< \psi\right|  \mathbb{A}\left|  \psi \right> \,\, \Rightarrow \,\, \left< \psi\right| \mathbb{A}\left|  \psi\right> \in\mathds{R} \,,
\]
por consiguiente los autovalores $\left\{  \lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right\} $ son reales. 
Más aún, si $\mathbb{A}$ es hermítico, y como sus autovalores son reales entonces:
\[
\left< \psi\right|  \mathbb{A}^{\dagger}=\lambda^{\ast}\left< \psi\right|  = 
\lambda\left< \psi\right|  \,\, \Rightarrow \,\, \left< \psi\right|  \mathbb{A}\left| \phi\right> =\lambda\left< \psi\right.  \left|  \phi\right>\,.
\]

\item  Para demostrar que los autovectores $\left\{  \left|  \psi_{1}\right>, \left|  \psi_{2}\right>,\cdots,\left|\psi_{n}\right> \right\} $ son ortogonales, consideremos dos autovectores con sus correspondientes autovalores de tal forma que se cumplen las siguientes ecuaciones:
\[
\mathbb{A}\left|  \psi\right> =\lambda\left|  \psi\right> \qquad \text{y} \qquad \mathbb{A}\left|  \varphi\right> =\mu\left| \varphi\right>\,,
\]
pero como $\mathbb{A}$ es hermítico entonces se cumple que: 
$\left<\varphi\right|  \mathbb{A}=\mu\left< \varphi\right| $, multiplicando a la izquierda por $\left|  \psi\right> $ y a
$\left< \psi\right|  \mathbb{A}=\lambda\left< \psi\right| $ por $\left< \varphi\right| $ a la derecha:
\[
\left.
\begin{array}
[c]{c}
\left(  \left< \varphi\right|  \mathbb{A}=\mu\left< \varphi \right|  \right)  \left|  \psi\right> \\
\\
\left< \varphi\right|  \left(  \mathbb{A}\left|  \psi\right>
=\lambda\left|  \psi\right> \right)
\end{array}
\right\}  \,\, \Rightarrow \,\, \left\{
\begin{array}
[c]{c}
\left< \varphi\right|  \mathbb{A}\left|  \psi\right>
=\mu\left< \varphi\right.  \left|  \psi\right> \\
\\
\left< \varphi\right|  \mathbb{A}\left|  \psi\right> =\left<\varphi\right.  \left|  \psi\right>
\end{array}
\right\} \,\, \Rightarrow \,\, \left(  \lambda-\mu\right)
\left< \varphi\right.  \left|  \psi\right> =0 \,,
\]
y como hemos supuesto que $\lambda\neq\mu$ con lo cual $\left<\varphi\right.  \left|  \psi\right> =0$, los autovectores correspondientes a dos autovalores son ortogonales. $\, \blacktriangleleft$
\end{itemize}

Es importante señalar el hecho de que si la matriz $ \mathbb{A}$ es real, entonces 
$\mathbb{A}= \mathbb{P}\check{\mathbb{A}} \mathbb{P}^T$,  
con $\check{\mathbb{A}}$ diagonal y $\mathbb{P}$ ortogonal: $ \mathbb{P}^T= \mathbb{P}^{-1}$.

En resumen, si  $\mathbb{A}: \textbf{\em V}^{n} \rightarrow \textbf{\em V}^{n}$ es un operador lineal  hermítico entonces:
\begin{itemize}
\item Sus autovalores son reales.
\item Los autovectores correspondientes a cada autovalor son ortogonales. 
\item Los autovectores de $ \mathbb{A}$ resultan ser una base ortonormal del espacio vectorial $\textbf{\em V}^{n}$.
\end{itemize}
\subsubsection{Autoespacios y autovalores degenerados}
\label{Autoespacios}
\index{Autoespacios}
\index{Autovalores degenerados!Matrices Hermíticas}
\index{Operadores Hermíticos!Autovalores degenerados}
Una vez más, consideraremos el caso autovalores degenerados, para operadores hermíticos o autoadjuntos. El siguiente teorema garantiza la existencia de  al menos un subespacio $\textbf{\em S}\left(  \lambda_{0}\right)  \subset \textbf{\em V}^{n}$. 
 
\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema 5}: Sea un operador lineal $\mathbb{A}:\textbf{\em V}^{n}\rightarrow \textbf{\em V}^{n}$, hermítico, con una representación matricial $n \times n$, tal que su polinomio característico $\mathcal{P}\left(  \lambda\right)  =\det\left| A_{j}^{i}-\lambda\delta_{j}^{i}\right|  =0$ tiene al menos una raíz degenerada $\lambda=\lambda_{0}$,  de orden $k\leq n$. Entonces existen $k$ autovectores, no triviales, que cumplen con:
$\mathbb{A}\left|  \psi_{j}\right> =\lambda_{0}\left| \psi_{j}\right>$ para  $j=1,2,\cdots,k$ . 
\end{mdframed}

\textbf{Demostración}:
La demostración también emerge de una variante del \textit{Método de Inducción Completa}. Para ello, probamos que se cumple para $j=1.$ Esta afirmación es obvia. Si existe un $\lambda=\lambda_{0}$  existe un $\left|  {\psi}_{j}\right> $, tal que cumple con la ecuación anterior y es linealmente independiente con él mismo. 

Suponemos que se cumple para $1\leq j=m\leq k$. Es decir,
existen $m$ autovectores $\left|{\psi}_{j}\right> $ de $\mathbb{A}$ 
para el autovalor $\lambda_{0}$. Definamos un subespacio $\textbf{\em S}_{\lambda_{0}}= \textbf{\em S} \left(  \lambda_{0}\right)   \subset \textbf{\em V}^{n}$ donde:
\[
\left|  {\psi}_{j}\right> \in \textbf{\em S}_{\lambda_{0}} \quad /  \quad
\mathbb{A}\left|  {\psi}_{j}\right> =\lambda_{0}\left|
{\psi}_{j}\right> \,\, \Rightarrow \,\, \mathbb{A}\left|
{\psi}_{j}\right> \in \textbf{\em S}_{\lambda_{0}}\quad\text{con }
j=1,2,\cdots,m \,,
\]
por lo tanto, podremos separar $\textbf{\em V}^{n}$ como una suma directa entre el subespacio $\textbf{\em S}_{\lambda_{0}}$ y $\boldsymbol{\mathcal{N}}$, su complemento ortogonal:
\[
\textbf{\em V}^{n}=\textbf{\em S}_{\lambda_{0}}\oplus\boldsymbol{\mathcal{N}}\quad /  \quad
\mathbb{A}\left|  \mathbf{\psi
}_{j}\right> =\lambda_{0}\left|  \mathbf{\psi}_{j}\right>
\quad \wedge \quad \left|  \mathbf{\phi}\right> \in\boldsymbol{\mathcal{N}}
\quad \Rightarrow \quad \left< {\phi}\right.  \left|  {\psi
}_{j}\right> =0 \,,
\]
claramente $\textbf{\em S}_{\lambda_{0}}$ es un subespacio invariante de $\mathbb{A}$ por cuanto su acción se circunscribe dentro del mismo subespacio $\textbf{\em S}_{\lambda_{0}}$. 

Mostraremos que se cumple para operadores hermíticos, por cuanto no es verdad en general, entonces:
\[
\left.
\begin{array}
[c]{c}
\left< \mathbf{\phi}\right.  \left|  \mathbf{\psi}_{j}\right> =0\\
\wedge\\
\mathbb{A}\left|  \mathbf{\psi}_{j}\right> =\lambda_{0}\left|
{\psi}_{j}\right>
\end{array}
\right\}  \,\, \Rightarrow \,\, \left< {\psi}_{j}\right.  \left|
{\phi}\right> =0=\left< {\psi}_{j}\right|
\mathbb{A}^{{\dagger}}\left|  {\phi}\right> =\left<
{\psi}_{j}\right|  \mathbb{A}\left|  {\phi}\right> \,,
\]
de donde se concluye que el vector es ortogonal a $\textbf{\em S}_{\lambda_{0}}$ y por lo tanto está en el complemento ortogonal $\mathbb{A}\left|  {\phi}\right> \in\boldsymbol{\mathcal{N}}$ (por hipótesis $\left|  \mathbf{\phi}\right> \in\boldsymbol{\mathcal{N}}$).  Esto implica que $\boldsymbol{\mathcal{N}}$ también es un espacio invariante del operador hermítico $\mathbb{A}$. 

Entonces, el espacio $\textbf{\em V}^{n}$ puede expresarse como una suma directa de los dos subespacios invariantes respecto al operador lineal $\mathbb{A}$  y su representación matricial en la base de autovectores tendrá la forma de una matriz diagonal a bloques:
\[
\left< \psi^{j}\right|  \mathbb{A}\left|  \psi
_{i}\right> =A_{i}^{j}\rightarrow\left(
\begin{array}
[c]{cccccc}
Q_{1}^{1} & \cdots &  Q_{m}^{1} & 0 & \cdots & 0\\
\vdots & \ddots & \vdots &  & \ddots & \vdots\\
Q_{1}^{m} &  & Q_{m}^{m} & 0 & \cdots & 0\\
0 & \cdots & 0 & 1 & 0 & 0\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
0 & \cdots & 0 & 0 & \cdots & 1
\end{array}
\right)  \left(
\begin{array}
[c]{cccccc}
1 & \cdots & 0 & 0 & \cdots & 0\\
\vdots & \ddots & \vdots & \vdots & \ddots & 0\\
0 & \cdots & 1 & 0 & \cdots & 0\\
0 & \cdots & 0 & R_{m+1}^{m+1} & \cdots &  R_{n}^{m+1}\\
\vdots & \ddots & \vdots & \vdots & \ddots & \vdots\\
0 & \cdots & 0 & R_{m+1}^{n} & \cdots &  R_{n}^{n}
\end{array}
\right) \,,
\]
donde $Q_{\beta}^{\alpha}$ y $R_{\upsilon}^{\mu}$ son matrices $m\times m$ y $\left(n-m\right)  \times\left(n-m\right)$,  respectivamente. La matriz $Q_{\beta}^{\alpha}$ opera en $\textbf{\em S}_{\lambda_{0}}$ mientras que $R_{\upsilon}^{\mu}$ actúa sobre el complemento ortogonal $\boldsymbol{\mathcal{N}}$. 

El polinomio característico de $\mathbb{A}$ puede expresarse como:
\[
\mathcal{P}\left(  \lambda\right)  =\det\left| A_{j}^{i}-\lambda\delta_{j}^{i}\right|  =0 \,\, \Rightarrow \,\, \mathcal{P}\left(  \lambda\right)=\det\left| Q_{j}^{i}-\lambda\delta_{j}^{i}\right|  \det\left|  R_{j}
^{i}-\lambda\delta_{j}^{i}\right|  =0 \,,
\]
y como $\lambda=\lambda_{0}$ es la raíz múltiple del polinomio característico que anula el $\det\left|  Q_{j}^{i}-\lambda\delta_{j} ^{i}\right| $,  tendremos que
\[
\det\left|  Q_{j}^{i}-\lambda_{0}\delta_{j}^{i}\right|  =0 \,\, \Rightarrow \,\,
\mathcal{P}\left(  \lambda\right)  =\left(  \lambda-\lambda_{0}\right)
^{m}\mathcal{F}\left(  \lambda\right)\,,  \quad\text{con } \mathcal{F}\left(
\lambda_{0}\right)  \neq0 \,,
\]
donde $\lambda_{0}$ no es raíz del polinomio $\mathcal{F}\left(
\lambda\right)$.  Ahora bien, para que se cumpla cuando $j=k$, el polinomio característico es
\[
j=k \,\, \Rightarrow \,\, \mathcal{P}\left(  \lambda\right)  =\left(
\lambda-\lambda_{0}\right)  ^{k}\mathcal{R}\left(  \lambda\right)
\,\, \Rightarrow \,\, \left(  \lambda-\lambda_{0}\right)  ^{m}\mathcal{F}\left(\lambda\right)  =\left(  \lambda-\lambda_{0}\right)  ^{k}\mathcal{R}\left( \lambda\right)\,.
\]
Otra vez, $\lambda_{0}$ no es raíz del polinomio $\mathcal{R}\left(\lambda\right)$. La ecuación anterior se cumple para todo $\lambda$, en particular para $\lambda=\lambda_{0}$. Por lo tanto:
\[
1=\left(  \lambda-\lambda_{0}\right)  ^{k-m}\frac{\mathcal{R}\left(
\lambda\right)  }{\mathcal{F}\left(  \lambda\right)  } \,.
\]
Es claro que $\lambda=\lambda_{0}$ obliga a que $k=m$. $\,  \blacktriangleleft$


\subsubsection{Autovalores y autovectores de matrices unitarias}
\label{AutovaloresMatricesUnitarias}
\index{Autovalores y Autovectores!Matrices unitarias}
\index{Matrices Unitarias!Autovalores y Autovectores}
Para finalizar esta sección volvamos a considerar la ecuación de autovalores (\ref{EcAutovalores1}) y la definición de operadores adjuntos que hemos discutido en la sección \ref{OpAdjunto}.
Entonces, si un operador $\mathbb{U}$ es unitario, se cumple que  $\mathbb{U}^{\dag} = \mathbb{U}^{-1}$ entonces si $\left|  \psi_j \right> $ es un autovector, normalizado del operador $\mathbb{U}$, correspondiente a un autovalor $\lambda_j$ tendremos que la norma al cuadrado de $\mathbb{U}\left|  \psi_j \right>$ será igual a:
\[
\mathbb{U}\left|  \psi_j \right> = \lambda_j \left|  \psi_{j}\right>\,\, \Rightarrow \,\, 
\left< \psi^{j}\right|  \mathbb{U}^{\dagger}\mathbb{U}\left|  \psi_{j}\right> =1=
\lambda_j^{\ast}\lambda_j \left< \psi^{j}\right.  \left|  \psi_{j}\right> =\lambda_j^{\ast}\lambda_j \,\, \Rightarrow \,\, \lambda_j={ e}^{i\varphi_{j}}\,,
\]
con $\varphi_{u}$ una función real. 

Con lo cual podemos concluir que, necesariamente, los autovalores de los operadores unitarios serán números complejos de módulo 1. Cuando los autovalores son diferentes, digamos $k \neq j$, entonces esta condición implica que $\left< \psi^{k}\right. \left|  \psi_{j}\right> =0$, con lo cual los autovectores de un operador unitarios son ortogonales.



\subsection{Autovalores y autovectores de matrices similares}
\label{AutovaloresMatricesSimilares}
\index{Autovalores y autovectores de operadores similares}

Como discutimos en la sección \ref{RepresentacionCambioBase}, dos operadores $\tilde{\mathbb{A}}$ y $\mathbb{A}$ que están relacionados por una transformación de similaridad. La ecuación  (\ref{TransformacionSimilaridad}), $\tilde{\mathbb{A}} =\mathbb{S}^{-1}\mathbb{A}\mathbb{S}$, conecta las distintas representaciones matriciales de un mismo operador, las cuales tendrán la misma traza y el mismo determinante, independientemente de su representación matricial. Tal y como se desprende de ecuación  (\ref{TransformaMatricesOperadores}) la representación matricial de los operadores $\mathbb{S}$ corresponde a la matriz de productos internos de los vectores base $\left< \tilde{\mathrm{e}}^{i}\right.  \left| {\mathrm{e}}_{k}\right>$.  

Ahora nos toca identificar otra propiedad fundamental inherente al operador y no a su representación matricial. Para ello complementaremos los teoremas expuestos allá con el siguiente. 

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema 6}: Dos matrices, $A_{l}^{k}$ y $\tilde{A}_{j}^{i},\ n\times n,$ similares tienen el mismo polinomio característico y con ello el mismo conjunto de autovalores.
\end{mdframed}

\textbf{Demostración}: Es inmediato verificar que:
$
\mathbb{\tilde{A}}-\lambda \mathbb{I}=
\mathbb{S}^{-1}\mathbb{A}\mathbb{S} -\lambda\mathbb{I}=
\mathbb{S}^{-1}\left(  \mathbb{A}-\lambda\mathbb{I}\right)  \mathbb{S}\,,
$
y dado que
\[
\det\left|  \mathbb{\tilde{A}}-\lambda \mathbb{I}\right|  =
\det\left| \mathbb{S}^{-1} \left( \mathbb{A} -\lambda \mathbb{I} \right) \mathbb{S}\right|=
\det\left| \mathbb{S}^{-1}\right|  \det \left|\mathbb{A} -\lambda \mathbb{I}\right|  
\det\left| \mathbb{S}\right|=\det \left|\mathbb{A} -\lambda \mathbb{I}\right| \,.
\]
Por lo tanto, ambos operadores $\mathbb{\tilde{A}}$ y $\mathbb{A}$, tendrán el mismo polinomio característico y con ello el mismo conjunto de autovalores. $\,\blacktriangleleft$


Haremos una lista de todas la propiedades para los autovalores y autovectores de operadores, expuestas en ésta y en la sección anterior. 
 
Sea un operador lineal $\mathbb{A}: \textbf{\em V}^{n} \rightarrow \textbf{\em V}^{n}$ con un polinomio característico que tiene $n$ raíces distintas:  $\left\{  \lambda_{1}, \lambda_{2}, \ldots ,\lambda_{n}\right\}$. Entonces tendremos que:
\begin{itemize}
\item  Los diferentes autovectores $\left\{\left|  \psi_{1}\right>, \left|  \psi_{2}\right>, \cdots ,\left|  \psi_{n}\right> \right\}$ correspondientes a los $\left\{  \lambda_{1},\lambda_{2},\ldots,\lambda_{n}\right\} $, formarán una base para $\textbf{\em V}$.

\item  La representación matricial del operador $\left<\psi^{k}\right|  \mathbb{A}\left|  \psi_{m}\right> $ en la base de autovectores  $\left\{  \left|  \psi_{1}\right>, \left|  \psi_{2}\right>, \cdots ,\left|  \psi_{n}\right> \right\}$, será diagonal:
\[
D_{m}^{k}=\left< \psi^{k}\right| \mathbb{A}\left|  \psi_{m}\right> = \mathrm{diag}\left( \lambda_{1}, \lambda_{2}, \ldots,\lambda_{n}\right)\,.
\]

\item  Cualquier otra representación matricial, $\left< u^{k}\right|  \mathbb{A}\left|  u_{m}\right>$, del operador $\mathbb{A}$ en otra base de $\textbf{\em V}$, estará relacionada con la representación diagonal mediante una transformación de similaridad:
\begin{equation}
\label{TransfDiagonal}
\mathbb{D}=\mathbb{S}\mathbb{A}\mathbb{S}^{-1} \quad \Longleftrightarrow \quad\mathrm{diag}\left(  \lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}\right)  =
S_{k}^{i}\left< u^{k}\right|  \mathbb{A}\left|  u_{m}\right>  \left(S^{-1}\right) _{j}^{m} \,,
\end{equation}
donde $S_{j}^{m}$ es una matriz, no singular y por lo tanto invertible, de cantidades que relacionan ambas bases, vale decir la matriz de productos internos entre ambas bases.
\item Considere $\left|\tilde{\phi}_{i}\right>  = \mathbb{U}\left|  \phi_{i}\right> $, es decir un vector transformado con un operador unitario $\mathbb{U}^{\dagger} =\mathbb{U}^{-1}$. Además suponga $\mathbb{A}\left|  \phi_{i}\right> = \lambda_i \left|  \phi_{i}\right> $  entonces $\mathbb{\tilde{A}}\left|  \tilde{\phi}_{i}\right> = \lambda_i \left|\tilde{\phi}_{i}\right>$, con $\mathbb{\tilde{A}}=\mathbb{UAU}^{\dagger}$. Los operadores transformados mediante matrices unitarias tienen los mismos autovalores y autovectores transformados un del otro. 
\end{itemize}



\subsection{Conjunto completo de observables que conmutan}
\index{Conjunto Completo de Observables que conmutan}

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Definición}: Diremos que un operador $\mathbb{A}:\textbf{\em V}^{n}\rightarrow\textbf{\em V}^{n}$ es un \textit{observable} si el conjunto de autovectores $\left\{\left|  {u}_{i (\mu)}\right> \right\} $ de un operador hermítico $\mathbb{A}$, forman una base de $\textbf{\em V}^{n}$. 
\[
\mathbb{A}\left|{u}_{i (\mu)  }\right>
=a_{i}\left|  {u}_{i(\mu)  }\right> \,\, \Rightarrow \,\, 
\left|  {u}_{i(\mu)  }\right>\left< {u}^{i( \mu)}\right|  = 1
\,\, \Longleftrightarrow \,\, \left< {u}^{i(  \mu)}\right|  \left.  {u}_{j( \nu)  }\right>=\delta_{j}^{i}\delta_{\nu}^{\mu}\,,
\]
donde el índice $\mu$ indica el grado de degeneración del autovalor $\lambda_{i}$. 
\end{mdframed}

Un ejemplo trivial de un observable lo constituyen los proyectores
$\mathbb{P}_{\left|  \psi\right> }=\left|  \psi\right> \left< \psi\right|$, con $\left< \psi\right|  \left.
\psi\right> =1$. Claramente, la ecuación de autovalores para un
proyector obliga a que tenga dos autovalores $0$ y $1.$ El autovalor nulo es infinitamente degenerado y está asociado a todos los vectores ortogonales a $\left|  \psi\right> $, mientras que el autovalor $1$ corresponde a un autovalor simple y está asociado a todos los vectores colineales al mismo vector $\left|  \psi\right> $. Esto es:
\[
\mathbb{P}_{\left|  \psi\right> }\left|  \psi\right> =\left|
\psi\right> \quad\text{y}\quad \mathbb{P}_{\left|  \psi\right>
}\left|  \phi\right> =0\quad\text{si }\left< \psi\right|  \left.
\phi\right> =0\,.
\]
Más aún, sea un vector arbitrario $\left|  \varphi\right> \in
\textbf{\em V}^{n}$, siempre se podrá expresar como:
\[
\left|  \varphi\right>  =\mathbb{P}_{\left|  \psi\right>
}\left|  \varphi\right> +\left(  \mathbb{I}-\mathbb{P}_{\left|
\psi\right> }\right)  \left|  \varphi\right> 
\,\, \Rightarrow \,\, \mathbb{P}_{\left|  \psi\right> }
\left(  \left|  \varphi\right>=
\mathbb{P}_{\left|  \psi\right> }\left|  \varphi\right> +\left(
\mathbb{I}-\mathbb{P}_{\left|  \psi\right> }\right)  \left|
\varphi\right> \right)\,,
\]
por lo tanto:
\[
\mathbb{P}_{\left|  \psi\right> }\left|  \varphi\right>  =
\mathbb{P}_{\left|  \psi\right> }\left(  \mathbb{P}_{\left|
\psi\right> }\left|  \varphi\right> \right)  +\left(
\mathbb{P}_{\left|  \psi\right> }-\mathbb{P}_{\left|  \psi\right>
}^{2}\right)  \left|  \varphi\right> =\mathbb{P}_{\left|  \psi
\right> }\left|  \varphi\right> \,\,\Longrightarrow \,\,
\mathbb{P}_{\left|  \psi\right> }\left(  \mathbb{P}_{\left|
\psi\right> }\left|  \varphi\right> \right)  =\mathbb{P}_{\left|
\psi\right> }\left|  \varphi\right> \,,
\]
ya que $\mathbb{P}_{\left|  \psi\right> }^{2}=\mathbb{P}_{\left|
\psi\right> }$, por definición de proyector. Entonces, se deduce que
$\mathbb{P}_{\left|  \psi\right> }\left|  \varphi\right> $ es un
autovector de $\mathbb{P}_{\left|  \psi\right> }$ con autovalor $1.$
Igualmente $\left(  \mathbb{I}-\mathbb{P}_{\left|  \psi\right> }\right)
\left|  \varphi\right> $ es un autovector de $\mathbb{P}_{\left|
\psi\right> }$ con autovalor $0$, y la demostración es inmediata:
\[
\mathbb{P}_{\left|  \psi\right> }\left(  \mathbb{I}-\mathbb{P}_{\left|
\psi\right> }\right)  \left|  \varphi\right> =\left(
\mathbb{P}_{\left|  \psi\right> }-\mathbb{P}_{\left|  \psi\right>
}^{2}\right)  \left|  \varphi\right> =0\,.
\]

Para el caso de autoespacios correspondientes a autovalores degenerados se puede definir un observable $\mathbb{A}$ de la forma:
\[
\mathbb{A}=\sum_{i}a_{i}\mathbb{P}_{i}
\quad\text{con: \ }\mathbb{P}_{i}=\left(\left|  \psi_{\cdot (\mu)  }\right> \left<\psi^{\cdot (\mu)}\right|  \right)  _{i}\quad\text{para: }\quad 
\mu=1,2,\cdots, k \,.
\]

Para los observables que conmutan se tienen los siguientes teoremas:

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema 7}: Si dos operadores lineales $\mathbb{A}$ y
$\mathbb{B}$,  hermíticos, conmutan, $\left[  \mathbb{A}
,\mathbb{B}\right]  =0$, y $\left|  \psi\right> $ es autovector de
$\mathbb{A}$ con autovalor $\sigma$, entonces $\mathbb{B}\left|  \psi\right>
$ también será autovector de $\mathbb{A}$ con el mismo autovalor $\sigma$.
\end{mdframed}

\textbf{Demostración}: La demostración es sencilla:
\[
\mathbb{A}\left|  \psi\right> =a\left|  \psi\right> \,\, \Rightarrow \,\, 
\mathbb{B}\left(  \mathbb{A}\left|  \psi\right> =\sigma \left|
\psi\right> \right) \,\, \Rightarrow \,\, \mathbb{BA}\left|
\psi\right> =\mathbb{A}\left(  \mathbb{B}\left|  \psi\right>\right)  =\sigma\left(  \mathbb{B}\left|  \psi\right> \right) \,. \quad \blacktriangleleft
\]

Ahora bien, de esta situación se puede distinguir un par de casos:
\begin{itemize}
\item  si el autovalor $\sigma$ es no degenerado los autovectores asociados con este autovalor son, por definición, colineales con $\left|  \psi\right> $. Por lo tanto $\mathbb{B}\left|  \psi\right>$,  será
necesariamente colineal con $\left|  \psi\right>$. La conclusión a
esta afirmación es que NECESARIAMENTE $\left|  \psi\right> $ es
autovector de $\mathbb{B}$.

\item  si el autovalor $\sigma$ es degenerado, $\mathbb{B}\left|  \psi\right> \in \textbf{\em S}_{\sigma}$, es decir $\mathbb{B}\left|  \psi\right> $ está en el autoespacio $\textbf{\em S}_{\sigma}$ con lo cual 
$\textbf{\em S}_{\sigma}$ es globalmente  invariante bajo la acción de $\mathbb{B}$.
\end{itemize}

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema 8}: Si dos observables $\mathbb{A}$ y $\mathbb{B}$
conmutan, $\left[  \mathbb{A},\mathbb{B}\right]  =0,$ y si $\left|  \psi
_{1}\right> $ y $\left|  \psi_{2}\right> $ son autovectores de
$\mathbb{A}$ para autovalores distintos, entonces el elemento de matriz $\left< \psi^{1}\right|  \mathbb{B}\left|  \psi_{2}\right> =0$
\end{mdframed}

\textbf{Demostración}: Si $\mathbb{A}\left|  \psi_{1}\right>
=\sigma_{1}\left|  \psi_{1}\right> \quad$y$\quad\mathbb{A}\left|  \psi
_{2}\right> =\sigma_{2}\left|  \psi_{2}\right> $ entonces:
\begin{align*}
0  &  =\left< \psi^{1}\right|  \left[  \mathbb{A},\mathbb{B}\right]
\left|  \psi_{2}\right> =\left< \psi^{1}\right|  \mathbb{AB-BA}
\left|  \psi_{2}\right> =\left(  \left< \psi^{1}\right|
\mathbb{A}\right)  \mathbb{B}\left|  \psi_{2}\right> -\left<
\psi^{1}\right|  \mathbb{B}\left(  \mathbb{A}\left|  \psi_{2}\right> \right) \\
&  =\sigma_{1}\left< \psi^{1}\right|  \mathbb{B}\left|  \psi_{2}\right>
-\sigma_{2}\left< \psi^{1}\right|  \mathbb{B}\left|  \psi_{2}\right>
=\left(  \sigma_{1}-\sigma_{2}\right)  \left< \psi^{1}\right|  \mathbb{B}\left|\psi_{2}\right> \,\, \Rightarrow \,\, \left< \psi^{1}\right|
\mathbb{B}\left|  \psi_{2}\right> =0 \,. \, \blacktriangleleft
\end{align*}

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema 9}: Si dos observables $\mathbb{A}$ y $\mathbb{B}$, 
son hermíticos, y conmutan, $\left[ \mathbb{A},\mathbb{B}\right]=0$, los autovectores $\left\{  \left|  \psi_{i}\right> \right\} $ comunes a $\mathbb{A}$ y $\mathbb{B}$ constituyen una base ortonormal para $\textbf{\em V}^{n}$.
\end{mdframed}

\textbf{Demostración}: Denotemos los autovectores de $\mathbb{A}$
como $\left|  \psi_{i\left(  \mu\right)  }\right>$, de tal modo que:
\[
\mathbb{A}\left|  \psi_{i \left(  \mu\right)  }\right> =\sigma_{i}\left|
\psi_{i \left(  \mu\right)  }\right> \quad\text{donde }i=1,2,..,n-k_{n} +1
\quad\text{y }\quad \mu=1,2,..,k_{n}\,,
\]
$k_{n}$ indica el orden de la degeneración de un determinado autovalor $\sigma_{n}$. 

Dado que $\mathbb{A}$ es un observable, los 
$\left| \psi_{i\left(\mu\right)}\right> $ forman una base, claramente:
\[
\left< \psi^{i \left(  \mu\right)  }\right.  \left|  \psi_{j \left(
\nu\right)  }\right> =\delta_{j}^{i}\delta_{\nu}^{\mu}\,.
\]

Dado que los elementos de la matriz $\left< \psi^{i \left(\nu\right)}\right|  \mathbb{B}\left|  \psi_{j \left(  \nu\right)  }\right>=\delta_{j}^{i}$, entonces esto quiere decir que los elementos 
$B_{j \left(  \nu\right)  }^{i \left(\mu\right)}=\left<\psi^{i \left(  \mu\right)  }\right|  \mathbb{B}\left|  \psi_{j \left(\nu\right)  }\right>$ serán nulos para $i\neq j$, pero no podemos decir nada
\textit{a priori} para el caso $\mu\neq\upsilon$ y $i=j$.  En general, al ordenar la base:
\[
\left|  \psi_{1 \left(  1\right)}\right>, \left|  \psi_{1 \left(2\right)}\right>, \cdots 
\left|  \psi_{1 \left(  k_{1}\right)}\right>, \left|  \psi_{2 \left(  1\right)  }\right> ,
\left| \psi_{2 \left(  2\right)  }\right> ,\cdots,\left|  \psi_{2 \left(k_{2}\right)  }\right>, \cdots ,
\left|  \psi_{3 \left(  1\right)}\right> ,\cdots\left|  \psi_{n-k_{n} \left(  1\right)  }\right>\,,
\]
para el caso que consideraremos será:
\[
\left|  \psi_{1 \left(  1\right)  }\right> ,\left|  \psi_{1 \left(
2\right)  }\right> ,\left|  \psi_{1 \left(  3\right)  }\right>
,\left|  \psi_{2 \left(  1\right)  }\right> ,\left|  \psi_{2 \left(
2\right)  }\right> ,\left|  \psi_{3 \left(  1\right)  }\right>
,\left|  \psi_{4 \left(  1\right)  }\right> ,\left|  \psi_{4 \left(
2\right)  }\right> ,\left|  \psi_{5 \left(  1\right)  }\right> \,. 
\]

La representación matricial de $\mathbb{B}$ en esa base, $\left<\psi^{i \left(\mu\right)  }\right|  \mathbb{B}\left|  \psi_{j \left( \nu\right)  }\right>$,  tendrá la forma de una matriz diagonal a
bloques:
\[
\left(
\begin{tabular}
[c]{ccccccccc}\cline{1-3}
\multicolumn{1}{|c}{$B_{1\ \left(  1\right)  }^{1\ \left(  1\right)  }$} &
$B_{1\ \left(  2\right)  }^{1\ \left(  1\right)  }$ & $B_{1\ \left(  3\right)
}^{1\ \left(  1\right)  }$ & \multicolumn{1}{|c}{$0$} & $0$ & $0$ & $0$ & $0$
& $0$\\
\multicolumn{1}{|c}{$B_{1\ \left(  1\right)  }^{1\ \left(  2\right)  }$} &
$B_{1\ \left(  2\right)  }^{1\ \left(  2\right)  }$ & $B_{1\ \left(  3\right)
}^{1\ \left(  2\right)  }$ & \multicolumn{1}{|c}{$0$} & $0$ & $0$ & $0$ & $0$
& $0$\\
\multicolumn{1}{|c}{$B_{1\ \left(  1\right)  }^{1\ \left(  3\right)  }$} &
$B_{1\ \left(  2\right)  }^{1\ \left(  3\right)  }$ & $B_{1\ \left(  3\right)
}^{1\ \left(  3\right)  }$ & \multicolumn{1}{|c}{$0$} & $0$ & $0$ & $0$ & $0$
& $0$\\\cline{1-5}
$0$ & $0$ & $0$ & \multicolumn{1}{|c}{$B_{2\ \left(  1\right)  }^{2\ \left(
1\right)  }$} & \multicolumn{1}{c|}{$B_{2\ \left(  2\right)  }^{2\ \left(
1\right)  }$} & $0$ & $0$ & $0$ & $0$\\
$0$ & $0$ & $0$ & \multicolumn{1}{|c}{$B_{2\ \left(  1\right)  }^{2\ \left(
2\right)  }$} & \multicolumn{1}{c|}{$B_{2\ \left(  2\right)  }^{2\ \left(
2\right)  }$} & $0$ & $0$ & $0$ & $0$\\\cline{4-6}
$0$ & $0$ & $0$ & $0$ & $0$ & \multicolumn{1}{|c}{$B_{3\ \left(  1\right)
}^{3\ \left(  1\right)  }$} & \multicolumn{1}{|c}{$0$} & $0$ & $0$
\\\cline{6-8}
$0$ & $0$ & $0$ & $0$ & $0$ & $0$ & \multicolumn{1}{|c}{$B_{4\ \left(
1\right)  }^{4\ \left(  1\right)  }$} & $B_{4\ \left(  2\right)  }^{4\ \left(
1\right)  }$ & \multicolumn{1}{|c}{$0$}\\
$0$ & $0$ & $0$ & $0$ & $0$ & $0$ & \multicolumn{1}{|c}{$B_{4\ \left(
1\right)  }^{4\ \left(  2\right)  }$} & $B_{4\ \left(  2\right)  }^{4\ \left(
2\right)  }$ & \multicolumn{1}{|c}{$0$}\\\cline{7-9}
$0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ &
\multicolumn{1}{|c|}{$B_{5\ \left(  1\right)  }^{5\ \left(  1\right)  }$
}\\\cline{9-9}
\end{tabular}
\right)  
\]

Tal y como hemos mencionado los subespacios: $\boldsymbol{\mathcal\mathrm{E}}_{1},\boldsymbol{\mathcal\mathrm{E}}_{2}$, y $\boldsymbol{\mathcal\mathrm{E}}_{4}$ 
corresponden a los autovalores
degenerados $\sigma_{1}, \sigma_{2},$ y $\sigma_{4}$ (de orden $3, 2$ y $2$ respectivamente).

Una vez más surgen dos casos a analizar:
\begin{itemize}
\item  Si $\sigma_{n}$ es un autovalor no degenerado, entonces existe un único autovector asociado a este autovalor (la dimensión del autoespacio es $1 \rightarrow k_{j}=1$, y no hace falta). 
Esto corresponde al ejemplo hipotético anterior para los autovalores simples $\sigma_{3},$ y $\sigma_{5}$.

\item  Si $\sigma_{n}$ es un autovalor degenerado, entonces existe un conjunto de autovectores asociados a este autovalor  $\sigma_{n}$ (en este caso la dimensión del autoespacio es $k_{n}$). Como los $\left|  \psi_{j \left(  \mu\right) }\right> $ son autovectores de $\mathbb{A}$ su representación matricial será diagonal a bloques. Ahora bien, como el autoespacio
$\textbf{\em S}_{a}$ es globalmente invariante bajo la acción de $\mathbb{B}$ y
$B_{j (\mu)}^{i\ (\mu)}=$ $\left< \psi^{i\left(\mu\right)}\right|  \mathbb{B}\left|  \psi_{j \left(\mu\right)}\right> $ es
hermítico, por ser $\mathbb{B}$ hermítico, entonces $\mathbb{B}$ es diagonalizable dentro del bloque que la define. Es decir, se podrá conseguir una base $\left|  \chi_{j \left(  \mu\right)  }\right> $ tal que la representación matricial de $\mathbb{B}$ en esa base es diagonal
\[
B_{j}^{i (\mu)}=\left< \psi^{i (\mu)}\right|  \mathbb{B}\left|
\psi_{j (\mu)}\right> \,\, \Longrightarrow \,\, 
\left< \chi^{i \left(\mu\right)  }\right|  \mathbb{B}\left|  \chi_{j (\mu)}\right> =\tilde{B}_{j (\mu)}^{i (\mu)}=\beta_{j (\mu)}\delta_{j}^{i}\,,
\]
que no es otra cosa que los vectores $\left|  \chi_{j (\mu)}\right> $
serán autovectores de $\mathbb{B}$
\[
\mathbb{B}\left|  \chi_{j (\mu)}\right> =\beta_{j (\mu)}\left|
\chi_{j (\mu)}\right> \,. \,\blacktriangleleft
\]
\end{itemize}

Es importante recalcar que los autovectores $\left|  \psi_{j (\mu
)}\right> $ de $\mathbb{A}$ asociados con un autovalor degenerado NO son necesariamente autovectores de $\mathbb{B}$.  Sólo que como $\mathbb{B}$ es hermítico puede ser diagonalizado dentro del autoespacio.

De ahora en adelante denotaremos los autovectores comunes a dos operadores  $\mathbb{A}$ y $\mathbb{B}$ con distintos autovalores como $\left|  u_{\left. i\right|  j (\mu)}\right> $ tal que
\[
\mathbb{A}\left|  u_{\left.  n\right|  m (\mu)}\right> =\sigma_{n}\left|
u_{\left.  n\right|  m (\mu)}\right> \qquad\text{y}\qquad
\mathbb{B}\left|  u_{\left.  n\right|  m (\mu)}\right> =\beta_{m}\left|
u_{\left.  n\right|  m (\mu)}\right>\,,
\]
donde hemos dejado ``espacio'' para permitir la degeneración la cual será indicada por el índice $\mu$.

La prueba del inverso del teorema anterior es bien simple.

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Teorema 10}: Si existe una base de autovectores $\left\{  \left|
u_{j \left(  \mu\right)  }\right> \right\} $ comunes a $\mathbb{A}$ y
$\mathbb{B,}$ entonces $\mathbb{A}$ y $\mathbb{B}$ conmutan, $\left[
\mathbb{A},\mathbb{B}\right]  =0$.
\end{mdframed}

\textbf{Demostración}: Es claro que:
\begin{align*}
\mathbb{AB}\left|  u_{\left.  n\right|  m (\mu)}\right>  &
=\beta_{m}\mathbb{A}\left|  u_{\left.  n\right|  m (\mu)}\right>
=\beta_{m}\sigma_{n}\left|  u_{\left.  n\right|  m (\mu)}\right> \,, \\
\mathbb{BA}\left|  u_{\left.  n\right|  m (\mu)}\right>  &
=\sigma_{n}\mathbb{B}\left|  u_{\left.  n\right|  m (\mu)}\right>
=\sigma_{n}\beta_{m}\left|  u_{\left.  n\right|  m (\mu)}\right>\,,
\end{align*}
restando miembro a miembro obtenemos de manera inmediata
\[
\left(  \mathbb{AB-BA}\right)  \left|  u_{\left.  n\right|  m (\mu
)}\right> =\left[  \mathbb{A,B}\right]  \left|  u_{\left.  n\right|
m (\mu)}\right> =\left(  \beta_{m}\sigma_{n}-\sigma_{n}\beta_{m}\right)  \left| u_{\left.  n\right|  m (\mu)}\right> =0 \,. \, \blacktriangleleft
\]

\begin{mdframed}[linecolor=OliveGreen,linewidth=0.3mm]
\textbf{Definición}: Los operadores: $\left\{ \mathbb{A,B,C,D\cdots}\right\}  $
constituye un conjunto completo de observables que conmutan si:

\begin{enumerate}
\item  Los operadores del conjunto conmutan entre ellos:
\[
\left[  \mathbb{A,B}\right]  =\left[  \mathbb{A,C}\right]  =\left[
\mathbb{A,D}\right]  =\left[  \mathbb{B,C}\right]  =\left[  \mathbb{B,D}
\right]  =\left[  \mathbb{C,D}\right]  =\mathbf{\cdots}=0
\]

\item  Al determinar el conjunto de autovalores para los operadores
\[
\left\{  \alpha_{n}, \beta_{m}, \gamma _{k}, \delta_{l},{\cdots}\right\}
\] 
se especifica \textbf{de manera unívoca} un único autovector común a todos estos operadores
\[
\left\{  \alpha_{n}, \beta_{m}, \gamma_{k}, \delta_{l},{\cdots}\right\} 
\,\, \Rightarrow \,\,
\left|  u_{\left.  n\right|  m\left|  k\right|  l{\cdots}\ (\mu)}\right> \,. 
\]
\end{enumerate}
\end{mdframed}


\subsection{{\color{Fuchsia}Ejemplos}}

\begin{enumerate}
\item Consideremos una vez más la transformación $\mathbb{D} =\mathbb{S}^{-1}\mathbb{A}\mathbb{S}$, descrita en la ecuación (\ref{TransfDiagonal}), donde $\mathbb{D}$ es un operador diagonal cuya representación matricial será 
\[
\mathbb{D}=\mathbb{S}^{-1}\mathbb{A}\mathbb{S} \quad \Longleftrightarrow \quad
\mathrm{diag}\left(  \lambda_{1}, \lambda_{2}, \ldots, \lambda_{n}\right)  \equiv
\lambda_{j}\delta^{i}_{j} = 
\left(S^{-1}\right)_{k}^{i} A^{k}_{m} S_{j}^{m} ,
\]con $A^{k}_{m} = \left< u^{k}\right|  \mathbb{A}\left|  u_{m}\right> $. Tal y como hemos comentado en la sección \ref{autovectoresmatricesimportantes}, debe haber restricciones sobre el operador $\mathbb{A}$ para que sea diagonalizable. En particular, allí pudimos comprobar que si $\mathbb{A}$ era hermítico, su representación matricial en la base de autovectores era diagonal. Entonces para este caso supondremos que $\mathbb{A}$ es diagonalizable mediante la transformación (\ref{TransfDiagonal}). Claramente esto implica que 
\[
\mathbb{D}=\mathbb{S}^{-1}\mathbb{A}\mathbb{S}  \quad \Rightarrow 
\mathbb{S}\mathbb{D} = \mathbb{A}\mathbb{S} \quad \Longleftrightarrow \quad
S^{i}_{m}\lambda_{j}\delta^{m}_{j}  =  A^{i}_{m}S_{j}^{m} \quad \Rightarrow 
A^{i}_{m}S_{j}^{m} = \lambda_{j} S^{i}_{j} ,
\] y esta última ecuación sugiere una ecuación de autovalores fijando un valor particular para $j$. Esto es:
\[
A^{i}_{m}S_{1}^{m} = \lambda_{1} S^{i}_{1}; \quad 
A^{i}_{m}S_{2}^{m} = \lambda_{2} S^{i}_{2}; \quad 
A^{i}_{m}S_{3}^{m} = \lambda_{3} S^{i}_{3}; \quad \cdots
A^{i}_{m}S_{j}^{m} = \lambda_{j} S^{i}_{j} \cdots \quad
A^{i}_{m}S_{n}^{m} = \lambda_{n} S^{i}_{n}.
\] Cada una de estas ecuaciones es una ecuación de autovalores para autovectores $S^{i}_{1}, S^{i}_{2}, S^{i}_{3}, \cdots S^{i}_{n}$. Vale decir la matriz de transformación $S_{j}^{m}$ está construida por columnas de autovectores.  Con lo cual al resolver la ecuación de autovalores para la matriz  $\mathbb{A}$ es inmediato construir la matriz de transformación $\mathbb{S}$ a partir de los autovectores de $\mathbb{A}$. 

\item Para ejemplificar numéricamente el ejemplo anterior, consideremos la siguiente matriz real y simétrica:
\[
 \mathbb{A}=
\left(\begin{array}{ccc}
1 & 0 & 3 \\
0 & -2 & 0 \\
3 & 0 & 1
\end{array}\right) \, .
\]

Los autovalores y autovectores para $ \mathbb{A}$ son respectivamente:
\[
P(\lambda)=-(\lambda-4)(\lambda+2)^2=0 \,\, \Rightarrow \,\,
\lambda_1= 4 \,, \, \lambda_2=-2 \,, \,  \lambda_3= -2 \,,
\]
\[
\left|  u_{1}\right>=
\left(\begin{array}{c}
1  \\
0  \\
1 
\end{array}\right) \,, \,\, 
\left|  u_{2}\right> =
\left(\begin{array}{c}
0  \\
1  \\
0 
\end{array}\right) \,, \,\, 
\left|  u_{3}\right> =
\left(\begin{array}{c}
-1  \\
0  \\
1 
\end{array}\right) \,.
\]

El lector debe verificar que este conjunto de vectores son mutuamente ortogonales porque claramente la matriz es simétrica. Por lo tanto, la matriz $ \mathbb{A}$ se puede diagonalizar a través de la transformación $ \mathbb{C}^{-1} \mathbb{A} \mathbb{C}$, donde $ \mathbb{C}$ se construye con los autovectores normalizados de $ \mathbb{A}$ como columnas.
\[
 \mathbb{C}=
\frac{1}{\sqrt{2}}
\left(\begin{array}{ccc}
1 & 0 & -1 \\
0 & \sqrt{2} & 0 \\
1 & 0 & 1
\end{array}\right)\,.
\]
Entonces, a pesar que los autovalores de $ \mathbb{A}$ son degenerados ($\lambda= 4, -2, -2$) sus tres autovectores son linealmente independientes, y se tiene que:
\[
 \mathbb{C}^{-1} \mathbb{A} \mathbb{C}=\frac{1}{{2}}
\left(\begin{array}{ccc}
1 & 0 & 1 \\
0 & \sqrt{2} & 0 \\
-1 & 0 & 1
\end{array}\right)
\left(\begin{array}{ccc}
1 & 0 & 3 \\
0 & -2 & 0 \\
3 & 0 & 1
\end{array}\right)
\left(\begin{array}{ccc}
1 & 0 & -1 \\
0 & \sqrt{2} & 0 \\
1 & 0 & 1
\end{array}\right)=
\left(\begin{array}{ccc}
4 & 0 & 0 \\
0 & -2 & 0 \\
0 & 0 & -2
\end{array}\right) \,.
\]


\item Dada la siguiente matriz:
\[
 \mathbb{A}=
\left(\begin{array}{ccc}
2 & 2 & -2 \\
2 & -1 & 4 \\
-2 & 4 & -1 
\end{array}\right)\,,
\]
Aquí también se cumple que $\mathbb{A}=\mathbb{A}^T$, con lo cual se puede diagonalizar. Calculemos nuevamente una matriz $ \mathbb{C}$ que permita diagonalizar a la matriz $\mathbb{A}$. 

Primeramente procedemos a calcular los autovalores de $ \mathbb{A}$.
\[
P(\lambda)= 
\left|\begin{array}{ccc}
2-\lambda  & 2 & -2 \\
2 & -1-\lambda & 4 \\
-2 & 4 & -1-\lambda 
\end{array}\right| = -(\lambda-3)^2(\lambda+6)=0\,.
\]
\begin{itemize}
\item Para $\lambda=-6$:
 \[
\left(\begin{array}{ccc}
8 & 2 & -2 \\
2 & 5 & 4 \\
-2 & 4 & 5 
\end{array}\right)
 \left(\begin{array}{r}
 x^1 \\ 
 x^2 \\
 x^3
\end{array}\right) = -6
\left(\begin{array}{r} x^1 \\ x^2 \\ x^3 
\end{array}\right) \,\, \Rightarrow \,\, 
\left\{
\begin{array}{r}
 14x^1+2x^2-2x^3=0 \\ 
 2x^1+11x^2+4x^3 = 0 \\
  -2x^1+4x^2 +11x^3=0
\end{array}
\right.
\]
Un autovector puede ser:
\[
\left| u_1 \right\rangle  =
\left(
\begin{array}{r}
1 \\ 
-2 \\
 2
\end{array}
\right)\,\, \Rightarrow \,\,
\left| \hat{u}_1 \right\rangle  =
\frac{1}{3}\left(
\begin{array}{r}
1 \\ 
-2 \\
 2
\end{array}
\right) \,.
\]
\item Para el autovalor degenerado $\lambda=3$, tenemos:
 \[
\left(\begin{array}{ccc}
-1 & 2 & -2 \\
2 & -4 & 4 \\
-2 & 4 & -4 
\end{array}\right)
 \left(\begin{array}{r}
 x^1 \\ 
 x^2 \\
 x^3
\end{array}\right) = 3
\left(\begin{array}{r} x^1 \\ x^2 \\ x^3 
\end{array}\right) \,\, \Rightarrow \,\, 
\left\{
\begin{array}{r}
 -4x^1+2x^2-2x^3=0 \\ 
  2x^1-7x^2+4x^3 = 0 \\
  -2x^1+4x^2 -7x^3=0
\end{array}
\right.
\]
En este caso podemos tomar como autovectores:
\[
\left| u_2 \right\rangle  =
\left(
\begin{array}{c}
1 \\ 
0 \\
-1/2
\end{array}
\right)\,\, \Rightarrow \,\,
\left| \hat{u}_2 \right\rangle  =
\frac{2}{\sqrt{5}}\left(
\begin{array}{c}
1 \\ 
0 \\
-1/2
\end{array}
\right)\,,\,\,
\left| u_3 \right\rangle  =
\left(
\begin{array}{r}
0 \\ 
1 \\
1
\end{array}
\right)\,\, \Rightarrow \,\,
\left| \hat{u}_3 \right\rangle  =
\frac{1}{\sqrt{2}}\left(
\begin{array}{r}
0 \\ 
1 \\
1
\end{array}
\right) \,.
\]
\end{itemize}

El lector debe comprobar que en este caso los autovectores NO son mutuamente ortogonales. 
Construimos la matriz $ \mathbb{C}$:
\[
 \mathbb{C}=
\begin{pmatrix}\frac{1}{3} & \frac{2}{\sqrt{5}} & 0 \\ 
-\frac{2}{3}& 0 & \frac{1}{\sqrt{2}} \\ 
\frac{2}{3} & -\frac{1}{\sqrt{5}} & \frac{1}{\sqrt{2}} \\ 
\end{pmatrix} \,.
\]
De manera que:
\[
 \mathbb{C}^{-1} \mathbb{A} \mathbb{C}=
\begin{pmatrix}\frac{1}{3} & -\frac{2}{3} & \frac{2}{3} \\ 
\frac{4\,\sqrt{5}}{9} & \frac{\sqrt{5}}{9} & -\frac{\sqrt{5}}{9} \\ 
\frac{2\sqrt{2}}{9} & \frac{5\,\sqrt{2}}{9} & 
\frac{4\sqrt{2}}{9} \\ \end{pmatrix}
\left(\begin{array}{ccc}
2 & 2 & -2 \\
2 & -1 & 4 \\
-2 & 4 & -1 
\end{array}\right)
\begin{pmatrix}\frac{1}{3} & \frac{2}{\sqrt{5}} & 0 \\ 
-\frac{2}{3}& 0 & \frac{1}{\sqrt{2}} \\ 
\frac{2}{3} & -\frac{1}{\sqrt{5}} & \frac{1}{\sqrt{2}} \\ 
\end{pmatrix}=
\left(\begin{array}{ccc}
-6 & 0 & 0 \\
0 & 3 & 0 \\
0 & 0 & 3 
\end{array}\right)\,.
\]


\item La siguiente matriz es un ejemplo de una matriz hermítica o autoadjunta:
\[
\mathbb{A}=
\left(\begin{array}{ccc}
1 & -1+2i & i \\
-1-2i & 2  & -1 \\
-i & -1 & 3
\end{array}\right) \,\, \Rightarrow \,\,
\mathbb{A}^{\dagger}=\mathbb{A}
\]

Los autovalores de esta matriz son:
\[
P(\lambda)=-\left(\lambda-4\right)\,\left(\lambda^2-2\,\lambda-4\right)=0 \,\, \Rightarrow \,\,
\lambda_1= 4\,, \,  \lambda_2= 1+\sqrt{5} \,, \, \lambda_3= 1-\sqrt{5} \,.
\]

Los tres autovalores {\it reales} generan los siguientes autovectores:
\[
\lambda_1= 4 \,\, \Rightarrow \,\,
\left| u_1\right>=
\left(
\begin{array}{c}
1   \\ 
-1-i  \\
1
\end{array}
\right)\,,
\]
\[
\lambda_2= 1+\sqrt{5} \,\, \Rightarrow \,\,
\left| u_2\right>=
\frac13\left(
\begin{array}{c}
3   \\ 
1-\sqrt{5}\,i  \\
-\sqrt{5}-2 - \left(1+\sqrt{5}\right)i
\end{array}
\right)\,,\quad
\lambda_3= 1-\sqrt{5} \,\, \Rightarrow \,\,
\left| u_3\right>=
\frac13\left(
\begin{array}{c}
3   \\ 
{1+\sqrt{5}\,i}  \\
{\sqrt{5}-2-\left(1- \sqrt{5}\right)i}
\end{array}
\right)\,.
\]
Estos vectores normalizados son:
\begin{eqnarray*}
\left| \hat{u}_1\right>=
\frac{\left| {u}_1\right>}{\sqrt{\left< {u}_1\right. \left| {u}_1 \right>}}=
\frac{1}{2}
\left(
\begin{array}{c}
1   \\ 
-1-i  \\
1
\end{array}
\right)\,, \quad 
\left| \hat{u}_2\right>=
\frac{\left| {u}_2\right>}{\sqrt{\left< {u}_2\right. \left| {u}_2 \right>}}=
{\frac {3}{\sqrt {30+6\,\sqrt {5}}} }
\left(
\begin{array}{c}
3   \\ 
1-\sqrt{5}\,i  \\
-\sqrt{5}-2 - \left(1+\sqrt{5}\right)i
\end{array}
\right)\,,
\end{eqnarray*}
\[
\left| \hat{u}_3\right>=
\frac{\left| {u}_3\right>}{\sqrt{\left< {u}_3\right. \left| {u}_3 \right>}}=
{\frac {3}{\sqrt {30-6\,\sqrt {5}}}}
 \left(
\begin{array}{c}
3   \\ 
{1+\sqrt{5}\,i}  \\
{\sqrt{5}-2-\left(1- \sqrt{5}\right)i}
\end{array}
\right)\,.
\]

Se puede demostrar que los autovectores son ortogonales:
\begin{eqnarray*}
\left< {u}_1\right. \left| {u}_2 \right> &=& 
({u}_1)^\dagger{u}_2 =0 \,\, \Rightarrow \,\,
\left(
\begin{array}{ccc}
1 & -1+i & 1
\end{array}
\right)
\left(
\begin{array}{c}
3   \\ 
1-\sqrt{5}\,i  \\
-\sqrt{5}-2 - \left(1+\sqrt{5}\right)i
\end{array}
\right) =0 \\
\left< {u}_1\right. \left| {u}_3 \right> &=&
({u}_1)^\dagger{u}_3=0 \,\, \Rightarrow \,\,
\left(
\begin{array}{ccc}
1 & -1+i & 1
\end{array}
\right)
\left(
\begin{array}{c}
3   \\ 
1+\sqrt{5}\,i  \\
\sqrt{5}-2 - \left(1-\sqrt{5}\right)i
\end{array}
\right) =0 \\
\left< {u}_2\right. \left| {u}_3 \right> &=&
({u}_2)^\dagger{u}_3=0 \,\, \Rightarrow \,\,
\left(
\begin{array}{ccc}
3 & 1+\sqrt{5}\,i & -\sqrt{5}-2 + \left(1+\sqrt{5}\right)i
\end{array}
\right)
\left(
\begin{array}{c}
3   \\ 
1+\sqrt{5}\,i  \\
\sqrt{5}-2 - \left(1-\sqrt{5}\right)i
\end{array}
\right) =0 
\end{eqnarray*}

La matriz $ \mathbb{A}$ es entonces diagonalizable si construimos la siguiente matriz a partir de los autovectores normalizados:
\[
 \mathbb{P}= 
\left(
\begin{array}{ccc}
\frac12 & \frac{9}{{\sqrt {30+6\,\sqrt {5}}}} & \frac{9}{{\sqrt {30-6\,\sqrt {5}}}} \\
\\
\frac{-1-i}{2}  & \frac{3\left(1-\sqrt{5}\,i\right)}{{\sqrt {30+6\,\sqrt {5}}}} & \frac{3\left(1+\sqrt{5}\,i\right)}{{\sqrt {30-6\,\sqrt {5}}}}  \\
\\
\frac12 & \frac{3\left(-\sqrt{5}-2 - \left(1+\sqrt{5}\right)i\right)}{{\sqrt {30+6\,\sqrt {5}}}} & \frac{3\left(\sqrt{5}-2 - \left(1-\sqrt{5}\right)i\right)}{{\sqrt {30-6\,\sqrt {5}}}}
\end{array}
\right)\,.
\]

Si elegimos la matriz diagonal a partir de los autovalores:
\[
\mathbb{D}= 
\left(
\begin{array}{ccc}
4 & 0 & 0 \\
0  &1+\sqrt {5} & 0  \\
0 &0 & 1-\sqrt {5}
\end{array}
\right)\,,
\]
entonces resulta que es posible factorizar la matriz $ \mathbb{A}$ de la siguiente forma (Para el lector se deja la comprobación)
\[
 \mathbb{A}= \mathbb{P} \mathbb{D} \mathbb{P}^\dagger\,.
\]


\item Dada la matriz:
\[
 \mathbb{A}=
\left( \begin {array}{ccc} 
\frac{1}{\sqrt{2}}&\frac{1}{\sqrt{2}}&0\\ 
\\
-\frac{i}{\sqrt{2}}&\frac{i}{\sqrt{2}}&0\\ 
\\
0 &0&i
\end {array} \right)\,.
\]

Esta matriz es unitaria, ya que: $ \mathbb{A}^\dagger= \mathbb{A}^{-1}$.

Los autovalores se obtienen de la manera usual:
\[
P(\lambda)=\left(\lambda-i\right)\left[2\,\lambda^2-\sqrt{2}(1+i)\lambda+2\,i\right]=0 \,,
\]
es decir: 
\[
\lambda_1= \frac{\sqrt{2}(1+i)-2\,\sqrt{-3i}}{4}\,, \, 
\lambda_2= \frac{\sqrt{2}(1+i)+2\,\sqrt{-3i}}{4} \,, \,
\lambda_3= i \,.
\]

Notemos que los valores propios están normalizados a la unidad:
\begin{eqnarray*}
\lambda_1\lambda_1^*&=&\left(\frac{\sqrt{2}(1+i)-2\,\sqrt{-3i}}{4}\right)\left(\frac{\sqrt{2}(1-i)-2\,\sqrt{3i}}{4}\right)= 1\\
\lambda_2\lambda_2^*&=&\left(\frac{\sqrt{2}(1+i)+2\,\sqrt{-3i}}{4}\right)\left(\frac{\sqrt{2}(1-i)+2\,\sqrt{3i}}{4}\right)= 1\\
\lambda_3\lambda_3^*&=& i(-i)=1
\end{eqnarray*}
  
Estos tres autovalores generan los siguientes autovectores:
\[
\left| u_1\right>_{\lambda_1}=
\left(
\begin{array}{c}
1   \\ 
\frac{i-\sqrt{-6i}-1}{2} \\
0
\end{array}
\right)\,, \,\,
\left| u_2\right>_{\lambda_2}=
\left(
\begin{array}{c}
1   \\ 
 \frac{i+\sqrt{-6i}-1}{2} \\
0
\end{array}
\right)\,, \,\,
\left| u_3\right>_{\lambda_3}=
\left(
\begin{array}{c}
0   \\ 
0 \\
1
\end{array}
\right)\,,
\]

Se puede demostrar que los autovectores son ortogonales:
\begin{eqnarray*}
\left< {u}_1\right. \left| {u}_2 \right> &=& 
({u}_1)^\dagger{u}_2 =0 \,\, \Rightarrow \,\,
\left(
\begin{array}{ccc}
1 &  \frac{-i-\sqrt{6i}-1}{2}& 0
\end{array}
\right)
\left(
\begin{array}{c}
1  \\ 
 \frac{i+\sqrt{-6i}-1}{2} \\
0
\end{array}
\right) =0\,, \\
\left< {u}_1\right. \left| {u}_3 \right> &=&
({u}_1)^\dagger{u}_3=0 \,\, \Rightarrow \,\,
\left(
\begin{array}{ccc}
1 &  \frac{-i-\sqrt{6i}-1}{2} & 0
\end{array}
\right)
\left(
\begin{array}{c}
0   \\ 
0 \\
1
\end{array}
\right) =0 \,,  \\
\left< {u}_2\right. \left| {u}_3 \right> &=&
({u}_2)^\dagger{u}_3=0 \,\, \Rightarrow \,\,
\left(
\begin{array}{ccc}
1 &  \frac{-i+\sqrt{6i}-1}{2} & 0
\end{array}
\right)
\left(
\begin{array}{c}
0   \\ 
0 \\
1
\end{array}
\right) =0 \,.
\end{eqnarray*}

Con los autovectores normalizados construimos la matriz $\mathbb{U}$, que también será unitaria, y la matriz diagonal  $\mathbb{D}$ con los autovalores. 
\[
 \mathbb{U}= 
 \left( 
 \begin {array}{ccc} 
{\frac {1}{\sqrt {3+\sqrt {3}}}}&{\frac {1}{\sqrt {3-\sqrt {3}}}}&0\\ 
{-\frac {\left(1-i\right)\left(\sqrt{3}+1\right)}{2\sqrt {3+\sqrt {3}}}}& {\frac { \left(1-i\right)\left(\sqrt {3}-1\right)}{2\sqrt {3-\sqrt {3}}}}&0\\ 
0&0&1
\end {array} 
\right) \,, \quad 
\mathbb{D}= 
\left(
\begin{array}{ccc}
\frac{\sqrt{2}(1+i)-2\,\sqrt{-3i}}{4} & 0 & 0 \\
0 & \frac{\sqrt{2}(1+i)+2\,\sqrt{-3i}}{4} & 0  \\
0 & 0 & i
\end{array}
\right)\,.
\]


\item Considere que el espacio de estados para un determinado sistema físico viene expandido por una base ortonormal $\left\{  \left|  \xi_{1}\right> ,\left|  \xi_{2}\right> ,\left|\xi_{3}\right> \right\}$.  Definimos dos operadores $\mathbb{L}_{z}$ y $\mathbb{S}$ de la siguiente manera:
\[
\mathbb{L}_{z}\left|  \xi_{1}\right> =\left|  \xi_{1}\right>, \,\, 
\mathbb{L}_{z}\left|  \xi_{2}\right> =0\,, \,\,  
\mathbb{L}_{z}\left|  \xi_{3}\right> =-\left|  \xi_{3}\right> , \,\, 
\mathbb{S}\left|  \xi_{1}\right> =\left|  \xi_{3}\right>, \,\, 
\mathbb{S}\left|  \xi_{2}\right> =\left| \xi_{2}\right>, \,\, 
\mathbb{S}\left|  \xi_{3}\right> =\left|  \xi_{1}\right> .
\]

En la base ortonormal $\left\{  \left|  \xi_{1}\right>, \left|\xi_{2}\right>, \left|  \xi_{3}\right> \right\}$ las
representaciones matriciales para $\mathbb{L}_{z}, \mathbb{L}_{z}^{2},\mathbb{S}$ y $\mathbb{S}^{2}$ serán las siguientes:
\begin{align*}
\left< \xi^{i}\right|  \mathbb{L}_{z}\left|  \xi_{j}\right>  &
=\left(
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & -1
\end{array}
\right)\,, \,\, \left< \xi^{i}\right|  \mathbb{L}_{z}^{2}\left|
\xi_{j}\right> =\left(
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 1
\end{array}
\right) \,, \\
& \\
\left< \xi^{i}\right|  \mathbb{S}\left|  \xi_{j}\right>  &  =\left(
\begin{array}
[c]{ccc}
0 & 0 & 1\\
0 & 1 & 0\\
1 & 0 & 0
\end{array}
\right) \,, \,\,  \left< \xi^{i}\right|  \mathbb{S}^{2}\left|
\xi_{j}\right> =\left(
\begin{array}
[c]{ccc}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{array}
\right) \,.
\end{align*}

Es claro que estas matrices son reales y simétricas y, por lo tanto, son hermíticas y, al ser el espacio de dimensión finita, deben ser diagonalizables y sus autovectores formarán base para ese espacio. Por lo tanto, $\mathbb{L}_{z},\mathbb{L}_{z}^{2},\mathbb{S}$ y $\mathbb{S}^{2}$ son observables.
Ahora bien: ¿Cuál será la forma más general de una representación matricial de un operador que conmute con $\mathbb{L}_{z}$? 

Notamos que los vectores de la base ortonormal $\left\{  \left|  \xi_{1}\right> ,\left|  \xi_{2}\right> ,\left|\xi_{3}\right> \right\}  $ son autovectores para $\mathbb{L}_{z}$ con autovalores $\left\{  1,0,-1\right\}$, con lo cual su representación matricial tiene que ser diagonal. Recuerde que si dos observables $\mathbb{A}$ y $\mathbb{B}$ conmutan, $\left[  \mathbb{A},\mathbb{B}\right]  =0,$ y si $\left|  \psi_{1}\right> $ y $\left|  \psi_{2}\right> $ son autovectores de $\mathbb{A}$ para autovalores distintos, entonces el elemento de matriz $\left< \psi^{1}\right|  \mathbb{B}\left|  \psi_{2}
\right> =0$, con lo cual: 
\[ 
[\mathbb{M}, \mathbb{L}_{z}] = 0 \quad \Leftrightarrow \quad \left< \xi^{i}\right|  \mathbb{M}\left|  \xi_{j}\right> =\left(
\begin{array}
[c]{ccc}
M_{1}^{1} & 0 & 0\\
0 & M_{2}^{2} & 0\\
0 & 0 & M_{3}^{3}
\end{array}
\right)\,.
\]

Esto se desprende de manera directa de: 
\[
0 = \left< \xi^{i}\right|  [\mathbb{M}, \mathbb{L}_{z}] \left|  \xi_{j}\right> = \left< \xi^{i}\right|  \mathbb{M} \mathbb{L}_{z} - \mathbb{L}_{z}  \mathbb{M}   \left|  \xi_{j}\right> = \left( \lambda_{j} - \lambda_{i} \right) \left< \xi^{i}\right|  \mathbb{M} \left|  \xi_{j}\right>\,, \quad \mathrm{con} \quad  \left( \lambda_{j} - \lambda_{i} \right) \neq 0 \quad \mathrm{para} \; i \neq j\,.
\]

Si nos planteamos la misma pregunta para $\mathbb{L}_{z}^{2}$,  vemos que sus autovalores son  $\left\{ 1, 0 \right\}$. Esto es:
\[
\mathbb{L}^{2}_{z}   \left|  \xi_{1}\right> =  \left|  \xi_{1}\right>; \quad \mathbb{L}^{2}_{z}   \left|  \xi_{2}\right> =  0; \quad 
\mathbb{L}^{2}_{z}   \left|  \xi_{3}\right> =  \left|  \xi_{3}\right>\,,
\] 
con lo cual tendremos que la representación matricial de ese operador que conmute con $\mathbb{L}_{z}^{2}$, no será diagonal, es decir: 
\[ 
[\mathbb{N}, \mathbb{L}^{2}_{z}] = 0 \quad \Leftrightarrow \quad
\left< \xi^{i}\right|  \mathbb{N}\left|  \xi_{j}\right> =\left(
\begin{array}
[c]{ccc}
N_{1}^{1} 	& 0 			 & N_{3}^{1} \\
0 		& N_{2}^{2} 	& 0\\
N_{1}^{3} & 0 			& N_{3}^{3}
\end{array}
\right)\,,
\]
ya que:
\[
0 = \left< \xi^{1}\right|  [\mathbb{N}, \mathbb{L}^{2}_{z}] \left|  \xi_{3}\right> 
\,\, \Rightarrow \,\,
\left< \xi^{1}\right|  \mathbb{N} \left| \xi_{3}\right> =\left< \xi^{1}\right|  \mathbb{N} \left|\xi_{3}\right> \,,
\]
y vale para cualquier elemento $N_{3}^{1}$ (y equivalentemente para $N_{1}^{3}$). 

%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{minipage}{7.4cm}
Adicionalmente, si ordenamos la base de autovectores de $\mathbb{L}_{z}$, como $\left\{  \left|  \xi_{1}\right>, \left|\xi_{3}\right>, \left|  \xi_{2}\right> \right\}$, tendremos entonces como representación matricial diagonal a bloques, correspondiente a un autovalor degenerado $1$, a: 
\[
\left< \xi^{i}\right|  \tilde{\mathbb{N}}\left|  \xi_{j}\right> =\left(
\begin{array}
[c]{ccc}
N_{1}^{1} 	& N_{3}^{1}  			 & 0\\
N_{1}^{3}  & N_{2}^{2} 	& 0\\
0			& 0 			& N_{3}^{3}
\end{array}
\right)  \,.
\]
\end{minipage} \hfill 
\begin{minipage}{8.0cm} 
\begin{tabular}{|ccc|}
\hline
{Autovectores} & {Autovalor} $ \mathbb{L}_{z}^{2}$ & {Autovalor} $ \mathbb{S}$ \\ \hline
$\left| {q}_{1} \right>=\left| \xi_{2} \right>$ & 0 & 1 \\
$\left| {q}_{2} \right>=\frac{1}{\sqrt{2}} \left(\left|\xi_{1}\right>+\left|\xi_{3}\right> \right)$ & 1 & 1 \\
$\left| {q}_{3} \right>=\frac{1}{\sqrt{2}} \left(\left|\xi_{1}\right> -\left|\xi_{3}\right> \right)$ & 1 & -1 \\ \hline
\end{tabular}
\caption{Dado que no hay líneas repetidas $\mathbb{L}_{z}^{2}$ y $\mathbb{S}$ forman un CCOC.}
\label{Tabla41}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%

Finalmente, la representación matricial, más general, de un operador que conmute con $\mathbb{S}^{2}$ es:
\[ [\mathbb{P}, \mathbb{S}^{2}] = 0 \quad \Leftrightarrow \quad
\left< \xi^{i}\right|  \mathbb{P}\left|  \xi_{j}\right> =\left(
\begin{array}[c]{ccc}
P_{1}^{1} & P_{2}^{1} & P_{3}^{1} \\
P_{1}^{2} & P_{2}^{2} & P_{3}^{2} \\
N_{1}^{3} & P_{2}^{3} & P_{3}^{3}
\end{array}
\right)\,.
\]

Ahora intentaremos construir una base común de autovectores para $ \mathbb{L}_{z}^{2}$ y $ \mathbb{S}$. Para ello notamos que $\left| \xi_{2} \right>$ es un autovector común a $ \mathbb{L}_{z}^{2}$ y $ \mathbb{S}$, por lo tanto, existirá un subespacio expandido por: $\left\{ \left| \xi_{1} \right>, \left| \xi_{3} \right> \right\}$. En ese subespacio las representaciones matriciales para  $ \mathbb{L}_{z}^{2}$ y $ \mathbb{S}$, serán:
\[
\left< \xi^{i}\right| \mathbb{L}_{z}^{2} \left|  \xi_{j}\right>_{\mathcal{S}_{13}}  = 
\left(\begin{array}{cc}
1 & 0 \\0 & 1
\end{array}\right) \,,\quad
\left< \xi^{i}\right| \mathbb{S} \left|  \xi_{j}\right>_{\mathcal{S}_{13}}   = 
\left(\begin{array}{cc}0 & 1 \\ 1 & 0\end{array}\right)\,.
\]

Acto seguido planteamos el problema de autovalores para $\mathbb{S}$, esto es:
\[
 \mathbb{S} \left|  q_{j}\right> = \lambda_{j}  \left|  u_{j}\right> 
\,\, \Rightarrow \,\, 
 \left(\begin{array}{cc}0 & 1 \\ 1 & 0\end{array}\right) {\left(\begin{array}{c}q_{1} \\q_{2}\end{array}\right)} = \lambda{\left(\begin{array}{c}q_{1} \\q_{2}\end{array}\right)} 
\,\, \Rightarrow \,\,
 \left\{ \begin{array}{l }
   \left| {q}_{2} \right>  = \frac{1}{\sqrt{2}} \left(  \left|  \xi_{1}\right> +  \left|  \xi_{3}\right> \right)   \\ \\
   \left| {q}_{3} \right> = \frac{1}{\sqrt{2}} \left(  \left|  \xi_{1}\right> -  \left|  \xi_{3}\right> \right)  
\end{array} \right. 
\]
con lo cual tendremos los resultados mostrados en la tabla  \ref{Tabla41}.


\item Consideremos otro ejemplo proveniente de la Mecánica Clásica. Se trata de dos osciladores armónicos, de igual masa, acoplados con resortes con la misma constante elástica $k$. 
La ecuaciones de movimiento para este sistema son:
%%%%%%%%%%%%%%%%%
\begin{figure}[h]
\begin{minipage}{9.0cm}
\begin{eqnarray*}
m\ddot{x}_{1} + kx_{1} - k(x_{2} -x_{1})&=&0 \,, \\
\\
m\ddot{x}_{2} + kx_{2} + k(x_{2} -x_{1})&=& 0 \,,
\end{eqnarray*}
\end{minipage} \hfill 
\begin{minipage}{6.0cm} 
\vspace{0.5 cm}
\includegraphics[width=2.3in]{VOLUMEN_1/04_Auto_Vectores/Figuras/Figura4_1.jpg}
\end{minipage}
\end{figure}
%%%%%%%%%%%%%%%%%

Podremos expresar estas ecuaciones en forma de operadores:
\[
\mathbb{D}\left| {x} \right>=0 \quad \Leftrightarrow \quad 
\left(\begin{array}{cc}m\frac{d^2}{dt^2} +2k & -k \\ -k & m\frac{d^2}{dt^2} +2k\end{array}\right) \left(\begin{array}{c}x^{1} \\x^{2}\end{array}\right) = 0 \,.
\]

Si pensamos esta ecuación como una ecuación de autovalores, el autovalor es claramente $\lambda = 0$, y como las masas y las constantes elásticas son iguales podemos intercambiar las partículas y la física (las ecuaciones de movimiento) no cambian. Esto se puede expresar matemáticamente como el operador permutación de las partículas:
\[
\mathbb{P}=\left(\begin{array}{cc}0 & 1 \\1 & 0 \end{array}\right) 
\quad \Rightarrow \quad 
\left(\begin{array}{cc}0 & 1 \\1 & 0 \end{array}\right) \left(\begin{array}{c}x^{1} \\x^{2}\end{array}\right) = \left(\begin{array}{c}x^{2} \\x^{1}\end{array}\right) \,.
\]

Es inmediato comprobar que $\left[ \mathbb{D},\mathbb{P} \right]=0$, con lo cual existirá una combinación lineal de autovectores de $\mathbb{D}$ (asociados con el autovalor $\lambda = 0$) los cuales también serán autovectores de $\mathbb{P}$. Para ello procedamos a calcular los autovalores y autovectores de $\mathbb{P}$:
\[
\mathbb{P}\left| {x} \right>=\lambda\left| {x} \right> 
\,\, \Rightarrow \,\, 
\left|\begin{array}{cc}-\lambda & 1 \\1 & -\lambda\end{array}\right| =0 
\,\, \Rightarrow \,\,   \lambda\pm 1 \,\, \Leftrightarrow \,\, \left| \hat{\mathrm{e}}_{1} \right>= \frac{1}{\sqrt{2}}\left( \begin{array}{c} 1 \\1\end{array}\right); \quad \left| \hat{\mathrm{e}}_{2} \right>= \frac{1}{\sqrt{2}}\left(\begin{array}{c}1 \\-1\end{array}\right)\,.
\]

Fácilmente podemos expresar el vector posición como una combinación lineal de estos dos autovectores de $\mathbb{P}$,  esto es:
\[
\left(\begin{array}{c}x^{1} \\x^{2}\end{array}\right) = \frac{\xi_{1}}{\sqrt{2}}\left( \begin{array}{c} 1 \\1\end{array}\right) + \frac{\xi_{2}}{\sqrt{2}}\left(\begin{array}{c}1 \\-1\end{array}\right) 
\,\, \Rightarrow \,\, 
\left\{\begin{array}{c}\xi_{1} =\frac{1}{\sqrt{2}}\left( x_{1}+x_{2} \right) \\ \\\xi_{2} =\frac{1}{\sqrt{2}}\left( x_{1}-x_{2} \right)\end{array}\right.
\]

Es claro que 
\[
\left| {u}_{1} \right>= \frac{1}{\sqrt{2}}\left( x_{1}+x_{2} \right)\left( \begin{array}{c} 1 \\1\end{array}\right)  \qquad \text{y} \qquad \left| {u}_{2} \right>= \frac{1}{\sqrt{2}}\left( x_{1}-x_{2} \right)\left(\begin{array}{r}1 \\-1\end{array}\right)\,,
\]
son autovectores de $\mathbb{P}$ y $\mathbb{D}$.

\item  Un operador cantidad de movimiento generalizado se define como aquel conjunto de operadores hermíticos que cumplen con:
\[
\left[  \mathbb{J}_{x},\mathbb{J}_{y}\right]  =i\hbar\mathbb{J}_{z}\,, \quad 
\left[  \mathbb{J}_{y},\mathbb{J}_{z}\right]  =i\hbar\mathbb{J}_{x}\,, \quad 
\left[  \mathbb{J}_{z},\mathbb{J}_{x}\right]  =i\hbar\mathbb{J}_{y}\,, \quad \text{es decir:} \quad \left[  \mathbb{J}_{i},\mathbb{J}_{j}\right] = i\hbar\epsilon_{ijk}\mathbb{J}_{k} \,,
\]
con $\epsilon_{ijk}$ el símbolo de Levy-Civita (Aquí los índices repetidos NO indican suma). 

Adicionalmente, definimos los siguientes operadores:
\[
\mathbb{J}^{2}=\mathbb{J}_{x}^{2}+\mathbb{J}_{y}^{2}+\mathbb{J}_{z}^{2}\,, \,\,\,
\mathbb{J}_{+}=\mathbb{J}_{x}+i\mathbb{J}_{y} \,, \,\,\,
\mathbb{J}_{-}=\mathbb{J}_{x}-i\mathbb{J}_{y}\,.
\]

Queremos demostrar que:
\[
\left[  \mathbb{J}^{2},\mathbb{J}_{+}\right] =\left[  \mathbb{J}^{2},\mathbb{J}_{-}\right]  =\left[  \mathbb{J}^{2},\mathbb{J}_{z}\right] =0 \,.
\]

Para probar esta propiedad se puede demostrar de forma genérica que  $\left[  \mathbb{J}^{2}_{k},\mathbb{J}_{m}\right] =0$, con $k,m = 1,2,3 \equiv x,y,z$, esto es:
  \[
  \left[  \mathbb{J}^{2}_{k},\mathbb{J}_{m}\right] = \left[  \mathbb{J}_{k}\mathbb{J}_{k},\mathbb{J}_{m}\right] =  \mathbb{J}_{k}\mathbb{J}_{k} \mathbb{J}_{m} - \mathbb{J}_{m} \mathbb{J}_{k}  \mathbb{J}_{k} =
   \mathbb{J}_{k}\mathbb{J}_{k} \mathbb{J}_{m}  - \left(  i\hbar\epsilon_{mkl}\mathbb{J}_{l} + \mathbb{J}_{k} \mathbb{J}_{m} \right) \mathbb{J}_{k}\,,
  \]
  con lo cual:
  \[
  \left[  \mathbb{J}^{2}_{k},\mathbb{J}_{m}\right] = \mathbb{J}_{k}\mathbb{J}_{k} \mathbb{J}_{m}  -  i\hbar\epsilon_{mkl}\mathbb{J}_{l}\mathbb{J}_{k} - \mathbb{J}_{k} \left(  i\hbar\epsilon_{mkn}\mathbb{J}_{n} + \mathbb{J}_{k} \mathbb{J}_{m} \right)\,,
   \] 
y claramente se anula por cuanto los índices no suman pero si son mudos, y $\epsilon_{mkl} = - \epsilon_{mlk}$.
\[
\left[  \mathbb{J}^{2}_{k},\mathbb{J}_{m}\right]=\mathbb{J}_{k}\mathbb{J}_{k} \mathbb{J}_{m}  -  
i\hbar\epsilon_{mkl}\mathbb{J}_{l}\mathbb{J}_{k} -  i\hbar\epsilon_{mkn}\mathbb{J}_{k}  \mathbb{J}_{n} -\mathbb{J}_{k}  \mathbb{J}_{k} \mathbb{J}_{m}\,,
   \]
al conmutar los cuadrados de las componentes con cualquiera de las componentes, y dado que los conmutadores son lineales entonces queda demostrado que: 
   \[
 \left[  \mathbb{J}^{2},\mathbb{J}_{\pm}\right] =   \left[ \mathbb{J}_{x}^{2}+\mathbb{J}_{y}^{2}+\mathbb{J}_{z}^{2},\mathbb{J}_{x} \pm i\mathbb{J}_{y} \right] = \left[ \mathbb{J}_{y}^{2},\mathbb{J}_{x} \right] + \left[ \mathbb{J}_{z}^{2},\mathbb{J}_{x}  \right] \pm i \left[ \mathbb{J}_{x}^{2}, \mathbb{J}_{y} \right]  \pm i \left[ \mathbb{J}_{z}^{2},\mathbb{J}_{y} \right] =0\,.
   \]
   
 \item Si definimos los autovectores comunes a $\mathbb{J}^{2}$ y
$\mathbb{J}_{z}$ como $\left|  j,m\right\rangle $ de la siguiente manera:
\[
\mathbb{J}^{2}\left|  j,m\right\rangle   =j\left(  j+1\right)  \hbar^{2}\left|  j,m\right\rangle\,, \quad
\mathbb{J}_{z}\left|  j,m\right\rangle   =m\hbar \left|  j,m\right\rangle\,, \quad \text{con: }
\left\langle  j,m\right. \left|  j',m' \right\rangle = \delta_{jj'}\delta_{mm'} \,,
\]
y adicionalmente tenemos que:
\[
\mathbb{J}_{-}\left|  j,m\right\rangle  = \hbar\sqrt{j(j+1)  - m(  m-1)  }\left|  j,m-1\right\rangle \,, \quad
\mathbb{J}_{+}\left|  j,m\right\rangle  = \hbar\sqrt{j (j+1)  - m (  m+1)  }\left|  j,m+1\right\rangle.          
\]

Si se supone (es fácil demostrarlo) que $-j\leq m\leq j$. Esto quiere decir que dado
algún valor  $j$, $m$ varían entre $-j$ y $j$ de uno en uno, esto es: 
$m=-j,-j+1,-j+2,\cdots,j-2,j-1,j$. Supongamos ahora que $j=\frac{1}{2}$.  

Busquemos:
\begin{enumerate}
\item La representación matricial para:  $\mathbb{J}_{z},\mathbb{J}_{-},\mathbb{J}_{+}, \mathbb{J}^2$, en la base de autovectores de $\mathbb{J}_{z}$ y $\mathbb{J}^2$.

Si $\left|  j,m\right\rangle $ son autovectores de  $\mathbb{J}^{2}$ y $\mathbb{J}_{z}$ su representación matricial  será diagonal y como $m$ varía entre  $-j$ y $j$ con incrementos de $1$ tendremos que serán matrices $2\times2$. La base ortogonal de autovectores será: $\big\{ \left|  \frac{1}{2},-\frac{1}{2}\right\rangle,\left|  \frac{1}{2}, \frac{1}{2}\right\rangle \big\}$.
  \[
  \left(\begin{array}{cc}
  \left\langle \frac{1}{2},\frac{1}{2}\right| \mathbb{J}_{z} \left|  \frac{1}{2},\frac{1}{2}\right\rangle  & 
  \left\langle \frac{1}{2},\frac{1}{2}\right| \mathbb{J}_{z} \left|  \frac{1}{2},-\frac{1}{2}\right\rangle    \\ \\
    \left\langle \frac{1}{2},-\frac{1}{2}\right| \mathbb{J}_{z} \left|  \frac{1}{2},\frac{1}{2}\right\rangle   & 
       \left\langle \frac{1}{2},-\frac{1}{2}\right| \mathbb{J}_{z} \left|  \frac{1}{2},-\frac{1}{2}\right\rangle 
\end{array}
  \right) \equiv
  \dfrac{\hbar}{2}  \left(\begin{array}{cc}
   1 &  0   \\ \\
  0   & -1 
\end{array}
  \right) \,,
  \]
  \[
  \left(\begin{array}{cc}
  \left\langle \frac{1}{2},\frac{1}{2}\right| \mathbb{J}^{2} \left|  \frac{1}{2},\frac{1}{2}\right\rangle  & 
  \left\langle \frac{1}{2},\frac{1}{2}\right| \mathbb{J}^{2} \left|  \frac{1}{2},-\frac{1}{2}\right\rangle    \\ \\
    \left\langle \frac{1}{2},-\frac{1}{2}\right| \mathbb{J}^{2} \left|  \frac{1}{2},\frac{1}{2}\right\rangle   & 
       \left\langle \frac{1}{2},-\frac{1}{2}\right| \mathbb{J}^{2} \left|  \frac{1}{2},-\frac{1}{2}\right\rangle 
\end{array}
  \right) \equiv
   \dfrac{3}{4}\hbar^{2} \left(\begin{array}{cc}
  1  &  0   \\ \\
     0   & 1 
\end{array}
  \right) \,.
  \]  
  
La representación matricial para $\mathbb{J}_{-},\mathbb{J}_{+}$ obviamente no será diagonal:
    \[
  \left(\begin{array}{cc}
  \left\langle \frac{1}{2},\frac{1}{2}\right| \mathbb{J}_{+} \left|  \frac{1}{2},\frac{1}{2}\right\rangle  & 
  \left\langle \frac{1}{2},\frac{1}{2}\right| \mathbb{J}_{+} \left|  \frac{1}{2},-\frac{1}{2}\right\rangle    \\ \\
    \left\langle \frac{1}{2},-\frac{1}{2}\right| \mathbb{J}_{+} \left|  \frac{1}{2},\frac{1}{2}\right\rangle   & 
       \left\langle \frac{1}{2},-\frac{1}{2}\right| \mathbb{J}_{+} \left|  \frac{1}{2},-\frac{1}{2}\right\rangle 
\end{array}
  \right) \equiv
  \hbar \left(\begin{array}{cc}
   0  &  1   \\ \\
  0  & 0 
\end{array}
  \right)\,,
  \]
   \[
  \left(\begin{array}{cc}
  \left\langle \frac{1}{2},\frac{1}{2}\right| \mathbb{J}_{-} \left|  \frac{1}{2},\frac{1}{2}\right\rangle  & 
  \left\langle \frac{1}{2},\frac{1}{2}\right| \mathbb{J}_{-} \left|  \frac{1}{2},-\frac{1}{2}\right\rangle    \\ \\
    \left\langle \frac{1}{2},-\frac{1}{2}\right| \mathbb{J}_{-} \left|  \frac{1}{2},\frac{1}{2}\right\rangle   & 
       \left\langle \frac{1}{2},-\frac{1}{2}\right| \mathbb{J}_{-} \left|  \frac{1}{2},- \frac{1}{2}\right\rangle 
\end{array}
  \right) \equiv
    \hbar  \left(\begin{array}{cc}
   0 &  0   \\ \\
1  & 0
\end{array}
  \right)\,.
  \]

 \item Calculemos los autovalores y autovalores para: $ \mathbb{J}_{z},  \mathbb{J}_{-},\mathbb{J}_{+}, \mathbb{J}^2$.

Otra vez, $\big\{ \left|  \frac{1}{2},-\frac{1}{2}\right\rangle,\left|  \frac{1}{2}, \frac{1}{2}\right\rangle \big\}$ son autovectores de  $\mathbb{J}^{2}$ y $\mathbb{J}_{z}$. En el caso de $\mathbb{J}^{2}$  con un autovalor de $ \frac{3}{4}\hbar^{2}$ para ambos autovectores y en el caso de $\mathbb{J}_{z}$ los autovalores serán $\pm  \frac{\hbar}{2} $ respectivamente. Para $ \mathbb{J}_{-},\mathbb{J}_{+}$ no tendrán autovalor distinto de cero en esta base.
\end{enumerate} 
\end{enumerate}

\newpage
\subsection{{\color{red}Practicando con Maxima}} 

Consideremos uno de los ejemplos anteriores donde:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i1) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A:matrix([1,0,3], [0,-2,0], [3,0,1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o1) }
\begin{pmatrix}1 & 0 & 3 \\ 0 & -2 & 0 \\ 3 & 0 & 1 \\ 
 \end{pmatrix}
\end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i2) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
eigenvectors(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o2) }
\left[ \left[ \left[ -2 , 4 \right]  , \left[ 2 , 1 \right] 
  \right]  , \left[ \left[ \left[ 1 , 0 , -1 \right]  , \left[ 0 , 1
  , 0 \right]  \right]  , \left[ \left[ 1 , 0 , 1 \right]  \right] 
  \right]  \right] 
\end{math}
\newline

El resultado es una lista con los autovalores y su multiplicidad, y para cada autovalor los autovectores como sublistas. Es necesario manipular estas sublistas para obtener los autovectores. Entonces, es mejor escribir:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i3) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[val,vec]:eigenvectors(A);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o3) }
\left[ \left[ \left[ -2 , 4 \right]  , \left[ 2 , 1 \right]   \right]  , \left[ \left[ \left[ 1 , 0 , -1 \right]  , \left[ 0 , 1 , 0 \right]  \right]  , \left[ \left[ 1 , 0 , 1 \right]  \right]  \right]  \right] 
\end{math}
\newline

Por lo tanto, una lista con los autovalores es la siguiente:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i4) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
autovalores:val[1];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o4) }
\left[ -2 , 4 \right]
 \end{math}
\newline
 
Notemos el orden: los dos primeros vectores corresponden al autovalor $-2$ y el tercero al autovalor $4$. 

Los autovectores como listas son:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i5) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
L1:vec[1];L2:vec[2];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ \left[ 1 , 0 , -1 \right]  , \left[ 0 , 1 , 0 \right]   \right] 
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o5) }
\left[ \left[ 1 , 0 , 1 \right]  \right] 
 \end{math}
\newline

Pero resulta conveniente separar cada autovector.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i6) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
L1[1];L1[2];L2[1];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o6) }
\left[ 1 , 0 , -1 \right] 
 \end{math}
 
%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o7) }
\left[ 0 , 1 , 0 \right]
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o8) }
\left[ 1 , 0 , 1 \right] 
 \end{math}
\newline
 
Vamos ahora a normalizar los autovectores:

 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i9) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
V1:L1[1]/sqrt(L1[1].L1[1]);V2:L1[2]/sqrt(L1[2].L1[2]);V3:L2[1]/sqrt(L2[1].L2[1]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o9) }
\left[ \frac{1}{\sqrt{2}} , 0 , -\frac{1}{\sqrt{2}} \right] 
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o10) }
\left[ 0 , 1 , 0 \right] 
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o11) }
\left[ \frac{1}{\sqrt{2}} , 0 , \frac{1}{\sqrt{2}} \right]
 \end{math}
\newline

Hacemos una lista con los vectores normalizados:

 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i12) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Lvec:[V1,V2,V3];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o12) }
\left[ \left[ \frac{1}{\sqrt{2}} , 0 , -\frac{1}{\sqrt{2}} \right] 
, \left[ 0 , 1 , 0 \right]  , \left[ \frac{1}{\sqrt{2}} , 0 , 
 \frac{1}{\sqrt{2}} \right]  \right] 
 \end{math}
\newline

Y construimos la matriz $ \mathbb{C}$:

 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i13) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
C:transpose(apply('matrix,Lvec));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o13) }
\begin{pmatrix}\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 0 & 1
  & 0 \\ -\frac{1}{\sqrt{2}} & 0 & \frac{1}{\sqrt{2}} \\ 
 \end{pmatrix}
 \end{math}
\newline

Para finalmente comprobar:

 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i14) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
transpose(C).A.C;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o14) }
\begin{pmatrix}-2 & 0 & 0 \\ 0 & -2 & 0 \\ 0 & 0 & 4 \\ 
 \end{pmatrix}
 \end{math}
\newline

Consideremos ahora otro de los ejemplos, donde teníamos la siguiente matriz:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i15) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
A:matrix([1,-1+2*%i,%i],[-1-2*%i,2,-1],[-%i,-1,3]);
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o15) }
\begin{pmatrix}1 & 2\,i-1 & i \\ -2\,i-1 & 2 & -1 \\ -i & -1 & 3 \\ \end{pmatrix}
 \end{math}
\newline

Es fácil ver que se trata de una matriz hermítica

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i16) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
transpose(conjugate(A))=A;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o16) }
\begin{pmatrix}1 & 2\,i-1 & i \\ -2\,i-1 & 2 & -1 \\ -i & -1 & 3 \\ \end{pmatrix}=\begin{pmatrix}1 & 2\,i-1 & i \\ -2\,i-1 & 2 & -1 \\ 
-i & -1 & 3 \\ \end{pmatrix}
 \end{math}
\newline

Calculamos los autovalores y autovectores:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i17) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
[val,vec]:eigenvectors(A)$
\end{verbatim}}
\end{minipage}
\newline

Al primer elemento de la lista, le asignaremos la variable {\tt autovalores}, como se muestra a continuación:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i18) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
autovalores:val[1];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o18) }
\left[ 1-\sqrt{5} , \sqrt{5}+1 , 4 \right] 
 \end{math}
\newline

Ahora procedemos a aislar los diferentes autovectores, es importante tener en mente el orden para utilizar las etiquetas apropiadas.

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i19) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
L1:vec[1]$L1[1]; L2:vec[2]$L2[1]; L3:vec[3]$L3[1];
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o20) }
\left[ 1 , \frac{\sqrt{5}\,i+1}{3} , \frac{\left(\sqrt{5}-1\right)
 \,i+\sqrt{5}-2}{3} \right] 
 \end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o22) }
\left[ 1 , -\frac{\sqrt{5}\,i-1}{3} , -\frac{\left(\sqrt{5}+1 \right)\,i+\sqrt{5}+2}{3} \right] 
 \end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o24) }
\left[ 1 , -i-1 , 1 \right]
 \end{math}
 \newline
 
Los autovectores normalizados son:
 
%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i25) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
u1:L1[1]/sqrt(L1[1].L1[1]),ratsimp;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o25) }
\left[ \frac{3}{\sqrt{-\left(4\,\sqrt{5}-14\right)\,i-2\,\sqrt{5}+8
 }} , \frac{\sqrt{5}\,i+1}{\sqrt{-\left(4\,\sqrt{5}-14\right)\,i-2\,
 \sqrt{5}+8}} , \frac{\left(\sqrt{5}-1\right)\,i+\sqrt{5}-2}{\sqrt{-
 \left(4\,\sqrt{5}-14\right)\,i-2\,\sqrt{5}+8}} \right] 
 \end{math}

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i26) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
u2:L2[1]/sqrt(L2[1].L2[1]),ratsimp;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o26) }
\left[ \frac{3}{\sqrt{\left(4\,\sqrt{5}+14\right)\,i+2\,\sqrt{5}+8}
 } , -\frac{\sqrt{5}\,i-1}{\sqrt{\left(4\,\sqrt{5}+14\right)\,i+2\,
 \sqrt{5}+8}} , -\frac{\left(\sqrt{5}+1\right)\,i+\sqrt{5}+2}{\sqrt{
 \left(4\,\sqrt{5}+14\right)\,i+2\,\sqrt{5}+8}} \right] 
 \end{math}
 
 %%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i27) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
u3:L3[1]/sqrt(L3[1].L3[1]),ratsimp;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o27) }
\left[ \frac{1}{\sqrt{2\,i+2}} , -\frac{i+1}{\sqrt{2\,i+2}} , 
 \frac{1}{\sqrt{2\,i+2}} \right] 
 \end{math}
\newline

Estos vectores serán ortogonales, como podemos ver:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i28) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
conjugate(u1).u2,ratsimp;conjugate(u1).u3,ratsimp;conjugate(u3).u2,ratsimp;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o28) }
0
\end{math}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o29) }
0
\end{math}
 
 %%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o30) }
0
\end{math}
\newline

Para construir la matriz  $C$ preparamos primero el siguiente arreglo:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i31) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Lvec:[u1,u2,u3]$
\end{verbatim}}
\end{minipage}
\newline

Con cada autovector como columna construimos la matriz $C$:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i32) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
C:transpose(apply('matrix,Lvec));
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o32) }
\begin{pmatrix}\frac{3}{\sqrt{-\left(4\,\sqrt{5}-14\right)\,i-2\,
 \sqrt{5}+8}} & \frac{3}{\sqrt{\left(4\,\sqrt{5}+14\right)\,i+2\,
 \sqrt{5}+8}} & \frac{1}{\sqrt{2\,i+2}} \\ \frac{\sqrt{5}\,i+1}{
 \sqrt{-\left(4\,\sqrt{5}-14\right)\,i-2\,\sqrt{5}+8}} & -\frac{
 \sqrt{5}\,i-1}{\sqrt{\left(4\,\sqrt{5}+14\right)\,i+2\,\sqrt{5}+8}}
  & -\frac{i+1}{\sqrt{2\,i+2}} \\ \frac{\left(\sqrt{5}-1\right)\,i+
 \sqrt{5}-2}{\sqrt{-\left(4\,\sqrt{5}-14\right)\,i-2\,\sqrt{5}+8}} & 
 -\frac{\left(\sqrt{5}+1\right)\,i+\sqrt{5}+2}{\sqrt{\left(4\,\sqrt{5
 }+14\right)\,i+2\,\sqrt{5}+8}} & \frac{1}{\sqrt{2\,i+2}} \\ 
 \end{pmatrix}
\end{math}
\newline

La inversa de la matriz $C$ se obtiene como ya sabemos:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i33) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Cinv:invert(C),ratsimp$
\end{verbatim}}
\end{minipage}
\newline

Aquí aplicamos una rutina de simplificación a cada uno de los elementos de la matriz. Esto lo hicimos con el comando {\tt ratsimp}. 

Para finalmente poder calcular $C^{-1} A C$ y obtener:

%%%%%% INPUT:
\begin{minipage}[t]{8ex}
{\color{red}\bf \begin{verbatim} (%i34) 
\end{verbatim}}
\end{minipage}
\begin{minipage}[t]{\textwidth}{\color{blue}
\begin{verbatim}
Cinv.A.C,ratsimp;
\end{verbatim}}
\end{minipage}

%%% OUTPUT:
\begin{math}\displaystyle \parbox{8ex}{\color{labelcolor}(\%o34) }
\begin{pmatrix}\frac{\sqrt{5}-5}{\sqrt{5}} & 0 & 0 \\ 0 & \frac{
 \sqrt{5}+5}{\sqrt{5}} & 0 \\ 0 & 0 & 4 \\ 
 \end{pmatrix}
\end{math}

\begin{center}
{\color{red}\rule{15.8cm}{0.4mm}}
\end{center}



\subsection{{\color{OliveGreen}Ejercicios}}
\begin{enumerate}
\item Encuentre los autovalores y autovectores de las matrices:
\[
 \mathbb{A}=
\left(
\begin{array}
[c]{cccc}
0 & -i & 0 & 0 \\
i & 0 & 0 & 0 \\
0 & 0 & 0 & -i \\
0 & 0& i & 0  
\end{array}
\right) \,,\quad 
 \mathbb{B}=
\left(
\begin{array}
[c]{cccc}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & -1 
\end{array}
\right)
\]

\item Diagonalizar unitariamente: 
\[
 \mathbb{A}=
\left(
\begin{array}
[c]{cc}
2 & -i \\
i & 1 
\end{array}
\right) \,,\quad 
 \mathbb{B}=
\left(
\begin{array}
[c]{ccc}
1 & 1+i & 2i\\
1-i & 5 & -3\\
-2i & -3 & 0
\end{array}
\right)
\]


\item Diagonalizar ortogonalmente: 
\[
 \mathbb{A}=
\left(
\begin{array}
[c]{cc}
1 & -3 \\
-3 & 1 
\end{array}
\right) \,,\quad 
 \mathbb{B}=
\left(
\begin{array}
[c]{ccc}
3 & -2 & 4\\
-2 & 6 & 2\\
4 & 2 & 3
\end{array}
\right)
\]

\item Demuestre que la matriz
\[
 \mathbb{C}=
\left(
\begin{array}
[c]{cc}
x & x-iy \\
x+iy & -z 
\end{array}
\right)
\]
es igual a $ \mathbb{C}=x{\boldsymbol \sigma}_1+y{\boldsymbol \sigma}_2+z{\boldsymbol \sigma}_3$. Donde las matrices ${\boldsymbol \sigma}_i$ son la matrices de Pauli.

\item Las transformaciones de Lorentz se pueden escribir de manera matricial como
\[
\left(
\begin{array}
[c]{cccc}
\gamma & 0 & 0 & -i \gamma v /c \\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
i \gamma v /c & 0 & 0 &  \gamma
\end{array}
\right)
\]
con $\gamma = (\sqrt{1-v^2/c^2})^{-1}$.  La matriz anterior ¿será  ortogonal? ¿será unitaria?

\item  Si los autovalores de una matriz hermítica (o semi-hermítica) $\mathbb{A}$ son todos iguales a $\lambda$, demuestre que $\mathbb{A} = \lambda \mathbb{I}$.

\item Si  una matriz $\mathbb{A}$ es semi-hermítica, demuestre que $(\mathbb{I}-\mathbb{A})(\mathbb{I}+\mathbb{A})^{-1}$ es ortogonal.

\item Dado un observable $\mathbb{A}$ y un vector de estado $\left|\psi\right>$ general, definiremos el valor esperado de $\mathbb{A}$ a la cantidad $\left< \mathbb{A}\right> = \left<\psi\right| \mathbb{A}\left|\psi\right> $, y la relación de dispersión de $\mathbb{A}$ como:
\[
\left< \left(\Delta\mathbb{A} \right)^{2} \right>  = 
\left< \left(\mathbb{A} - \left< \mathbb{A}\right> \mathbb{I} \right)^{2} \right> =
\left< \mathbb{A}^{2}\right> - \left< \mathbb{A}\right>^{2} \equiv 
\left<\psi\right| \mathbb{A}^{2}\left|\psi\right> -\left<\psi\right| \mathbb{A}\left|\psi\right>^{2} \,,
\] 
donde $\mathbb{I}$ es el operador identidad. Nótese que el valor esperado es un número que representa la dispersión de un observable y tiene la misma estructura e interpretación de la varianza en estadística. 
\begin{enumerate}
  \item Muestre que la dispersión siempre es positiva, i.e $\left< \left(\Delta\mathbb{A} \right)^{2} \right> \geqslant 0$. Para ello: 
  \begin{enumerate}
  \item Inicie mostrando que para cualquier operador hermítico $\mathbb{C}$ se cumple $\left< \mathbb{C}^{2} \right> \geqslant 0$.
  \item Termine mostrando que $\mathbb{A} - \left< \mathbb{A}\right> \mathbb{I}$ es un operador hermítico.
\end{enumerate}

  \item Muestre que la dispersión se anula para el caso en que $\left|\psi\right>$ es autovector de $\mathbb{A}$ con autovalor $\left< \mathbb{A}\right>$.
  
  \item Utilizando la desigualdad de Cauchy-Schwarz muestre que las relaciones de dispersión entre dos observables 
  $\mathbb{A}$ y $\mathbb{B}$ siempre cumplen con: 
  \[
\left< \left(\Delta\mathbb{A} \right)^{2} \right> \left< \left(\Delta\mathbb{B} \right)^{2} \right> \geqslant 
\frac{1}{4} \left| \left< [\mathbb{A}, \mathbb{B}]\right> \right|^{2} \quad \mathrm{con} \quad
[\mathbb{A}, \mathbb{B}] = \mathbb{A} \mathbb{B} -  \mathbb{B} \mathbb{A}\,.
\] 

Esta es la forma general de la relación de incertidumbre.\footnote{Para detalles de las implicaciones de este problema se puede consultar Dumitru, S. ``On the uncertainty relations and quantum measurements: conventionalities, short comings, reconsideration. {\textrm arXiv preprint quant-ph/0504058 (2005)}. Y también  Dumitru, S. ``A possible general approach regarding the conformability of angular observables with mathematical rules of Quantum Mechanics. \textrm{arXiv preprint quant-ph/0602147} (2006).}

\item En Mecánica Cuántica se define el operador de spin como $\mathbb{S}_{i} = \frac{\hbar}{2}\mathbf{\sigma}_{i} $, donde  las $\mathbf{\sigma}_{i}$ son las matrices de Pauli y los valores de  
 $i = 1,2,3$ representan las direcciones $x,y,z$, respectivamente.
  \begin{enumerate} 
  \item Encuentre la expresión para el conmutador: $[\mathbb{S}_{i},\mathbb{S}_{j}]$, con $i,j = 1,2,3$. 
  \item Considere un vector de estado general $\left|\psi\right> = a\left|+\right> + b\left|-\right>$, donde $a$ y $b$ son números complejos que cumplen con: $a^{2} + b^{2}=1$ y $\left\{\left|+\right>,\left|-\right>\right\}$ la base de autovectores de $\mathbb{S}_{z}$. Muestre que: 
 \[
\left< \left(\Delta\mathbb{S}_{z} \right)^{2} \right> \left< \left(\Delta\mathbb{S}_{x} \right)^{2} \right> \geqslant 
\hbar^{4} [\mathrm{Im}(ab^{*})]^{2} \,,
 \] 
con   $\mathrm{Im}(\circ)$ la parte imaginaria del argumento.
\end{enumerate} 
\end{enumerate}

\item Dada la matriz: 
\[
\left(
\begin{array}
[c]{ccccccccc}
0 & q & 0 & 0 & 0 & \cdots & 0 & 0 & 0 \\
2q & 2^2 & q & 0 & 0 & \cdots & 0 & 0 & 0\\
0 & q & 4(2^2) & q & 0 & \cdots & 0 & 0 & 0\\
\vdots & \vdots& \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
0 & 0 & 0 & 0 & 0 & \cdots & 4(N-3)^2 & q & 0\\
0 & 0 & 0 & 0 & 0 & \cdots & q & 4(N-2)^2 & q\\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & q & 4(N-1)^2
\end{array}
\right)
\]
Encuentre los autovalores y autovectores cuando $q=1$  y $N=10$.

\item Realice los ejercicios anteriores utilizando el programa {\bf Maxima}. 


\end{enumerate}









